{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"line-height:1.2;\">\n",
    "\n",
    "<h1 style=\"color:#B0EE8F; margin-bottom: 0.2em;\">Common practices in Machine Learning 1</h1>\n",
    "\n",
    "</div>\n",
    "\n",
    "<div style=\"line-height:1.2;\">\n",
    "\n",
    "<h4 style=\"margin-top: 0.2em; margin-bottom: 0.5em;\">Scikit-learn tips and tricks. Focus on Imputers, Encoders, Tokenizers, and Taggers.</h4>\n",
    "\n",
    "</div>\n",
    "\n",
    "<div style=\"margin-top: 3px;\">\n",
    "<span style=\"display: inline-block;\">\n",
    "    <h3 style=\"color: lightblue; display: inline; margin-bottom: 0;\">Keywords:</h3> fancyimpute + font size in markdown + nltk.tag + DictVectorizer + CountVectorizer + TfidfVectorizer \n",
    "    + np.vstack + np.hstack\n",
    "</span>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown \n",
    "from nltk.tag import UnigramTagger \n",
    "from nltk.tag import BigramTagger \n",
    "from nltk.tag import TrigramTagger \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pprint import pprint\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from fancyimpute import KNN, MatrixFactorization, BiScaler, IterativeImputer, SoftImpute\n",
    "from fancyimpute import IterativeSVD, SimpleFill\n",
    "\n",
    "from category_encoders import TargetEncoder\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.datasets import load_iris, make_blobs\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, LabelBinarizer, MultiLabelBinarizer, LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#B0EE8F\"> <b>Imputing </b></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#B0EE8F\"> Recap Fancyimpute: </h3>\n",
    "<div style=\"margin-top: -20px;\">\n",
    "Fancyimpute module => for matrix completion and imputation techniques. <br>\n",
    "It provides a set of tools and algorithms to fill in missing values in matrices:\n",
    "\n",
    "- Matrix Factorization\n",
    "- K-Nearest Neighbors (KNN)\n",
    "- SoftImpute\n",
    "- Iterative Imputer\n",
    "- BiScaler\n",
    "- Nuclear Norm Minimization\n",
    "- Bayesian Low Rank Matrix Completion\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a numpy array with missing values\n",
    "features = np.array([[1, 2], [3, 4], [5, np.nan]])\n",
    "\n",
    "## Standardize features\n",
    "scaler = StandardScaler()\n",
    "standardized_features = scaler.fit_transform(features)\n",
    "# First feature value\n",
    "true_value = standardized_features[0, 0]\n",
    "# Introduce a missing value\n",
    "standardized_features[0, 0] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### => KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         -1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.22474487  1.        ]]\n",
      "True Value: -1.224744871391589\n",
      "\n",
      " Imputed Feature: [[ 0.         -1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.22474487  1.        ]]\n",
      "\n",
      " Imputed Value: 1.224744871391589\n"
     ]
    }
   ],
   "source": [
    "\"\"\" KNN: Missing values are imputed based on the values of their k-nearest neighbors in the dataset. \"\"\"\n",
    "imputer = KNN(k=5, verbose=0)\n",
    "imputed_features = imputer.fit_transform(standardized_features)\n",
    "imputed_value = imputed_features[-1, 0]\n",
    "\n",
    "print(imputed_features)\n",
    "print(\"True Value:\", true_value)\n",
    "print(\"\\n Imputed Feature:\", imputed_features)\n",
    "print(\"\\n Imputed Value:\", imputed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### => Matrix Factorization (SVD = Singular Value Decomposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.61237244, -1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.22474487,  0.        ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputer = SimpleFill()\n",
    "imputed_features_svd = imputer.fit_transform(standardized_features)\n",
    "imputed_features_svd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### => SoftImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Max Singular Value of X_init = 1.414214\n",
      "[SoftImpute] Iter 1: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 2: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 3: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 4: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 5: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 6: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 7: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 8: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 9: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 10: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 11: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 12: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 13: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 14: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 15: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 16: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 17: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 18: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 19: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 20: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 21: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 22: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 23: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 24: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 25: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 26: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 27: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 28: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 29: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 30: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 31: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 32: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 33: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 34: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 35: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 36: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 37: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 38: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 39: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 40: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 41: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 42: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 43: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 44: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 45: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 46: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 47: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 48: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 49: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 50: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 51: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 52: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 53: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 54: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 55: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 56: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 57: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 58: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 59: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 60: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 61: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 62: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 63: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 64: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 65: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 66: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 67: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 68: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 69: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 70: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 71: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 72: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 73: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 74: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 75: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 76: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 77: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 78: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 79: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 80: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 81: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 82: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 83: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 84: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 85: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 86: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 87: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 88: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 89: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 90: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 91: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 92: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 93: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 94: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 95: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 96: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 97: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 98: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 99: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Iter 100: observed MAE=0.017071 rank=2\n",
      "[SoftImpute] Stopped after iteration 100 for lambda=0.028284\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.22474487,  0.        ]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputer = SoftImpute()\n",
    "imputed_features_soft = imputer.fit_transform(standardized_features)\n",
    "imputed_features_soft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### => Iterative Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.44947994e+00, -1.00000000e+00],\n",
       "       [ 0.00000000e+00,  1.00000000e+00],\n",
       "       [ 1.22474487e+00, -3.99999933e-06]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputer = IterativeImputer()\n",
    "imputed_features_bsr = imputer.fit_transform(standardized_features)\n",
    "imputed_features_bsr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### => BiScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BiScaler] Initial log residual value = 1.793231\n",
      "[BiScaler] Iter 1: log residual = 0.693147, log improvement ratio=1.100084\n",
      "[BiScaler] Iter 2: log residual = 0.693147, log improvement ratio=0.000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[nan, -1.],\n",
       "       [-1.,  1.],\n",
       "       [ 1., nan]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a BiScaler imputer\n",
    "imputer = BiScaler()\n",
    "# Impute the missing values using BiScaler\n",
    "imputed_features_bi = imputer.fit_transform(standardized_features)\n",
    "imputed_features_bi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### => Nuclear Norm Minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MatrixFactorization] Iter 10: observed MAE=0.756164 rank=40\n",
      "[MatrixFactorization] Iter 20: observed MAE=0.706554 rank=40\n",
      "[MatrixFactorization] Iter 30: observed MAE=0.662719 rank=40\n",
      "[MatrixFactorization] Iter 40: observed MAE=0.623491 rank=40\n",
      "[MatrixFactorization] Iter 50: observed MAE=0.587722 rank=40\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.06696763, -1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.22474487,  0.51991123]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a MatrixFactorization imputer using Nuclear Norm Minimization\n",
    "imputer = MatrixFactorization()\n",
    "# Impute the missing values using Nuclear Norm Minimization\n",
    "imputed_features_mnm = imputer.fit_transform(standardized_features)\n",
    "imputed_features_mnm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#B0EE8F\"> Sklearn Imputer </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_imputer = SimpleImputer(strategy=\"mean\")\n",
    "features_mean_imputed = mean_imputer.fit_transform(features) \n",
    "# Compare true and imputed values \n",
    "print(\"True Value:\", true_value) \n",
    "print(\"Imputed Value:\", features_mean_imputed[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#B0EE8F\"> <b>Encoding </b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 1],\n",
       "       [1, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 1],\n",
       "       [0, 0, 1, 1, 0],\n",
       "       [1, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1\n",
    "multiclass_feature = [(\"Texas\", \"Florida\"), (\"California\", \"Alabama\"), (\"Texas\", \"Florida\"),\n",
    "                    (\"Delware\", \"Florida\"), (\"Texas\", \"Alabama\")] \n",
    "# Create multiclass one-hot encoder \n",
    "one_hot_multiclass = MultiLabelBinarizer() \n",
    "# One-hot encode multiclass feature \n",
    "one_hot_multiclass.fit_transform(multiclass_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amsterdam', 'ciao', 'forever', 'paris', 'tokyo']\n",
      "[4 4 3 2]\n",
      "['forever', 'forever', 'ciao']\n"
     ]
    }
   ],
   "source": [
    "# 2 \n",
    "le = LabelEncoder()\n",
    "le.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\", \"ciao\", \"forever\"])\n",
    "\n",
    "print(list(le.classes_))\n",
    "print(le.transform([\"tokyo\", \"tokyo\", \"paris\", \"forever\"]))\n",
    "print(list(le.inverse_transform([2, 2, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TargetEncoder(cols=['paris', 'tokyo', 'amsterdam', 'ciao', 'ecco'])\n",
      "TargetEncoder(cols=['color'])\n"
     ]
    }
   ],
   "source": [
    "# 3\n",
    "enc = TargetEncoder(cols=['paris','tokyo', \"amsterdam\", \"ciao\", \"ecco\"])\n",
    "# Target with default parameters\n",
    "ce_target = TargetEncoder(cols = ['color'])\n",
    "print(enc)\n",
    "print(ce_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Score\n",
      "0     Low\n",
      "1     Low\n",
      "2  Medium\n",
      "3  Medium\n",
      "4    High\n",
      "5  Barely\n",
      "\n",
      "   Score\n",
      "0      1\n",
      "1      1\n",
      "2      2\n",
      "3      2\n",
      "4      4\n",
      "5      3\n",
      "[[0. 2. 2. 0.]\n",
      " [0. 0. 3. 5.]\n",
      " [4. 0. 4. 0.]\n",
      " [0. 0. 5. 2.]\n",
      " [0. 1. 1. 0.]]\n",
      "\n",
      "  (0, 1)\t2.0\n",
      "  (0, 2)\t2.0\n",
      "  (1, 2)\t3.0\n",
      "  (1, 3)\t5.0\n",
      "  (2, 0)\t4.0\n",
      "  (2, 2)\t4.0\n",
      "  (3, 2)\t5.0\n",
      "  (3, 3)\t2.0\n",
      "  (4, 1)\t1.0\n",
      "  (4, 2)\t1.0\n"
     ]
    }
   ],
   "source": [
    "# 4\n",
    "dataf = pd.DataFrame({\"Score\":[\"Low\",\"Low\",\"Medium\",\"Medium\",\"High\",\"Barely\"]})\n",
    "print(dataf)\n",
    "print()\n",
    "scale_mapper = {\"Low\":1,\"Medium\":2,\"Barely\":3,\"High\":4}\n",
    "dataf[\"Score\"].replace(scale_mapper, inplace=True)\n",
    "print(dataf)\n",
    "\n",
    "data_dict = [\n",
    "            {\"Red\": 2, \"Green\": 2},\n",
    "            {\"Red\": 3, \"Yellow\": 5},\n",
    "            {\"Red\": 4, \"Blue\": 4},\n",
    "            {\"Red\": 5, \"Yellow\": 2},\n",
    "            {\"Red\": 1, \"Green\": 1},\n",
    "            ]\n",
    "\n",
    "## Create vectorized dictionaries\n",
    "dict_vectorizer = DictVectorizer(sparse=False)\n",
    "dict_vectorizer1 = DictVectorizer(sparse=True)\n",
    "\n",
    "features = dict_vectorizer.fit_transform(data_dict)\n",
    "features1 = dict_vectorizer1.fit_transform(data_dict)\n",
    "print(features)\n",
    "print()\n",
    "print(features1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>color=blue</th>\n",
       "      <th>color=green</th>\n",
       "      <th>color=red</th>\n",
       "      <th>size=large</th>\n",
       "      <th>size=medium</th>\n",
       "      <th>size=small</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   color=blue  color=green  color=red  size=large  size=medium  size=small\n",
       "0         0.0          0.0        1.0         1.0          0.0         0.0\n",
       "1         1.0          0.0        0.0         0.0          1.0         0.0\n",
       "2         0.0          1.0        0.0         0.0          0.0         1.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5\n",
    "data_dict = [{'color': 'red', 'size': 'large'}, {'color': 'blue', 'size': 'medium'}, {'color': 'green', 'size': 'small'}]\n",
    "\n",
    "dict_vectorizer = DictVectorizer(sparse=False)\n",
    "features = dict_vectorizer.fit_transform(data_dict)\n",
    "feature_names = dict_vectorizer.get_feature_names_out()\n",
    "\n",
    "piddi = pd.DataFrame(features, columns=feature_names)\n",
    "piddi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">\n",
    "Replacing categorical feature containing missing values, with values predicted with KNeighborsClassifier :</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2.1    1.45]\n",
      " [  2.1    1.43]\n",
      " [  0.89   1.05]\n",
      " [-12.1    1.78]]\n",
      "[0. 1. 1. 0.]\n",
      "\n",
      "[[0.87 1.31]\n",
      " [0.87 1.31]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0,2.10,1.45],[1,2.10,1.43],[1,0.89,1.05],[0,-12.10,1.78]])\n",
    "X_with_nan = np.array([[np.nan, 0.87, 1.31],[np.nan, 0.87, 1.31]])\n",
    "print(X[:,1:]) \n",
    "print(X[:,0])\n",
    "print()\n",
    "print(X_with_nan[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputed_values\n",
      "[1. 1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  1.  ,   0.87,   1.31],\n",
       "       [  1.  ,   0.87,   1.31],\n",
       "       [  0.  ,   2.1 ,   1.45],\n",
       "       [  1.  ,   2.1 ,   1.43],\n",
       "       [  1.  ,   0.89,   1.05],\n",
       "       [  0.  , -12.1 ,   1.78]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = KNeighborsClassifier(3, weights='distance')\n",
    "trained_model = clf.fit(X[:,1:], X[:,0])\n",
    "imputed_values = trained_model.predict(X_with_nan[:,1:])\n",
    "print(\"inputed_values\")\n",
    "print(imputed_values)\n",
    "X_with_imputed = np.hstack((imputed_values.reshape(-1,1), X_with_nan[:,1:]))\n",
    "\n",
    "# Stack arrays in sequence vertically (row wise)\n",
    "np.vstack((X_with_imputed, X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.  ,   0.87,   1.31],\n",
       "       [  0.  ,   0.87,   1.31],\n",
       "       [  0.  ,   2.1 ,   1.45],\n",
       "       [  1.  ,   2.1 ,   1.43],\n",
       "       [  1.  ,   0.89,   1.05],\n",
       "       [  0.  , -12.1 ,   1.78]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join the two feature matrices \n",
    "X_complete = np.vstack((X_with_nan, X)) \n",
    "imputer = SimpleImputer(strategy='most_frequent') \n",
    "imputer.fit_transform(X_complete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">\n",
    "Handling imbalanced classes: </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "\n",
      "[0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "features_iris = iris.data\n",
    "target_iris = iris.target\n",
    "features_iris = features_iris[40,:]\n",
    "target_iris = target_iris[40:]\n",
    "# create a binary target vector indicating if class is 0\n",
    "target_iris2 = np.where((target_iris == 0), 0, 1) \n",
    "print(target_iris)\n",
    "print()\n",
    "print(target_iris2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {0: .9, 1: 0.1} \n",
    "clf1 = RandomForestClassifier(class_weight=weights)\n",
    "clf2 = RandomForestClassifier(class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_iris.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\"> Alternatively, it is possible to downsample the majority class or upsample the minority class <br>\n",
    "Downsampling => sample without replacement from the majority class <br>\n",
    "(i.e., the class with more observations) to create a new subset of observations equal in size to the minority class <br>\n",
    "\n",
    "For example, if the minority class has 10 observations, 10 observations are randomly selected from the majority class <br> \n",
    "to use 20 observations in total as data <br>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3  4  5]\n",
      " [10 12 13 14 15]]\n",
      "[ 1  2  3  4  5 10 12 13 14 15]\n",
      "ab.shape (5,)\n",
      "ab.size 5\n",
      "ab.ndim 1\n",
      "cd.shape (2, 5)\n",
      "cd.size 10\n",
      "cd.ndim 2\n"
     ]
    }
   ],
   "source": [
    "ab = np.array([1,2,3,4,5])\n",
    "bc = np.array([10,12,13,14,15])\n",
    "\n",
    "cd = np.vstack((ab, bc))        #nb! not cd = np.vstack(ab,bc)\n",
    "de = np.hstack((ab, bc))        #nb! not de = np.hstack(ab, bc)\n",
    "\n",
    "print(cd)\n",
    "print(de)\n",
    "print(f\"ab.shape {ab.shape}\")\n",
    "print(f\"ab.size {ab.size}\")\n",
    "print(f\"ab.ndim {ab.ndim}\")\n",
    "print(f\"cd.shape {cd.shape}\")\n",
    "print(f\"cd.size {cd.size}\")\n",
    "print(f\"cd.ndim {cd.ndim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      " [5.1 4.9 4.7 4.6 5.  5.4 4.6 5.  4.4 4.9 5.4 4.8 4.8 4.3 5.8 5.7 5.4 5.1\n",
      " 5.7 5.1 5.4 5.1 4.6 5.1 4.8 5.  5.  5.2 5.2 4.7 4.8 5.4 5.2 5.5 4.9 5.\n",
      " 5.5 4.9 4.4 5.1 5.  4.5 4.4 5.  5.1 4.8 5.1 4.6 5.3 5.  6.4 7.7 5.5 4.9\n",
      " 5.6 6.2 6.7 6.  5.9 5.8 6.4 5.7 6.7 5.8 6.9 5.1 6.4 5.6 6.3 6.9 6.5 5.6\n",
      " 7.3 5.  5.9 6.3 7.2 6.3 5.2 7.7 7.2 6.  5.7 6.3 5.5 5.8 5.8 5.7 7.9 6.1\n",
      " 6.5 5.  6.1 6.2 5.5 6.6 6.4 5.6 6.1 6.7]\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "# Select the first feature as a column vector\n",
    "features_iris = iris.data[:, 0]  \n",
    "target_iris = iris.target\n",
    "\n",
    "# Create a binary target vector indicating if class is 0\n",
    "target_iris2 = np.where(target_iris == 0, 0, 1)\n",
    "\n",
    "## Split the indices of class 0 and class 1\n",
    "i_class0 = np.where(target_iris2 == 0)[0]\n",
    "i_class1 = np.where(target_iris2 == 1)[0]\n",
    "\n",
    "# Downsample class 1 without replacement\n",
    "i_class1_downsampled = np.random.choice(i_class1, size=len(i_class0), replace=False)\n",
    "\n",
    "# Join together class 0's target vector with the downsampled class 1's target vector\n",
    "target_downsampled = np.hstack((target_iris2[i_class0], target_iris2[i_class1_downsampled]))\n",
    "# Join together class 0's feature vector with the downsampled class 1's feature vector\n",
    "features_downsampled = np.hstack((features_iris[i_class0], features_iris[i_class1_downsampled]))\n",
    "\n",
    "print(target_iris2[i_class0] )\n",
    "print(\"\\n\", target_iris2[i_class1_downsampled])\n",
    "print(\"\\n\", target_downsampled)\n",
    "print(\"\\n\", features_downsampled)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#B0EE8F\"> <b> Tokenizing text wiht NLTK </b> </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'science', 'of', 'today', 'is', 'the', 'technology', 'of', 'tomorrrow']\n",
      "['The science of today is the technology of tomorrrow.', 'Tomorrow is today.']\n"
     ]
    }
   ],
   "source": [
    "\"\"\" The Natural Language Toolkit (NLTK) can be used for natural language processing tasks such as: \\\n",
    "    tokenization, stemming, and part-of-speech tagging. \n",
    "    \n",
    "    The punkt package contains pre-trained models for tokenization and sentence segmentation in various languages. \\\n",
    "    ---> The catch block is used to avoid the download to be done every time, if the package is already present.\n",
    "    _The word_tokenize() function, tokenizes a given text string into words, \\\n",
    "        using the TreebankWordTokenizer from the nltk.tokenize module. \n",
    "    _The Sentence tokenization is used by the sent_tokenize() function in the nltk.tokenize module ;  \\\n",
    "    This tokenizer is designed to split text into sentences using a set of heuristics \n",
    "    that take into account common abbreviations, punctuation marks, and other linguistic patterns.\n",
    "\"\"\"\n",
    "# nltk.download('punkt') \n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt') \n",
    "\n",
    "stri = \"The science of today is the technology of tomorrrow\"\n",
    "stri2 = \"The science of today is the technology of tomorrrow. Tomorrow is today.\"\n",
    "yea = nltk.tokenize.word_tokenize(stri) \n",
    "azz = nltk.tokenize.sent_tokenize(stri2)\n",
    "print(yea)\n",
    "print(azz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n",
      "['going', 'go', 'store', 'park']\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Removing stopwords means to eliminate from a string the common linking words \\\n",
    "(like pronouns or articles) that contain little informational value.\n",
    "The set of stopwords contains the words that we want to remove before processing.\n",
    "\"\"\"\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "tokenized_words = ['i', 'am', 'going', 'to', 'go', 'to', 'the', 'store', 'and', 'park']\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "res = [word for word in tokenized_words if word not in stop_words]\n",
    "\n",
    "print(stop_words[:10])\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"> => Stemming </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'am', 'humbl', 'by', 'thi', 'tradit', 'meet']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Stemming means to reduce a word to its stem by identifying and removing affixes (e.g., gerunds) \\\n",
    "while keeping the root meaning of the word. \n",
    "For example, both “tradition” and “traditional” have “tradit” as their stem, \\\n",
    "indicating that while they are different words they represent the same general concept. \n",
    "\n",
    "By stemming our text data, we transform it to something less readable, \\\n",
    "but closer to its base meaning and thus more suitable for comparison across observations.\n",
    "nltk PorterStemmer implements the widely used Porter stemming algorithm to remove or \\\n",
    "replace common SUFFIXES to produce the word stem.\n",
    "\"\"\"\n",
    "\n",
    "tokenized_words = ['i', 'am', 'humbled', 'by', 'this', 'traditional', 'meeting']\n",
    "porter = nltk.stem.porter.PorterStemmer()\n",
    "stemm = [porter.stem(word) for word in tokenized_words]\n",
    "stemm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\"> Tagging: <br>\n",
    "</font>\n",
    "\n",
    "    - NNP Proper noun, singular \n",
    "    - NN Noun, singular or mass \n",
    "    - RB Adverb \n",
    "    - VBD Verb, past tense \n",
    "    - VBG Verb, gerund or present participle\n",
    "    - JJ Adjective \n",
    "    - PRP Personal pronoun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/notto4/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Chris', 'NNP'), ('loved', 'VBD'), ('outdoor', 'RP'), ('running', 'VBG')]\n",
      "filtered result = ['Chris']\n",
      "[[1 1 0 1 0 1 1 1 0]\n",
      " [1 0 1 1 0 0 0 0 1]\n",
      " [1 0 1 1 1 0 0 0 1]]\n",
      "<class 'sklearn.preprocessing._label.MultiLabelBinarizer'>\n",
      "['DT' 'IN' 'JJ' 'NN' 'NNP' 'PRP' 'VBG' 'VBP' 'VBZ']\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Tag Part of speech \"\"\"\n",
    "try:\n",
    "    nltk.data.find('taggers/averaged_perceptron_tagger')\n",
    "except LookupError:\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "text_data = \"Chris loved outdoor running\"\n",
    "text_tagged = nltk.pos_tag(nltk.word_tokenize(text_data))\n",
    "\n",
    "resu = [word for word, tag in text_tagged if tag in ['NNP','NN','NNS','NNPS']]\n",
    "print(text_tagged)\n",
    "print(f\"filtered result = {resu}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 1 0 1 1 1 0]\n",
      " [1 0 1 1 0 0 0 0 1]\n",
      " [1 0 1 1 1 0 0 0 1]]\n",
      "\n",
      " <class 'sklearn.preprocessing._label.MultiLabelBinarizer'>\n",
      "['DT' 'IN' 'JJ' 'NN' 'NNP' 'PRP' 'VBG' 'VBP' 'VBZ']\n"
     ]
    }
   ],
   "source": [
    "tweets = [\"I am eating a burrito for breakfast\", \"Political science is an amazing field\", \"San Francisco is an awesome city\"]\n",
    "tagged_tweets = []\n",
    "\"\"\" Tag each word and each tweet \"\"\"\n",
    "for tweet in tweets:\n",
    "    tweet_tag = nltk.pos_tag(nltk.word_tokenize(tweet))\n",
    "    tagged_tweets.append([tag for word, tag in tweet_tag])\n",
    "\n",
    "\"\"\" Use one-hot encoding to convert the tags into features \"\"\"    \n",
    "one_hot_tweet = MultiLabelBinarizer()\n",
    "enco_res = one_hot_tweet.fit_transform(tagged_tweets)\n",
    "print(enco_res)\n",
    "print(\"\\n\", one_hot_tweet.__class__)\n",
    "print(one_hot_tweet.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\"> \n",
    "Taggers: <br>\n",
    "</font>\n",
    "\n",
    "    - UnigramTagger \n",
    "    - BigramTagger \n",
    "    - TrigramTagger "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8174734002697437"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "- UnigramTagger: based on unigrams (single words),\n",
    "- BigramTagger : based on bigrams (pairs of words)\n",
    "- TrigramTagger : based on trigrams (triplets of words)\n",
    "\n",
    "The list of tagged sentences is retrieved from the Brown Corpus, specifically those in the 'news' category.\n",
    "\"\"\"\n",
    "#nltk.download('brown')\n",
    "try:\n",
    "    nltk.data.find('corpora/brown')\n",
    "except LookupError:\n",
    "    nltk.download('brown')\n",
    "\n",
    "# Get some text from the Brown Corpus, broken into sentences \n",
    "sentences = brown.tagged_sents(categories='news')  \n",
    "# Split into 4000 sentences for training and 623 for testing \n",
    "train, test = sentences[:4000], sentences[4000:]   \n",
    "\n",
    "unigram = UnigramTagger(train) \n",
    "bigram = BigramTagger(train, backoff=unigram) \n",
    "trigram = TrigramTagger(train, backoff=bigram) \n",
    "\n",
    "# Show the accuracy \n",
    "#trigram.evaluate(test)     #deprecated !! \n",
    "trigram.accuracy(test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"> Encoding text as bag of words: </font> <br>\n",
    "Bag-of-words models output a feature for every unique word in text data, with each feature containing a count of occurrences in observations. <br>\n",
    "Every feature can be set to be the combination of two words (called a 2-gram) or even three words (3-gram). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['beats' 'best' 'both' 'brazil' 'is' 'italy' 'love' 'sweden']\n",
      "  (0, 6)\t1\n",
      "  (0, 3)\t2\n",
      "  (1, 7)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 1)\t1\n",
      "  (2, 5)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 2)\t1\n",
      "[[0 0 0 2 0 0 1 0]\n",
      " [0 1 0 0 1 0 0 1]\n",
      " [1 0 1 0 0 1 0 0]]\n",
      "[[2]\n",
      " [0]\n",
      " [0]]\n",
      "{'brazil': 0}\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Transforming text into features is by using a bag-of-words model. \n",
    "\n",
    "=> CountVectorizer can transform a given text into a vector on the basis of the frequency (count) of each word \\\n",
    "that occurs in the entire text.\n",
    "It creates a matrix in which each unique word is represented by a column of the matrix, \\\n",
    "and each text sample from the document is a row in the matrix. \n",
    "\n",
    "=> \"ngram_range\" sets the minimum and maximum size of the n-grams. \n",
    "\"\"\"\n",
    "text_data = np.array(['I love Brazil. Brazil!', 'Sweden is best', 'Italy beats both']) \n",
    "count_matri = CountVectorizer() \n",
    "bag_of_words = count_matri.fit_transform(text_data)\n",
    "\n",
    "print(count_matri.get_feature_names_out())\n",
    "print(bag_of_words)\n",
    "print(bag_of_words.toarray())\n",
    "\n",
    "# Create feature matrix with arguments\n",
    "count_2gram = CountVectorizer(ngram_range=(1,2), stop_words=\"english\", vocabulary=['brazil'])\n",
    "bag2 = count_2gram.fit_transform(text_data)\n",
    "print(bag2.toarray())\n",
    "print(count_2gram.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#B0EE8F\"> <b> Weighting word importance: </b> </h2>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Term frequency (tf):** <br>\n",
    "&emsp;&emsp;&emsp;\n",
    "The more a word appears in a document, the more likely it is important to that document. <br>\n",
    "**Document frequency (df):** <br>\n",
    "&emsp;&emsp;&emsp;\n",
    "if a word appears in many documents, it is likely less important to any individual document. <br>\n",
    "\n",
    "By combining these two statistics, it is possible to assign a score to every word representing how important that word is in a document. <br>\n",
    "Specifically, multiply tf to idf (inverse of document frequency) [where t is a word and d is a document].\n",
    "\n",
    "$$tf - idf(t,d) = tf(t,d) \\times idf(t)$$\n",
    "\n",
    "$$idt(f) = \\frac{\\log (1 + n_{d})} { 1 + df(d,t)} + 1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      "{'love': 6, 'brazil': 3, 'sweden': 7, 'is': 4, 'best': 1, 'italy': 5, 'beats': 0, 'both': 2}\n",
      "Feature_matrix:\n",
      "   (0, 3)\t0.8944271909999159\n",
      "  (0, 6)\t0.4472135954999579\n",
      "  (1, 1)\t0.5773502691896257\n",
      "  (1, 4)\t0.5773502691896257\n",
      "  (1, 7)\t0.5773502691896257\n",
      "  (2, 2)\t0.5773502691896257\n",
      "  (2, 0)\t0.5773502691896257\n",
      "  (2, 5)\t0.5773502691896257\n",
      "\n",
      " [[0.         0.         0.         0.89442719 0.         0.\n",
      "  0.4472136  0.        ]\n",
      " [0.         0.57735027 0.         0.         0.57735027 0.\n",
      "  0.         0.57735027]\n",
      " [0.57735027 0.         0.57735027 0.         0.         0.57735027\n",
      "  0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "feature_matrix = tfidf.fit_transform(text_data)\n",
    "print(\"Vocabulary:\")\n",
    "print(tfidf.vocabulary_)\n",
    "print(\"Feature_matrix:\\n\", feature_matrix)\n",
    "print(\"\\n\", feature_matrix.toarray())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "just_for_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
