{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"line-height:0.4\">\n",
    "<h1 style=\"color:#0FCBC6\"> PySpark 3: Functions, Preprocessing, and Classifications </h1>\n",
    "<span style=\"display: inline-block;\">\n",
    "    <h3 style=\"color: lightblue; display: inline;\">Keywords:</h3> keyword_only + pandas inferSchema + VectorAssembler + mlflow + avgMetrics\n",
    "</span>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#0FCBC6\"> Recap: </h3>\n",
    "<div style=\"margin-top: -8px;\">\n",
    "The findspark library is used to searche for the location of the Spark installation and sets the necessary environment variables to ensure <br> \n",
    "that the Python interpreter can find and use the Spark libraries and executables.\n",
    "\n",
    "SPARK_HOME environment variable, which should point to the root directory where Spark is installed.\n",
    "\n",
    "+ findspark.init() simplifies the process of setting up the local development environment for working with PySpark. <br>\n",
    "It allows to import PySpark and work with Spark functionality directly \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import findspark\n",
    "#findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.ml.classification import LogisticRegression, GBTClassifier, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, QuantileDiscretizer\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.pipeline import Transformer,Estimator\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "\n",
    "from pyspark.sql.functions import isnan, col, when, rand\n",
    "\n",
    "from pyspark.ml.param.shared import Param, Params, HasOutputCols\n",
    "from pyspark.sql.functions import count, lit, regexp_extract\n",
    "from pyspark import keyword_only\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "import mlflow\n",
    "from mlflow import spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#0FCBC6\"> Recap: Possible Warnings </h3>\n",
    "<div style=\"margin-top: -8px;\">\n",
    "\n",
    "- WARN Utils: Your hostname, hpmint resolves to a loopback address: 127.0.1.1; using 192.168.1.81 instead (on interface eno1)\n",
    "- WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
    "Setting default log level to \"WARN\".\n",
    "For SparkR, use setLogLevel(newLevel).\n",
    "- WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "- WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
    "- WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#0FCBC6\"><u>Example 1 Logistic Regression</u></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LogisticRegression with PySpark\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#0FCBC6\"> => Load and prepare data </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+---+---------------+---------+\n",
      "| User ID|Gender|Age|EstimatedSalary|Purchased|\n",
      "+--------+------+---+---------------+---------+\n",
      "|15624510|  Male| 19|          19000|        0|\n",
      "|15810944|  Male| 35|          20000|        0|\n",
      "|15668575|Female| 26|          43000|        0|\n",
      "+--------+------+---+---------------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Construct the full path to the CSV file\n",
    "csv_file_path = os.path.join(current_dir, \"datasets_for_pyspark/social_network_ads.csv\")\n",
    "\n",
    "# Read the CSV file using the full path\n",
    "df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feature_1', 'feature_2', 'feature_3', 'feature_4', 'feature_5']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename the columns to make easier to create the VectorAssembler with a unique feature\n",
    "columns = [f'feature_{i}' for i in range(1, 6)]\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------+---------+---------+\n",
      "|feature_1|feature_2|feature_3|feature_4|feature_5|\n",
      "+---------+---------+---------+---------+---------+\n",
      "| 15624510|     Male|       19|    19000|        0|\n",
      "| 15810944|     Male|       35|    20000|        0|\n",
      "| 15668575|   Female|       26|    43000|        0|\n",
      "+---------+---------+---------+---------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_ok = df.toDF(*columns)\n",
    "dataset_ok.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------+---------+------+\n",
      "|feature_1|feature_2|feature_3|feature_4|target|\n",
      "+---------+---------+---------+---------+------+\n",
      "| 15624510|     Male|       19|    19000|     0|\n",
      "| 15810944|     Male|       35|    20000|     0|\n",
      "| 15668575|   Female|       26|    43000|     0|\n",
      "+---------+---------+---------+---------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rename a column\n",
    "old_column_name = \"feature_5\"\n",
    "new_column_name = \"target\"\n",
    "dataset_ok = dataset_ok.withColumnRenamed(old_column_name, new_column_name)\n",
    "dataset_ok.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feature_1', 'feature_2', 'feature_3', 'feature_4', 'target']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_columns = dataset_ok.columns\n",
    "dataset_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the last column\n",
    "column_to_remove = dataset_columns[-1]\n",
    "df_removed = dataset_ok.drop(column_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------+---------+\n",
      "|feature_1|feature_2|feature_3|feature_4|\n",
      "+---------+---------+---------+---------+\n",
      "| 15624510|     Male|       19|    19000|\n",
      "| 15810944|     Male|       35|    20000|\n",
      "| 15668575|   Female|       26|    43000|\n",
      "| 15603246|   Female|       27|    57000|\n",
      "| 15804002|     Male|       19|    76000|\n",
      "| 15728773|     Male|       27|    58000|\n",
      "| 15598044|   Female|       27|    84000|\n",
      "| 15694829|   Female|       32|   150000|\n",
      "| 15600575|     Male|       25|    33000|\n",
      "| 15727311|   Female|       35|    65000|\n",
      "| 15570769|   Female|       26|    80000|\n",
      "| 15606274|   Female|       26|    52000|\n",
      "| 15746139|     Male|       20|    86000|\n",
      "| 15704987|     Male|       32|    18000|\n",
      "| 15628972|     Male|       18|    82000|\n",
      "| 15697686|     Male|       29|    80000|\n",
      "| 15733883|     Male|       47|    25000|\n",
      "| 15617482|     Male|       45|    26000|\n",
      "| 15704583|     Male|       46|    28000|\n",
      "| 15621083|   Female|       48|    29000|\n",
      "+---------+---------+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_removed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feature_1', 'feature_2', 'feature_3', 'feature_4']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd_columns = dataset_columns[:-1]\n",
    "dd_columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"line-height:0.5\">\n",
    "<h3 style=\"color:#0FCBC6\"> => Transform data </h3>\n",
    "Encoding categorical data is neceeray to create the VectorAssembler \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringIndexer_4eb872988d3f"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a StringIndexer for a single column\n",
    "string_indexer = StringIndexer(inputCol=\"feature_2\", outputCol=\"feature2_encoded\")\n",
    "string_indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------+---------+----------------+\n",
      "|feature_1|feature_2|feature_3|feature_4|feature2_encoded|\n",
      "+---------+---------+---------+---------+----------------+\n",
      "| 15624510|     Male|       19|    19000|             1.0|\n",
      "| 15810944|     Male|       35|    20000|             1.0|\n",
      "| 15668575|   Female|       26|    43000|             0.0|\n",
      "| 15603246|   Female|       27|    57000|             0.0|\n",
      "| 15804002|     Male|       19|    76000|             1.0|\n",
      "| 15728773|     Male|       27|    58000|             1.0|\n",
      "| 15598044|   Female|       27|    84000|             0.0|\n",
      "| 15694829|   Female|       32|   150000|             0.0|\n",
      "| 15600575|     Male|       25|    33000|             1.0|\n",
      "| 15727311|   Female|       35|    65000|             0.0|\n",
      "| 15570769|   Female|       26|    80000|             0.0|\n",
      "| 15606274|   Female|       26|    52000|             0.0|\n",
      "| 15746139|     Male|       20|    86000|             1.0|\n",
      "| 15704987|     Male|       32|    18000|             1.0|\n",
      "| 15628972|     Male|       18|    82000|             1.0|\n",
      "| 15697686|     Male|       29|    80000|             1.0|\n",
      "| 15733883|     Male|       47|    25000|             1.0|\n",
      "| 15617482|     Male|       45|    26000|             1.0|\n",
      "| 15704583|     Male|       46|    28000|             1.0|\n",
      "| 15621083|   Female|       48|    29000|             0.0|\n",
      "+---------+---------+---------+---------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit and transform the DataFrame\n",
    "dataframe_ok = string_indexer.fit(dataset_ok).transform(df_removed)\n",
    "dataframe_ok.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------+---------+\n",
      "|feature_1|feature_2|feature_3|feature_4|\n",
      "+---------+---------+---------+---------+\n",
      "| 15624510|      1.0|       19|    19000|\n",
      "| 15810944|      1.0|       35|    20000|\n",
      "| 15668575|      0.0|       26|    43000|\n",
      "| 15603246|      0.0|       27|    57000|\n",
      "| 15804002|      1.0|       19|    76000|\n",
      "| 15728773|      1.0|       27|    58000|\n",
      "| 15598044|      0.0|       27|    84000|\n",
      "| 15694829|      0.0|       32|   150000|\n",
      "| 15600575|      1.0|       25|    33000|\n",
      "| 15727311|      0.0|       35|    65000|\n",
      "| 15570769|      0.0|       26|    80000|\n",
      "| 15606274|      0.0|       26|    52000|\n",
      "| 15746139|      1.0|       20|    86000|\n",
      "| 15704987|      1.0|       32|    18000|\n",
      "| 15628972|      1.0|       18|    82000|\n",
      "| 15697686|      1.0|       29|    80000|\n",
      "| 15733883|      1.0|       47|    25000|\n",
      "| 15617482|      1.0|       45|    26000|\n",
      "| 15704583|      1.0|       46|    28000|\n",
      "| 15621083|      0.0|       48|    29000|\n",
      "+---------+---------+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replace the feature2 with \n",
    "dataframe_tmp  = dataframe_ok.withColumn(\"feature_2\", col(\"feature2_encoded\")).drop(\"feature2_encoded\")\n",
    "dataframe_tmp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=dd_columns, outputCol=\"assembled_features\")\n",
    "dataframe = assembler.transform(dataframe_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|  assembled_features|\n",
      "+--------------------+\n",
      "|[1.562451E7,1.0,1...|\n",
      "|[1.5810944E7,1.0,...|\n",
      "|[1.5668575E7,0.0,...|\n",
      "+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = dataframe.select(\"assembled_features\")\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|target|\n",
      "+------+\n",
      "|     0|\n",
      "|     0|\n",
      "|     0|\n",
      "|     0|\n",
      "|     0|\n",
      "|     0|\n",
      "|     0|\n",
      "|     1|\n",
      "|     0|\n",
      "|     0|\n",
      "|     0|\n",
      "|     0|\n",
      "|     0|\n",
      "|     0|\n",
      "|     0|\n",
      "|     0|\n",
      "|     1|\n",
      "|     1|\n",
      "|     1|\n",
      "|     1|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select the \"target\" column from df2 and alias it as \"target_column\"\n",
    "target_column = dataset_ok.select(\"target\").alias(\"target_column\")\n",
    "target_column.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|  assembled_features| id|\n",
      "+--------------------+---+\n",
      "|[1.562451E7,1.0,1...|  0|\n",
      "|[1.5810944E7,1.0,...|  1|\n",
      "|[1.5668575E7,0.0,...|  2|\n",
      "|[1.5603246E7,0.0,...|  3|\n",
      "+--------------------+---+\n",
      "only showing top 4 rows\n",
      "\n",
      "None\n",
      "+---------+---------+---------+---------+------+---+\n",
      "|feature_1|feature_2|feature_3|feature_4|target| id|\n",
      "+---------+---------+---------+---------+------+---+\n",
      "| 15624510|     Male|       19|    19000|     0|  0|\n",
      "| 15810944|     Male|       35|    20000|     0|  1|\n",
      "| 15668575|   Female|       26|    43000|     0|  2|\n",
      "| 15603246|   Female|       27|    57000|     0|  3|\n",
      "+---------+---------+---------+---------+------+---+\n",
      "only showing top 4 rows\n",
      "\n",
      "None\n",
      "+--------------------+------+\n",
      "|  assembled_features|target|\n",
      "+--------------------+------+\n",
      "|[1.562451E7,1.0,1...|     0|\n",
      "|[1.5810944E7,1.0,...|     0|\n",
      "|[1.5668575E7,0.0,...|     0|\n",
      "|[1.5603246E7,0.0,...|     0|\n",
      "|[1.5804002E7,1.0,...|     0|\n",
      "|[1.5728773E7,1.0,...|     0|\n",
      "|[1.5598044E7,0.0,...|     0|\n",
      "|[1.5694829E7,0.0,...|     1|\n",
      "|[1.5600575E7,1.0,...|     0|\n",
      "|[1.5727311E7,0.0,...|     0|\n",
      "|[1.5570769E7,0.0,...|     0|\n",
      "|[1.5606274E7,0.0,...|     0|\n",
      "|[1.5746139E7,1.0,...|     0|\n",
      "|[1.5704987E7,1.0,...|     0|\n",
      "|[1.5628972E7,1.0,...|     0|\n",
      "|[1.5697686E7,1.0,...|     0|\n",
      "|[1.5733883E7,1.0,...|     1|\n",
      "|[1.5617482E7,1.0,...|     1|\n",
      "|[1.5704583E7,1.0,...|     1|\n",
      "|[1.5621083E7,0.0,...|     1|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Add the \"target_column\" to df1 using withColumn \"\"\"\n",
    "# Add a unique ID to both DataFrames\n",
    "df1_with_id = df.withColumn(\"id\", monotonically_increasing_id())\n",
    "df2_with_id = dataset_ok.withColumn(\"id\", monotonically_increasing_id())\n",
    "\n",
    "print(df1_with_id.show(4))\n",
    "print(df2_with_id.show(4))\n",
    "\n",
    "# Join the DataFrames based on the common ID column\n",
    "joined_df = df1_with_id.join(df2_with_id, on=\"id\", how=\"inner\").drop(\"id\")\n",
    "\n",
    "# Select the desired columns\n",
    "result_df = joined_df.select(\"assembled_features\", \"target\")\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#0FCBC6\"> => Train data </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = result_df.randomSplit([0.8, 0.2], seed=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|  assembled_features|target|\n",
      "+--------------------+------+\n",
      "|[1.5566689E7,0.0,...|     0|\n",
      "|[1.5570769E7,0.0,...|     0|\n",
      "|[1.5570932E7,1.0,...|     0|\n",
      "|[1.5571059E7,0.0,...|     0|\n",
      "|[1.5573452E7,0.0,...|     0|\n",
      "+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#0FCBC6\"> => Logistic Regression </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression = LogisticRegression(featuresCol=\"assembled_features\", labelCol=\"target\")\n",
    "model = logistic_regression.fit(train_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#0FCBC6\"> => Inspect the model coefficients and intercept </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients:  [-8.708251472782162e-08,0.41870463194828345,0.23618911044528476,3.319963305616845e-05]\n",
      "Intercept: -11.242\n"
     ]
    }
   ],
   "source": [
    "coefficients = model.coefficients\n",
    "intercept = model.intercept\n",
    "\n",
    "print(\"Coefficients: \", coefficients)\n",
    "print(\"Intercept: {:.3f}\".format(intercept))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#0FCBC6\"> => Evaluate the model on test data </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+--------------------+--------------------+----------+\n",
      "|  assembled_features|target|       rawPrediction|         probability|prediction|\n",
      "+--------------------+------+--------------------+--------------------+----------+\n",
      "|[1.5569641E7,0.0,...|     1|[-4.2553027869232...|[0.01399028859221...|       1.0|\n",
      "|[1.5574305E7,1.0,...|     0|[2.15313286257781...|[0.89596116836296...|       0.0|\n",
      "|[1.5579781E7,0.0,...|     0|[4.18864681938546...|[0.98505980052822...|       0.0|\n",
      "|[1.5582492E7,1.0,...|     1|[1.48319526631109...|[0.81505472177296...|       0.0|\n",
      "|[1.5583681E7,1.0,...|     1|[0.63814126480846...|[0.65433317020271...|       0.0|\n",
      "|[1.5584114E7,1.0,...|     0|[5.74805626154796...|[0.99682116391712...|       0.0|\n",
      "|[1.5587177E7,1.0,...|     0|[0.57491263980694...|[0.63989596665148...|       0.0|\n",
      "|[1.5591433E7,1.0,...|     0|[1.95163493450095...|[0.87562480537294...|       0.0|\n",
      "|[1.5592877E7,1.0,...|     0|[4.02530464734309...|[0.98245532948915...|       0.0|\n",
      "|[1.5594577E7,1.0,...|     0|[5.54597792851044...|[0.99611205078884...|       0.0|\n",
      "|[1.5595917E7,1.0,...|     0|[2.43957034959598...|[0.91979539744941...|       0.0|\n",
      "|[1.5599081E7,0.0,...|     1|[1.24129257119936...|[0.77578892485371...|       0.0|\n",
      "|[1.560155E7,0.0,3...|     0|[2.30482131413840...|[0.90927555176439...|       0.0|\n",
      "|[1.5603319E7,1.0,...|     0|[2.96003402611238...|[0.95073558763607...|       0.0|\n",
      "|[1.5609637E7,0.0,...|     0|[0.52698660319755...|[0.62878001119814...|       0.0|\n",
      "|[1.5611191E7,0.0,...|     1|[-2.6391437264796...|[0.06666129106302...|       1.0|\n",
      "|[1.5613014E7,0.0,...|     1|[-0.9420120101386...|[0.28049410513033...|       1.0|\n",
      "|[1.561442E7,0.0,2...|     0|[6.23959096135203...|[0.99805314438797...|       0.0|\n",
      "|[1.5614827E7,0.0,...|     0|[1.97777285717686...|[0.87844354709639...|       0.0|\n",
      "|[1.5617134E7,1.0,...|     0|[1.95766472126408...|[0.87627999940520...|       0.0|\n",
      "+--------------------+------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(test_data)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC: 0.9300\n"
     ]
    }
   ],
   "source": [
    "# AUC-ROC\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"target\")\n",
    "auc = evaluator.evaluate(predictions)\n",
    "print(f\"AUC-ROC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8590\n",
      "Precision: 0.8599\n",
      "Recall: 0.8590\n"
     ]
    }
   ],
   "source": [
    "# Accuracy, Precision, and Recall\n",
    "multi_evaluator = MulticlassClassificationEvaluator(labelCol=\"target\", predictionCol=\"prediction\")\n",
    "accuracy = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"accuracy\"})\n",
    "precision = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"weightedPrecision\"})\n",
    "recall = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"weightedRecall\"})\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#0FCBC6\"><u>Example 2 Pipeline</u></h2>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>PySpark pipeline: </h3>\n",
    "<div style=\"margin-top: -15px;\">\n",
    "The pipeline is a sequence of stages (executed in order) when fitted, each of them acts as an estimator (or Transformer).    <br>\n",
    "<div style=\"line-height:1.5\">\n",
    "GOAL => Predict if a person is survided or not, using the Titanic dataset.  <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Spark session creation \"\"\"\n",
    "spa = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName('Example2TitanicData') \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[PassengerId: int, Survived: int, Pclass: int, Name: string, Sex: string, Age: double, SibSp: int, Parch: int, Ticket: string, Fare: double, Cabin: string, Embarked: string]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"./datasets_for_pyspark/titanic.csv\"\n",
    "data_titanic_raw = spa.read.csv(path, inferSchema=True, header=True)\n",
    "data_titanic_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_titanic_raw.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|PassengerId|Survived|Pclass|Name|Sex|Age|SibSp|Parch|Ticket|Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|          0|       0|     0|   0|  0|177|    0|    0|     0|   0|  687|       2|\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "\n",
      "The type of dataframes is: <class 'pyspark.sql.dataframe.DataFrame'>\n",
      "size of df1 is: 891\n",
      "size of df2 is: 891\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 4 rows\n",
      "\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_if_there_are_null_values(df):\n",
    "    \"\"\"\" Search for null values ()= 0) for each column of given DataFrame. \n",
    "    \n",
    "    Details:\n",
    "        - For each column \"c\":\n",
    "            - counts the number of NULLs ('when' condition returns the column value)\n",
    "            - alias the results with the original column name\n",
    "\n",
    "    Returns:\n",
    "        - Selection of rows from the df with null values [pyspark.sql.dataframe.DataFrame]\n",
    "    \"\"\"    \n",
    "    df.select([count(when(col(c).isNull(),c)).alias(c) for c in df.columns]).show()\n",
    "    return df\n",
    "\n",
    "def check_if_there_are_nan_values(df):\n",
    "    df.select([col_name for col_name in df.columns if df.select(isnan(col(col_name))).collect()[0][0]])\n",
    "    return df\n",
    "\n",
    "df1 = check_if_there_are_null_values(data_titanic_raw)\n",
    "df2 = check_if_there_are_nan_values(data_titanic_raw)\n",
    "\n",
    "print(\"The type of dataframes is: {}\".format(type(df1)))\n",
    "print(f\"size of df1 is: {df1.count()}\")\n",
    "print(f\"size of df2 is: {df2.count()}\")\n",
    "\n",
    "df1.show(4), df2.show(4)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#0FCBC6\"> => Preprocessing </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocess_transform(Transformer, HasOutputCols, DefaultParamsReadable, DefaultParamsWritable):\n",
    "\t\"\"\" Custom Transformer class, to execute the whole process of refining data, preparing data in the proper way. \\\\\n",
    "\t\tIt implements sklean-style fit/transform methods for use in ML pipelines.\n",
    "\t\"\"\"\t\n",
    "\tvalue = Param(Params._dummy(),\"value\",\"value to fill\")\n",
    "\t\"\"\"\n",
    "\t@keyword_only\n",
    "\tdef __init__(self, df, outputCols=None): \n",
    "\t\tsuper(preprocess_transform, self).__init__()\n",
    "\t\tself.df = df\n",
    "\t\tkwargs = self._input_kwargs\n",
    "\t\tself._set(**kwargs)\n",
    "\t\"\"\"\n",
    "\t@keyword_only\n",
    "\tdef __init__(self, df, outputCols=None):\n",
    "\t\tsuper(preprocess_transform, self).__init__()\n",
    "\n",
    "\t\t# Define _input_kwargs\n",
    "\t\tself._input_kwargs = {}\n",
    "\n",
    "\t\tself.df = df\n",
    "\t\tkwargs = self._input_kwargs\n",
    "\t\tself._set(**kwargs)\n",
    "\n",
    "\t@keyword_only\n",
    "\tdef setParams(self, outputCols=None):\n",
    "\t\t\"\"\" Set the output columns for the transformer.\n",
    "\n",
    "\t\tParameters:\n",
    "\t\t\t- List of output columns after transformation [list]\n",
    "\t\tReturns: \n",
    "\t\t\t- \n",
    "\t\t\"\"\"\n",
    "\t\tkwargs = self._input_kwargs\n",
    "\t\treturn self._set(**kwargs)\n",
    "\n",
    "\tdef setValue(self, value):\n",
    "\t\t\"\"\" Set the value to use for imputing missing values.\n",
    "\n",
    "\t\tParameters:\n",
    "\t\t\t- Value to use for missing value imputation\n",
    "\t\tReturns: \n",
    "\t\t\t- \n",
    "\t\t\"\"\"\n",
    "\t\treturn self._set(value=value)\n",
    "\n",
    "\tdef getValue(self):\n",
    "\t\t\"\"\" Get the currently set value for missing value imputation. \"\"\"\n",
    "\t\treturn self.getOrDefault(self.value)\n",
    "\t\t\n",
    "\tdef feature_generation(self, df):\n",
    "\t\t\"\"\" Generate new feature columns. \n",
    "\t\t\n",
    "\t\tParameters:\n",
    "\t\t\t- DataFrame to check and adjust [pyspark.sql.dataframe.DataFrame]\n",
    "\n",
    "\t\tDetails:\n",
    "\t\t\t- Genearate the following new features:\n",
    "\t\t\t\t- Initial: Extract title from Name as new Initial column:\n",
    "\t\t\t\t\t- Extract the first word (before a dot) in the \"Name\" column as a new column \"Initial\" using a regex pattern.\n",
    "\t\t\t\t\t- pattern => ([A-Za-z]+) matches one or more alphabetic characters\n",
    "\t\t\t\t- Normalized titles: Change titles into standard categories\n",
    "\t\t\t\t- Family_Size: Adds family size column by adding SibSp and Parch columns\n",
    "\t\t\t\t- Alone: Adds indicator for traveling alone. \\\\\n",
    "\t\t\t\t\tlit() takes a value as input and wraps it into a Column expression.\t\n",
    "\t\tReturns:\n",
    "\t\t\t- Tranformed DataFrame with imputed data [pyspark.sql.dataframe.DataFrame]\t\t\t\n",
    "\t\t\n",
    "\n",
    "\t\t\"\"\"\n",
    "\t\t# Extract title from Name as new Initial column\n",
    "\t\tdf = df.withColumn(\"Initial\", regexp_extract(col(\"Name\"),\"([A-Za-z]+)\\.\",1))\n",
    "\t\t\n",
    "\t\t# Change titles\n",
    "\t\tdf = df.replace(['Mlle','Mme', 'Ms', 'Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don'],\n",
    "\t\t\t\t\t\t['Miss','Miss','Miss','Mr','Mr',  'Mrs',  'Mrs',  'Other',\t'Other','Other','Mr','Mr','Mr'])\n",
    "\t\t\n",
    "\t\t# Create Family_Size \n",
    "\t\tdf = df.withColumn(\"Family_Size\", col('SibSp') + col('Parch'))\n",
    "\t\t# Create Alone indicator (a literal/constant value).\t\t\n",
    "\t\tdf = df.withColumn('Alone', lit(0))\n",
    "\t\t# Set Alone to 1 if Family_Size is 0, to 0 otherwise\n",
    "\t\tdf = df.withColumn(\"Alone\",when(df[\"Family_Size\"] ==0, 1).otherwise(df[\"Alone\"]))\n",
    "\t\t\n",
    "\t\treturn df\n",
    "\t\n",
    "\tdef Age_impute(self, df):\n",
    "\t\t\"\"\" Impute missing age values based on the mean age for each title.\n",
    "\t\t\n",
    "\t\tParameters:\n",
    "\t\t\t- Input DataFrame to check and adjust [pyspark.sql.dataframe.DataFrame]\n",
    "\n",
    "\t\tDetails:\n",
    "\t\t\t- Calculate mean age for each title using groupby\n",
    "\t\t\t- Rename column to semantic name\n",
    "\t\t\t- Get list of titles for Initial adn Age features\n",
    "\t\t\t\t- Convert the df to an RDD to leverage RDD operations, flattens into a 1D RDD, and collects the RDD rows into a Python list.\n",
    "\t\t\t- Impute age with mean if null, for each title\n",
    "\n",
    "\t\tReturns:\n",
    "\t\t\t- Tranformed DataFrame with imputed data [pyspark.sql.dataframe.DataFrame]\n",
    "\t\t\"\"\"\n",
    "\t\tAge_mean = df.groupBy(\"Initial\").avg('Age')\n",
    "\t\tAge_mean = Age_mean.withColumnRenamed('avg(Age)','mean_age')\n",
    "\t\t## Get lists \n",
    "\t\tInitials_list = Age_mean.select(\"Initial\").rdd.flatMap(lambda x: x).collect()\n",
    "\t\tMean_list = Age_mean.select(\"mean_age\").rdd.flatMap(lambda x: x).collect()\n",
    "\t\t\n",
    "\t\t# Iterate through the Initials_list and Mean_list simultaneously using zip and imputing the mean age if age is null for that title.\n",
    "\t\tfor i, j in zip(Initials_list, Mean_list):\n",
    "\t\t\tdf = df.withColumn(\"Age\",when((df[\"Initial\"] == i) & (df[\"Age\"].isNull()), j).otherwise(df[\"Age\"]))\n",
    "\t\treturn df\n",
    "\t\n",
    "\tdef Embark_impute(self, df):\n",
    "\t\t\"\"\" Impute missing embark values. \n",
    "\t\t\n",
    "\t\tParameters:\n",
    "\t\t\t- Dataframe to check and adjust [pyspark.sql.dataframe.DataFrame]\n",
    "\n",
    "\t\tDetails:\n",
    "\t\t\t- Groups the data by the Embarked column\n",
    "\t\t\t\t- count the rows for each embarkation point\n",
    "\t\t\t\t- sort descending by the counts\n",
    "\t\t\t\t- collect to the driver and takes the top result\n",
    "\t\t\t\n",
    "\t\t\t- fills NA values in the DataFrame. \\\\\n",
    "\t\t\t\t{'Embarked': mode_value} specifies to fill NAs in the Embarked column with the mode value\n",
    "\n",
    "\t\tReturns:\n",
    "\t\t\t- Tranformed Dataframe with imputed data pyspark.sql.dataframe.DataFrame]\t\t\t\n",
    "\t\t\"\"\"\n",
    "\t\tmode_value = df.groupBy('Embarked').count().sort(col('count').desc()).collect()[0][0]\n",
    "\t\tdf = df.fillna({'Embarked':mode_value})\n",
    "\t\treturn df\n",
    "\n",
    "\tdef Fare_impute(self, df):\n",
    "\t\t\"\"\" Impute missing fare values based on the average fare for each passenger class ('Pclass').\n",
    "\t\t\n",
    "\t\tParameters:\n",
    "\t\t\t- Data to check and adjust [pyspark.sql.dataframe.DataFrame]\n",
    "\n",
    "\t\tDetails:\n",
    "\t\t\t- For each passenger class 'i' in 'Pclass':\n",
    "\t\t\t\t- 1 Calculate average fare for that class:\n",
    "\t\t\t\t\t- Group by class and compute average fare\n",
    "\t\t\t\t\t- Select only the class and mean columns\n",
    "\t\t\t\t\t- Filter for the current class\n",
    "\t\t\t\t\t- Get the mean fare value\n",
    "\n",
    "\t\t\t\t- 2 Update 'Fare' column:\n",
    "\t\t\t\t\t- Check if fare is missing\n",
    "\t\t\t\t\t- Check if class matches\n",
    "\t\t\t\t\t- Impute mean fare value if above conditions met\n",
    "\t\t\t\t\t- Keep original if not missing\n",
    "\n",
    "\t\tReturns:\n",
    "\t\t\t- Tranformed df [pyspark.sql.dataframe.DataFrame]\t\t\t\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tSelect_pclass = df.filter(col('Fare').isNull()).select('Pclass')\n",
    "\t\tif Select_pclass.count() > 0:\n",
    "\t\t\tPclass = Select_pclass.rdd.flatMap(lambda x: x).collect()\n",
    "\t\t\tfor i in Pclass:\n",
    "\t\t\t\tmean_pclass_fare = df.groupBy('Pclass').mean().select('Pclass','avg(Fare)').filter(col('Pclass')== i).collect()[0][1]\n",
    "\t\t\t\tdf = df.withColumn(\"Fare\",when((col('Fare').isNull()) & (col('Pclass') == i), mean_pclass_fare).otherwise(col('Fare')))\n",
    "\t\treturn df\n",
    "\n",
    "\tdef all_impute_together(self, df):\n",
    "\t\t\"\"\" Impute all missing values.  \"\"\"\n",
    "\n",
    "\t\tdf = self.Age_impute(df)\n",
    "\t\tdf = self.Embark_impute(df)\n",
    "\t\tdf = self.Fare_impute(df)\n",
    "\t\treturn df\n",
    "\n",
    "\tdef stringToNumeric_conv(self, df, col_list):\n",
    "\t\t\"\"\" Convert categorical string columns to numeric. \n",
    "\t\tParameters:\n",
    "\t\t\t- Data to check and adjust [pyspark.sql.dataframe.DataFrame]\n",
    "\t\t\t- List of categorical columns to convert [list]\n",
    "\n",
    "\t\tDetails:\n",
    "\t\t\t- Create a StringIndexer for each column (with list ch√¨omprehension) to encode string columns \\\\\n",
    "\t\t\t\tFit the StringIndexer on the DataFrame\n",
    "\t\t\t- Create pipeline with the indexers \n",
    "\t\t\t\tPipeline() chains together transformations into a pipeline\n",
    "\t\t\t- Fit the Pipeline and transform the DataFrame \n",
    "\n",
    "\t\tReturns:\n",
    "\t\t\t- Converted DataFrame [pyspark.sql.dataframe.DataFrame]\t\t\t\t\n",
    "\t\t\"\"\"\n",
    "\t\tindexer = [StringIndexer(inputCol=column,outputCol=column+\"_index\").fit(df) for column in col_list]\n",
    "\t\tstring_change_pipeline = Pipeline(stages=indexer)\n",
    "\t\tdf = string_change_pipeline.fit(df).transform(df)\n",
    "\t\treturn df\n",
    "\n",
    "\tdef drop_column(self, df, col_list):\n",
    "\t\t\"\"\" Remove given columns entirely. \"\"\"\n",
    "\t\tfor i in col_list:\n",
    "\t\t\tdf = df.drop(col(i))\n",
    "\t\treturn df\n",
    "\n",
    "\tdef _transform(self):\n",
    "\t\t\"\"\" Main transformation logic. Apply all feature engineering steps and transformations to the DataFrame. \"\"\"\t\t\n",
    "\t\tprint(\"...pending tranformations...\")\n",
    "\n",
    "\t\tcol_list = [\"Sex\",\"Embarked\",\"Initial\"]\n",
    "\t\tdataset = self.feature_generation(self.df)\n",
    "\t\tdf_impute = self.all_impute_together(dataset)\n",
    "\t\tdf_numeric = self.stringToNumeric_conv(df_impute, col_list)\n",
    "\t\tdf_final = self.drop_column(df_numeric, ['Cabin','Name','Ticket','Family_Size','SibSp','Parch','Sex','Embarked','Initial'])\n",
    "\t\t\n",
    "\t\tprint(\"Done.\")\n",
    "\t\treturn df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...pending tranformations...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# If not on Google Colab, works only with less samples (10% of the original data)\n",
    "data_titanic_raw = data_titanic_raw.sample(fraction=0.1) \n",
    "my_model = preprocess_transform(df=data_titanic_raw)\n",
    "\n",
    "dataframe_final = my_model._transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+------------------+------+-----+---------+--------------+-------------+\n",
      "|PassengerId|Survived|Pclass|               Age|  Fare|Alone|Sex_index|Embarked_index|Initial_index|\n",
      "+-----------+--------+------+------------------+------+-----+---------+--------------+-------------+\n",
      "|          3|       1|     3|              26.0| 7.925|    1|      1.0|           0.0|          1.0|\n",
      "|         18|       1|     2|35.166666666666664|  13.0|    1|      0.0|           0.0|          0.0|\n",
      "|         30|       0|     3|35.166666666666664|7.8958|    1|      0.0|           0.0|          0.0|\n",
      "|         42|       0|     2|              27.0|  21.0|    0|      1.0|           0.0|          2.0|\n",
      "+-----------+--------+------+------------------+------+-----+---------+--------------+-------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe_final.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+------------------+-------+-----+---------+--------------+-------------+\n",
      "|PassengerId|Survived|Pclass|               Age|   Fare|Alone|Sex_index|Embarked_index|Initial_index|\n",
      "+-----------+--------+------+------------------+-------+-----+---------+--------------+-------------+\n",
      "|          3|       1|     3|              26.0|  7.925|    1|      1.0|           0.0|          1.0|\n",
      "|         18|       1|     2|35.166666666666664|   13.0|    1|      0.0|           0.0|          0.0|\n",
      "|         30|       0|     3|35.166666666666664| 7.8958|    1|      0.0|           0.0|          0.0|\n",
      "|         49|       0|     3|35.166666666666664|21.6792|    0|      0.0|           1.0|          0.0|\n",
      "+-----------+--------+------+------------------+-------+-----+---------+--------------+-------------+\n",
      "only showing top 4 rows\n",
      "\n",
      "+-----------+--------+------+------------------+------+-----+---------+--------------+-------------+\n",
      "|PassengerId|Survived|Pclass|               Age|  Fare|Alone|Sex_index|Embarked_index|Initial_index|\n",
      "+-----------+--------+------+------------------+------+-----+---------+--------------+-------------+\n",
      "|         42|       0|     2|              27.0|  21.0|    0|      1.0|           0.0|          2.0|\n",
      "|         63|       0|     1|              45.0|83.475|    0|      0.0|           0.0|          0.0|\n",
      "|         88|       0|     3|35.166666666666664|  8.05|    1|      0.0|           0.0|          0.0|\n",
      "|         94|       0|     3|              26.0|20.575|    0|      0.0|           0.0|          0.0|\n",
      "+-----------+--------+------+------------------+------+-----+---------+--------------+-------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split dataset after transformation \n",
    "train_final, test_final = dataframe_final.randomSplit([0.7, 0.3]) \n",
    "train_final.show(4), test_final.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge multiple columns into a vector column, with a feature transformer assembler\n",
    "feature = VectorAssembler(inputCols=['Pclass','Age','Fare','Alone','Sex_index','Embarked_index','Initial_index'],outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Classifiers \"\"\"\n",
    "lr = LogisticRegression(labelCol='Survived',featuresCol='features')\n",
    "rf = RandomForestClassifier(labelCol=\"Survived\", featuresCol=\"features\", numTrees=10)\n",
    "gb = GBTClassifier(labelCol=\"Survived\", featuresCol=\"features\", maxIter=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#0FCBC6\"> => Grid Param Search for 3 classification techniques </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[PassengerId: int, Survived: int, Pclass: int, Age: double, Fare: double, Alone: int, Sex_index: double, Embarked_index: double, Initial_index: double, features: vector, rawPrediction: vector, probability: vector, prediction: double]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" 1) Pipeline for Logistic Regression. \n",
    "Pipeline stages are:\n",
    "+ initilization\n",
    "+ fit \n",
    "+ transform. \n",
    "\"\"\"\n",
    "pipeline = Pipeline(stages=[feature, lr])\n",
    "\n",
    "paramGrid = ParamGridBuilder().addGrid(lr.regParam, [0.1, 0.01]).addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]).build()\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "crossval = CrossValidator(estimator = pipeline,\n",
    "                        estimatorParamMaps = paramGrid,\n",
    "                        evaluator = evaluator,\n",
    "                        numFolds = 3) \n",
    "\n",
    "## Run cross-validation, and choose the best set of parameters\n",
    "cvModel = crossval.fit(train_final)\n",
    "prediction = cvModel.transform(train_final)\n",
    "prediction.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7589495850365416,\n",
       " 0.7589495850365416,\n",
       " 0.7589495850365416,\n",
       " 0.7461290722160286,\n",
       " 0.7589495850365416,\n",
       " 0.7589495850365416,\n",
       " 0.7589495850365416,\n",
       " 0.7589495850365416,\n",
       " 0.77746810355506,\n",
       " 0.77746810355506,\n",
       " 0.77746810355506,\n",
       " 0.77746810355506]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average metrics on the holdout folds for Logistic Regression. \n",
    "cvModel.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/18 15:42:31 WARN DAGScheduler: Broadcasting large task binary with size 1054.1 KiB\n",
      "23/08/18 15:42:34 WARN DAGScheduler: Broadcasting large task binary with size 1140.3 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+------------------+-------+-----+---------+--------------+-------------+--------------------+--------------------+--------------------+----------+\n",
      "|PassengerId|Survived|Pclass|               Age|   Fare|Alone|Sex_index|Embarked_index|Initial_index|            features|       rawPrediction|         probability|prediction|\n",
      "+-----------+--------+------+------------------+-------+-----+---------+--------------+-------------+--------------------+--------------------+--------------------+----------+\n",
      "|          3|       1|     3|              26.0|  7.925|    1|      1.0|           0.0|          1.0|[3.0,26.0,7.925,1...|[28.9016284899647...|[0.28901628489964...|       1.0|\n",
      "|         18|       1|     2|35.166666666666664|   13.0|    1|      0.0|           0.0|          0.0|[2.0,35.166666666...|[34.6597009457690...|[0.34659700945769...|       1.0|\n",
      "|         30|       0|     3|35.166666666666664| 7.8958|    1|      0.0|           0.0|          0.0|[3.0,35.166666666...|[96.7520337363687...|[0.96752033736368...|       0.0|\n",
      "|         49|       0|     3|35.166666666666664|21.6792|    0|      0.0|           1.0|          0.0|[3.0,35.166666666...|[69.9163123827833...|[0.69916312382783...|       0.0|\n",
      "|         54|       1|     2|              29.0|   26.0|    0|      1.0|           0.0|          2.0|[2.0,29.0,26.0,0....|[2.88851138541540...|[0.02888511385415...|       1.0|\n",
      "+-----------+--------+------+------------------+-------+-----+---------+--------------+-------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 2) Pipeline for Random Forest. \n",
    "Pipeline stages are:\n",
    "+ initilization\n",
    "+ fit \n",
    "+ transform. \n",
    "\"\"\"\n",
    "pipeline = Pipeline(stages=[feature, rf])\n",
    "\n",
    "paramGrid = ParamGridBuilder().addGrid(rf.numTrees, [100,300]).build()\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "crossval = CrossValidator(estimator = pipeline,\n",
    "                        estimatorParamMaps = paramGrid,\n",
    "                        evaluator = evaluator,\n",
    "                        numFolds = 3)  \n",
    "\n",
    "# Run cross-validation\n",
    "cvModel = crossval.fit(train_final)\n",
    "prediction = cvModel.transform(train_final)\n",
    "prediction.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7589495850365416,\n",
       " 0.7589495850365416,\n",
       " 0.7589495850365416,\n",
       " 0.7461290722160286,\n",
       " 0.7589495850365416,\n",
       " 0.7589495850365416,\n",
       " 0.7589495850365416,\n",
       " 0.7589495850365416,\n",
       " 0.77746810355506,\n",
       " 0.77746810355506,\n",
       " 0.77746810355506,\n",
       " 0.77746810355506]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average metrics on the holdout folds for Random Forest.\n",
    "cvModel.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/18 15:42:47 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 60 to 44 (= number of training instances)\n",
      "23/08/18 15:42:49 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 60 to 44 (= number of training instances)\n",
      "23/08/18 15:43:09 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 60 to 44 (= number of training instances)\n",
      "23/08/18 15:43:12 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 60 to 44 (= number of training instances)\n",
      "23/08/18 15:43:26 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 60 to 44 (= number of training instances)\n",
      "23/08/18 15:43:30 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 60 to 44 (= number of training instances)\n",
      "23/08/18 15:43:41 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 60 to 49 (= number of training instances)\n",
      "23/08/18 15:43:43 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 60 to 49 (= number of training instances)\n",
      "23/08/18 15:43:54 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 60 to 49 (= number of training instances)\n",
      "23/08/18 15:43:57 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 60 to 49 (= number of training instances)\n",
      "23/08/18 15:44:13 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 60 to 49 (= number of training instances)\n",
      "23/08/18 15:44:18 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 60 to 49 (= number of training instances)\n",
      "23/08/18 15:44:30 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 60 to 41 (= number of training instances)\n",
      "23/08/18 15:44:32 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 60 to 41 (= number of training instances)\n",
      "23/08/18 15:44:42 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 60 to 41 (= number of training instances)\n",
      "23/08/18 15:44:45 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 60 to 41 (= number of training instances)\n",
      "23/08/18 15:45:02 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 60 to 41 (= number of training instances)\n",
      "23/08/18 15:45:06 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 60 to 41 (= number of training instances)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[PassengerId: int, Survived: int, Pclass: int, Age: double, Fare: double, Alone: int, Sex_index: double, Embarked_index: double, Initial_index: double, features: vector, rawPrediction: vector, probability: vector, prediction: double]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" 3) Pipeline for Gradient Boosting. \n",
    "Pipeline stages are:\n",
    "+ initilization\n",
    "+ fit \n",
    "+ transform. \n",
    "\"\"\"\n",
    "pipeline = Pipeline(stages=[feature, gb])\n",
    "\n",
    "paramGrid = ParamGridBuilder().addGrid(gb.maxDepth, [2, 4, 6]).addGrid(gb.maxBins, [20, 60]).addGrid(gb.maxIter, [10, 20]).build()\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "crossval = CrossValidator(estimator = pipeline,\n",
    "                        estimatorParamMaps = paramGrid,\n",
    "                        evaluator= evaluator,\n",
    "                        numFolds = 3)  \n",
    "\n",
    "# Run cross-validation\n",
    "cvModel = crossval.fit(train_final)\n",
    "prediction = cvModel.transform(train_final)\n",
    "prediction.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7589495850365416,\n",
       " 0.7589495850365416,\n",
       " 0.7589495850365416,\n",
       " 0.7461290722160286,\n",
       " 0.7589495850365416,\n",
       " 0.7589495850365416,\n",
       " 0.7589495850365416,\n",
       " 0.7589495850365416,\n",
       " 0.77746810355506,\n",
       " 0.77746810355506,\n",
       " 0.77746810355506,\n",
       " 0.77746810355506]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average metrics on the holdout folds for Gradient Boosting. \n",
    "cvModel.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store model \n",
    "cvModel.bestModel.write().overwrite().save('./datasets_for_pyspark/Titanic_model_saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+------------------+-------+-----+---------+--------------+-------------+--------------------+--------------------+--------------------+----------+\n",
      "|PassengerId|Survived|Pclass|               Age|   Fare|Alone|Sex_index|Embarked_index|Initial_index|            features|       rawPrediction|         probability|prediction|\n",
      "+-----------+--------+------+------------------+-------+-----+---------+--------------+-------------+--------------------+--------------------+--------------------+----------+\n",
      "|         42|       0|     2|              27.0|   21.0|    0|      1.0|           0.0|          2.0|[2.0,27.0,21.0,0....|[-1.3259026792203...|[0.06587782434721...|       1.0|\n",
      "|         63|       0|     1|              45.0| 83.475|    0|      0.0|           0.0|          0.0|(7,[0,1,2],[1.0,4...|[-0.9424782990884...|[0.13182058510852...|       1.0|\n",
      "|         88|       0|     3|35.166666666666664|   8.05|    1|      0.0|           0.0|          0.0|[3.0,35.166666666...|[-0.0270877719365...|[0.48645942565675...|       1.0|\n",
      "|         94|       0|     3|              26.0| 20.575|    0|      0.0|           0.0|          0.0|(7,[0,1,2],[3.0,2...|[2.11625482626405...|[0.98569178198062...|       0.0|\n",
      "|        178|       0|     1|              50.0|28.7125|    1|      1.0|           1.0|          1.0|[1.0,50.0,28.7125...|[-1.3259026792203...|[0.06587782434721...|       1.0|\n",
      "|        179|       0|     2|              30.0|   13.0|    1|      0.0|           0.0|          0.0|[2.0,30.0,13.0,1....|[0.34790328321397...|[0.66725737913379...|       0.0|\n",
      "|        181|       0|     3| 22.76923076923077|  69.55|    0|      1.0|           0.0|          1.0|[3.0,22.769230769...|[1.32590267922033...|[0.93412217565278...|       0.0|\n",
      "|        296|       0|     1|35.166666666666664|27.7208|    1|      0.0|           1.0|          0.0|[1.0,35.166666666...|[-1.5775854144097...|[0.04088801592568...|       1.0|\n",
      "|        317|       1|     2|              24.0|   26.0|    0|      1.0|           0.0|          2.0|[2.0,24.0,26.0,0....|[-1.3259026792203...|[0.06587782434721...|       1.0|\n",
      "|        371|       1|     1|              25.0|55.4417|    0|      0.0|           1.0|          0.0|[1.0,25.0,55.4417...|[1.32590267922033...|[0.93412217565278...|       0.0|\n",
      "+-----------+--------+------+------------------+-------+-----+---------+--------------+-------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load trained model to perform prediction => Returns the entire df with \"prediction\" column like before\n",
    "my_model_laoded = PipelineModel.load('./datasets_for_pyspark/Titanic_model_saved')\n",
    "predictions = my_model_laoded.transform(test_final)\n",
    "predictions.show(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#0FCBC6\"> => Metrics </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auroc is : 0.736\n",
      "aurpr is : 0.2162962962962963\n"
     ]
    }
   ],
   "source": [
    "# Rename last column since BinaryClassificationEvaluator expects a column named \"label\" by default, \n",
    "predictions_renamed = predictions.withColumnRenamed(\"Survived\", \"label\")\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "\n",
    "# Area under the ROC Curve\n",
    "auroc = evaluator.evaluate(predictions_renamed, {evaluator.metricName: \"areaUnderROC\"})\n",
    "# Area under the precision-recall curve\n",
    "aurpr = evaluator.evaluate(predictions_renamed, {evaluator.metricName: \"areaUnderPR\"})\n",
    "\n",
    "print(f\"auroc is : {auroc}\")\n",
    "print(f\"aurpr is : {aurpr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy on Pandas DataFrame for comparison\n",
    "\n",
    "pred_pdf = predictions.toPandas()\n",
    "accuracy = accuracy_score(pred_pdf['Survived'], pred_pdf['prediction'])\n",
    "print(f\"The accuracy is {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
