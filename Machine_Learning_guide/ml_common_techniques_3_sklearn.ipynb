{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"line-height:0.5\">\n",
    "<h1 style=\"color:#B0EE8F \">  Common practices in Machine Learning 3 </h1>\n",
    "</div>\n",
    "<div style=\"line-height:1.2\">\n",
    "<h4>  11 examples based on Scikit-learn for classification and regression. <br> \n",
    "Focus on Imputers, ColumnTransformers, CountVectorizers, Pipeline. </h4>\n",
    "</div>\n",
    "<div style=\"margin-top: -15px;\">\n",
    "<span style=\"display: inline-block;\">\n",
    "    <h3 style=\"color: lightblue; display: inline;\">Keywords:</h3> fetch_openml + enable_iterative_imputer + pandas isna() + Pipeline / make_pipeline + statistics_\n",
    "</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import copy \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import fetch_openml, load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer, make_column_selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" IterativeImputer.\n",
    "It is experimental, the API might change without any deprecation cycle. \n",
    "N.B.\n",
    "To use it, you need to explicitly import enable_iterative_imputer.\n",
    "\"\"\"\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer      \n",
    "# from sklearn.experimental import enable_hist_gradient_boosting        #useless now, but once was used\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"line-height:0.5\">\n",
    "<h2 style=\"color:#B0EE8F \">  # 1) colum Tranformer </h2>\n",
    "</div>\n",
    "Use columTranformer to apply different preprocessing to different columns: <br>\n",
    "    - select from DataFrame columns by name <br>\n",
    "    - passthrough or drop unspecified columns <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Fare Embarked     Sex  Age\n",
      "0   28.84        S    male   73\n",
      "1   39.76        C  female   45\n",
      "2   90.45        C  female   46\n",
      "3   91.01        S  female   79\n",
      "4   64.45        C  female   37\n",
      "5   96.66        Q  female   53\n",
      "6   73.52        S  female   79\n",
      "7   75.98        C    male   26\n",
      "8   89.20        S  female   23\n",
      "9   21.29        C    male   78\n",
      "10  66.98        S    male   56\n",
      "11  66.84        C    male   50\n",
      "12  25.16        Q  female   67\n",
      "13  64.86        C    male   65\n",
      "14  72.04        Q    male   23\n",
      "15  38.14        Q  female   24\n",
      "16  89.00        C  female   46\n",
      "17  47.72        C    male   62\n",
      "18  15.85        Q  female   26\n",
      "19  85.47        S    male   30\n",
      "20  12.38        S    male   24\n",
      "21  85.27        S  female   35\n",
      "22  20.22        C    male   76\n",
      "23  85.76        Q    male   22\n",
      "24  69.79        C  female   54\n",
      "25  75.60        S    male   47\n",
      "26  23.75        C  female   67\n",
      "27  43.07        C  female   74\n",
      "28  93.95        S    male   37\n",
      "29  23.31        S  female   67\n",
      "30  37.20        S  female   62\n",
      "31  34.94        Q    male   30\n",
      "32  79.89        S    male   73\n",
      "33  61.62        Q  female   20\n",
      "34  61.07        Q  female   25\n",
      "35  60.68        Q    male   22\n",
      "36  37.39        S    male   46\n",
      "37  65.68        S    male   70\n",
      "38  64.36        C    male   37\n",
      "39  89.77        C    male   72\n"
     ]
    }
   ],
   "source": [
    "fare = np.random.uniform(10.0, 100.0, size=40)  \n",
    "embarked = np.random.choice(['S', 'C', 'Q'], size=40)\n",
    "sex = np.random.choice(['male', 'female'], size=40)\n",
    "age = np.random.randint(18, 80, size=40)\n",
    "\n",
    "# Create the dataframe object\n",
    "df = pd.DataFrame({'Fare': fare, 'Embarked': embarked, 'Sex': sex, 'Age': age})\n",
    "df['Fare'] = df['Fare'].round(2)\n",
    "# Print the dataframe\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.  ,  0.  ,  1.  ,  0.  ,  1.  , 73.  , 28.84],\n",
       "       [ 1.  ,  0.  ,  0.  ,  1.  ,  0.  , 45.  , 39.76],\n",
       "       [ 1.  ,  0.  ,  0.  ,  1.  ,  0.  , 46.  , 90.45],\n",
       "       [ 0.  ,  0.  ,  1.  ,  1.  ,  0.  , 79.  , 91.01],\n",
       "       [ 1.  ,  0.  ,  0.  ,  1.  ,  0.  , 37.  , 64.45],\n",
       "       [ 0.  ,  1.  ,  0.  ,  1.  ,  0.  , 53.  , 96.66],\n",
       "       [ 0.  ,  0.  ,  1.  ,  1.  ,  0.  , 79.  , 73.52],\n",
       "       [ 1.  ,  0.  ,  0.  ,  0.  ,  1.  , 26.  , 75.98],\n",
       "       [ 0.  ,  0.  ,  1.  ,  1.  ,  0.  , 23.  , 89.2 ],\n",
       "       [ 1.  ,  0.  ,  0.  ,  0.  ,  1.  , 78.  , 21.29],\n",
       "       [ 0.  ,  0.  ,  1.  ,  0.  ,  1.  , 56.  , 66.98],\n",
       "       [ 1.  ,  0.  ,  0.  ,  0.  ,  1.  , 50.  , 66.84],\n",
       "       [ 0.  ,  1.  ,  0.  ,  1.  ,  0.  , 67.  , 25.16],\n",
       "       [ 1.  ,  0.  ,  0.  ,  0.  ,  1.  , 65.  , 64.86],\n",
       "       [ 0.  ,  1.  ,  0.  ,  0.  ,  1.  , 23.  , 72.04],\n",
       "       [ 0.  ,  1.  ,  0.  ,  1.  ,  0.  , 24.  , 38.14],\n",
       "       [ 1.  ,  0.  ,  0.  ,  1.  ,  0.  , 46.  , 89.  ],\n",
       "       [ 1.  ,  0.  ,  0.  ,  0.  ,  1.  , 62.  , 47.72],\n",
       "       [ 0.  ,  1.  ,  0.  ,  1.  ,  0.  , 26.  , 15.85],\n",
       "       [ 0.  ,  0.  ,  1.  ,  0.  ,  1.  , 30.  , 85.47],\n",
       "       [ 0.  ,  0.  ,  1.  ,  0.  ,  1.  , 24.  , 12.38],\n",
       "       [ 0.  ,  0.  ,  1.  ,  1.  ,  0.  , 35.  , 85.27],\n",
       "       [ 1.  ,  0.  ,  0.  ,  0.  ,  1.  , 76.  , 20.22],\n",
       "       [ 0.  ,  1.  ,  0.  ,  0.  ,  1.  , 22.  , 85.76],\n",
       "       [ 1.  ,  0.  ,  0.  ,  1.  ,  0.  , 54.  , 69.79],\n",
       "       [ 0.  ,  0.  ,  1.  ,  0.  ,  1.  , 47.  , 75.6 ],\n",
       "       [ 1.  ,  0.  ,  0.  ,  1.  ,  0.  , 67.  , 23.75],\n",
       "       [ 1.  ,  0.  ,  0.  ,  1.  ,  0.  , 74.  , 43.07],\n",
       "       [ 0.  ,  0.  ,  1.  ,  0.  ,  1.  , 37.  , 93.95],\n",
       "       [ 0.  ,  0.  ,  1.  ,  1.  ,  0.  , 67.  , 23.31],\n",
       "       [ 0.  ,  0.  ,  1.  ,  1.  ,  0.  , 62.  , 37.2 ],\n",
       "       [ 0.  ,  1.  ,  0.  ,  0.  ,  1.  , 30.  , 34.94],\n",
       "       [ 0.  ,  0.  ,  1.  ,  0.  ,  1.  , 73.  , 79.89],\n",
       "       [ 0.  ,  1.  ,  0.  ,  1.  ,  0.  , 20.  , 61.62],\n",
       "       [ 0.  ,  1.  ,  0.  ,  1.  ,  0.  , 25.  , 61.07],\n",
       "       [ 0.  ,  1.  ,  0.  ,  0.  ,  1.  , 22.  , 60.68],\n",
       "       [ 0.  ,  0.  ,  1.  ,  0.  ,  1.  , 46.  , 37.39],\n",
       "       [ 0.  ,  0.  ,  1.  ,  0.  ,  1.  , 70.  , 65.68],\n",
       "       [ 1.  ,  0.  ,  0.  ,  0.  ,  1.  , 37.  , 64.36],\n",
       "       [ 1.  ,  0.  ,  0.  ,  0.  ,  1.  , 72.  , 89.77]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Create a ColumnTransformer object using scikit-learn's make_column_transformer() function, \n",
    "and then applying it to a pandas dataframe df.\n",
    "\n",
    "1. Apply one-hot encoding to the 'Embarked' and 'Sex' columns, and fills in missing values in the 'Age' column \n",
    "with the mean value of the non-missing values. \n",
    "2. Concatenate the transformed columns with the original columns (since remainder='passthrough'), \n",
    "and returns the resulting numpy array.        \n",
    "\n",
    "N.B.\n",
    "a) OneHotEncoder transformer is used to encode categorical features as one-hot numeric arrays\n",
    "b) SimpleImputer object (a transformer) fills in missing (nan) values in the input data. \n",
    "Not specify a value for the strategy parameter mean to use the default strategy of 'mean'.\n",
    "C) The ColumnTransformer object is used to apply different transformations to different columns of the input data. \n",
    "    __two transformers: ohe and imp. \n",
    "    The ohe transformer is applied to the 'Embarked' and 'Sex' columns, and the imp transformer is applied to the 'Age' column. \n",
    "    The remainder='passthrough' parameter specifies that any columns not explicitly transformed should be passed through unchanged.\n",
    "        All remaining columns that were not specified in `transformers` will be automatically passed through. \n",
    "        This subset of columns is concatenated with the output of the transformers.\n",
    "\n",
    "d) fit_transform() => Apply the ColumnTransformer object ct to the input dataframe df. \n",
    "    To each transformer to the appropriate columns of the input data, producing a transformed output. \n",
    "    The output is a numpy array with the transformed data.\n",
    "\"\"\"\n",
    "ohe = OneHotEncoder()\n",
    "imp = SimpleImputer()\n",
    "# imp = SimpleImputer(strategy='mean') # fill in missing values with the mean of each column\n",
    "\n",
    "ct = make_column_transformer((ohe, ['Embarked', 'Sex']), (imp, ['Age']), remainder='passthrough') \n",
    "ct.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"line-height:0.5\">\n",
    "<h2 style=\"color:#B0EE8F \">  # 2) Seven ways to select columns using ColumnTransformer  </h2>\n",
    "</div>\n",
    "\n",
    "- column name \n",
    "- integer position\n",
    "- slice\n",
    "- boolean mask\n",
    "- regex pattern\n",
    "- dtypes to include\n",
    "- dtypes to exclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ColumnTransformer(transformers=[('onehotencoder', OneHotEncoder(),\n",
      "                                 ['Embarked', 'Sex'])])\n",
      "ColumnTransformer(transformers=[('onehotencoder', OneHotEncoder(), [1, 2])])\n",
      "ColumnTransformer(transformers=[('onehotencoder', OneHotEncoder(),\n",
      "                                 slice(1, 3, None))])\n",
      "ColumnTransformer(transformers=[('onehotencoder', OneHotEncoder(),\n",
      "                                 [False, True, False, True])])\n",
      "ColumnTransformer(transformers=[('onehotencoder', OneHotEncoder(),\n",
      "                                 <sklearn.compose._column_transformer.make_column_selector object at 0x7f04a7517a30>)])\n",
      "ColumnTransformer(transformers=[('onehotencoder', OneHotEncoder(),\n",
      "                                 <sklearn.compose._column_transformer.make_column_selector object at 0x7f04a5fdd9f0>)])\n",
      "ColumnTransformer(transformers=[('onehotencoder', OneHotEncoder(),\n",
      "                                 <sklearn.compose._column_transformer.make_column_selector object at 0x7f04a5fdcd30>)])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 1.],\n",
       "       [1., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 1., 0.],\n",
       "       [0., 0., 1., 1., 0.],\n",
       "       [1., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 1., 0.],\n",
       "       [0., 0., 1., 1., 0.],\n",
       "       [1., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 1., 0.],\n",
       "       [1., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 1.],\n",
       "       [1., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 1., 0.],\n",
       "       [1., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 1.],\n",
       "       [0., 1., 0., 1., 0.],\n",
       "       [1., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 1., 0.],\n",
       "       [0., 0., 1., 0., 1.],\n",
       "       [0., 0., 1., 0., 1.],\n",
       "       [0., 0., 1., 1., 0.],\n",
       "       [1., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 1.],\n",
       "       [1., 0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0., 1.],\n",
       "       [1., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0., 1.],\n",
       "       [0., 0., 1., 1., 0.],\n",
       "       [0., 0., 1., 1., 0.],\n",
       "       [0., 1., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 1.],\n",
       "       [0., 1., 0., 1., 0.],\n",
       "       [0., 1., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 1.],\n",
       "       [0., 0., 1., 0., 1.],\n",
       "       [1., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2 \n",
    "\"\"\" Seven ways to select columns using ColumnTransformer \n",
    "\n",
    "\"\"\"\n",
    "ct = make_column_transformer((ohe, ['Embarked', 'Sex']))\n",
    "ct1 = make_column_transformer((ohe, [1, 2]))\n",
    "ct2 = make_column_transformer((ohe, slice(1,3)))\n",
    "ct3 = make_column_transformer((ohe, [False, True, False, True]))\n",
    "ct4 = make_column_transformer((ohe, make_column_selector(pattern='E|S')))\n",
    "ct5 = make_column_transformer((ohe, make_column_selector(dtype_include=object)))\n",
    "ct6 = make_column_transformer((ohe, make_column_selector(dtype_exclude='number')))\n",
    "\n",
    "print(ct)\n",
    "print(ct1)\n",
    "print(ct2)\n",
    "print(ct3)\n",
    "print(ct4)\n",
    "print(ct5)\n",
    "print(ct6)\n",
    "# to obtain the same result\n",
    "# ct.fit_transform(df)\n",
    "# ct1.fit_transform(df)\n",
    "# ct2.fit_transform(df)\n",
    "# ct3.fit_transform(df)\n",
    "# ct4.fit_transform(df)\n",
    "# ct5.fit_transform(df)\n",
    "ct6.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"line-height:0.5\">\n",
    "<h2 style=\"color:#B0EE8F \">  # 3) sklearn Tranformers methods </h2>\n",
    "</div>\n",
    "Models has only the fit method, Transformers has the transform method: <br>\n",
    "Difference between \"models\" and \"transformers\" object in sklearn: <br>\n",
    "&emsp; - fit = transformer learns something about the data <br>\n",
    "&emsp; - transform = use what it is learned to do the data tranformation <br>\n",
    "- CountVectorized <br>\n",
    "&emsp; - fit = learn vocabulary <br>\n",
    "&emsp; - transform = creates a document-term matrix using the vocabulary <br>\n",
    "- SimpleImputer <br>\n",
    "&emsp; - fit = learn the value to impute <br>\n",
    "&emsp; - transform = fills in missing entries using the imputation value <br>\n",
    "- StandartScarler <br>\n",
    "&emsp; - fit = learns the mean scale of each feature <br>\n",
    "&emsp; - transform = standaridizes the features using the mean and scale <br>\n",
    "- HashingVectorizer <br>\n",
    "&emsp; - fit = statelles transformer <br>\n",
    "&emsp; - transform = creates the document-term matrix using a hash of the token <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### => CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 3 1 2 0 1 2 0 2]\n",
      " [1 0 0 1 1 0 1 1 1]]\n",
      "Feature names are:\n",
      " ['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n"
     ]
    }
   ],
   "source": [
    "# Dummy list of sentences\n",
    "documents = [\n",
    "    \"This is the first document.\",\n",
    "    \"Is this the first document?\"\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "]\n",
    "# Create an instance of CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer on the documents and transform the documents into a document-term matrix\n",
    "# X is the counts of each word in the documents is stored as a sparse matrix where each row corresponds to a document, \n",
    "# and each column corresponds to a word in the vocabulary.\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Convert the matrix to a dense array\n",
    "print(X.toarray())\n",
    "\n",
    "# Get the feature names (words in the vocabulary)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(\"Feature names are:\\n\", feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### => StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized data:\n",
      " [[-1.22474487 -1.22474487 -1.22474487]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 1.22474487  1.22474487  1.22474487]]\n",
      "\n",
      "Mean of each feature:\n",
      "[4. 5. 6.]\n",
      "\n",
      "Standard deviation of each feature:\n",
      "[2.44948974 2.44948974 2.44948974]\n"
     ]
    }
   ],
   "source": [
    "data = np.array([\n",
    "    [1.0, 2.0, 3.0],\n",
    "    [4.0, 5.0, 6.0],\n",
    "    [7.0, 8.0, 9.0]\n",
    "])\n",
    "\n",
    "# Create an instance of StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Fit the scaler \n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "print(\"Standardized data:\\n\", scaled_data)\n",
    "print(\"\\nMean of each feature:\")\n",
    "print(scaler.mean_)\n",
    "print(\"\\nStandard deviation of each feature:\")\n",
    "print(scaler.scale_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### => HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.          0.          0.          0.\n",
      "  -0.57735027  0.57735027 -0.57735027  0.        ]\n",
      " [ 0.          0.         -0.30151134  0.          0.          0.30151134\n",
      "  -0.30151134  0.60302269 -0.60302269  0.        ]\n",
      " [ 0.          0.40824829  0.40824829  0.         -0.40824829 -0.40824829\n",
      "   0.          0.40824829 -0.40824829  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of HashingVectorizer\n",
    "vectorizer = HashingVectorizer(n_features=10)\n",
    "# Transform the documents into a document-term matrix\n",
    "X = vectorizer.transform(documents)\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#B0EE8F \">  # 4) Encoding </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Shape</th>\n",
       "      <th>Class</th>\n",
       "      <th>Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>circle</td>\n",
       "      <td>first</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>oval</td>\n",
       "      <td>second</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>square</td>\n",
       "      <td>third</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>triangle</td>\n",
       "      <td>fourth</td>\n",
       "      <td>XL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Shape   Class Size\n",
       "0    circle   first    S\n",
       "1      oval  second    M\n",
       "2    square   third    L\n",
       "3  triangle  fourth   XL"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array([\n",
    "    ['circle', 'first', 'S'],\n",
    "    ['oval', 'second', 'M'],\n",
    "    ['square', 'third', 'L'],\n",
    "    ['triangle', 'fourth', 'XL']])\n",
    "\n",
    "df1 = pd.DataFrame(data, columns=['Shape', 'Class', 'Size'])\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" 4.1 Dummy Econding -> no relation between class.\n",
    "N.B.\n",
    "Using \"ohe1 = OneHotEncoder(sparse=False)\" lead to: \"FutureWarning: `sparse` was renamed to `sparse_output` in newer versions\"\n",
    "When some categories are unknown (not present in the training set)...they can be replaced  (e.g. with zeros).\n",
    "\"\"\"\n",
    "ohe1 = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "ohe1.fit_transform(df1[['Shape']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [1., 1.],\n",
       "       [2., 2.],\n",
       "       [3., 3.]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" 4.2 Encode the single with Ordinal Encoder when order of features matters! \n",
    "there is also a relation between two values of rows ..like the class and the size that are always related! \n",
    "\"\"\"\n",
    "catego = [['first', 'second', 'third', 'fourth'], ['S','M','L','XL']]\n",
    "ore1 = OrdinalEncoder(categories=catego)\n",
    "ore1.fit_transform(df1[['Class', 'Size']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#B0EE8F \">  # 5) Imputing </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df\n",
      "   feat1 feat2 feat3 feat4 label\n",
      "0  10.0  20.0   nan   3.2     A\n",
      "1   1.4  32.9   5.0   nan     B\n",
      "2   nan   nan   nan   nan     C\n",
      "test_df\n",
      "    feat1  feat2  feat3  feat4  feat5  feat6\n",
      "0   10.0   20.0    NaN    3.2    NaN   91.2\n",
      "1    6.7    3.1    7.9   21.1    NaN    8.8\n",
      "\n",
      "df_88\n",
      "    feat1  feat2  feat3  feat4  feat5  feat6\n",
      "0   10.0   20.0    NaN    3.2    NaN   91.2\n",
      "1    6.7    3.1    7.9   21.1    NaN    8.8\n",
      "[[10.   12.3  20.    2.1   6.8   9.99 18.8   3.2  43.1  91.2 ]\n",
      " [32.3  12.3   1.5   2.1   6.8  12.1  18.8  74.2  43.1  91.2 ]]\n",
      "\n",
      "[[10.   12.3  20.    2.1   6.8   9.99 18.8   3.2  43.1  91.2 ]\n",
      " [32.3  12.3   1.5   2.1   6.8  12.1  18.8  74.2  43.1  91.2 ]]\n",
      "\n",
      "[[10.    0.   20.    2.1   6.8   9.99  0.    3.2   0.   91.2   1.    0.\n",
      "   0.    1.    1.    0.  ]\n",
      " [32.3  12.3   1.5   0.    0.   12.1  18.8  74.2  43.1   0.    0.    1.\n",
      "   1.    0.    0.    1.  ]]\n"
     ]
    }
   ],
   "source": [
    "data = np.array([\n",
    "    [10.0, 20.0, np.nan, 3.2, 'A'],\n",
    "    [1.4, 32.9, 5.0, np.nan, 'B'],\n",
    "    [np.nan, np.nan, np.nan, np.nan, 'C'],])\n",
    "\n",
    "data_test = np.array([\n",
    "    [10.0, 20.0, np.nan, 3.2, np.nan, 91.2],\n",
    "    [6.7, 3.1, 7.9, 21.1, np.nan, 8.8],])\n",
    "\n",
    "data_88 = np.array([\n",
    "    [10.0, np.nan, 20.0, 2.1, 6.8, 9.99, np.nan, 3.2, np.nan, 91.2],\n",
    "    [32.3, 12.3, 1.5, np.nan, np.nan, 12.1, 18.8, 74.2, 43.1, np.nan],])\n",
    "\n",
    "train_df = pd.DataFrame(data, columns=['feat1', 'feat2', 'feat3', 'feat4', 'label'])\n",
    "test_df = pd.DataFrame(data_test, columns=['feat1', 'feat2', 'feat3', 'feat4', 'feat5', 'feat6'])\n",
    "\n",
    "df_88 = pd.DataFrame(data_88, columns=['feat1', 'feat2', 'feat3', 'feat4', 'feat5', 'feat6', 'feat7', 'feat8', 'feat9', 'feat10']) \n",
    "\n",
    "print(\"train_df\\n {}\".format(train_df))\n",
    "print(\"test_df\\n {}\".format(test_df))\n",
    "print()\n",
    "print(\"df_88\\n {}\".format(test_df))\n",
    "\n",
    "imp_me = SimpleImputer(strategy='median')\n",
    "imp_mo = SimpleImputer(strategy='most_frequent')\n",
    "imp_co = SimpleImputer(strategy='constant', add_indicator=True)     #to indicate where the location of the substition!\n",
    "\n",
    "#### Fit_transform the inputer\n",
    "a = imp_me.fit_transform(df_88)\n",
    "b = imp_mo.fit_transform(df_88)\n",
    "c = imp_co.fit_transform(df_88)\n",
    "print(a)\n",
    "print()\n",
    "print(b)\n",
    "print()\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['A', 'A'], dtype=object)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputer = SimpleImputer()\n",
    "clf = LogisticRegression()\n",
    "# Create a 2-step pipelines that applies impute before fitting the classifier\n",
    "pipe = make_pipeline(imputer, clf) \n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pipe.fit(train_df[['feat1', 'feat2', 'feat3', 'feat4']], train_df['label'])\n",
    "\n",
    "# Use the pipeline to make predictions on the test data\n",
    "preds = pipe.predict(test_df[['feat1', 'feat2', 'feat3', 'feat4']])\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#B0EE8F \">  # 6) Pipeline </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"line-height:0.1\">\n",
    "<h4>Pipeline or make_pipeline? </h4>\n",
    "</div>\n",
    "<div style=\"line-height:1.6\">\n",
    "\n",
    "Pipeline is a class that allows you to create a pipeline by sequentially applying a list of transformers and a final estimator. <br>\n",
    "It takes a list of tuples, where each tuple contains the name you want to give to a step in the pipeline and the corresponding transformer or estimator object. <br>\n",
    "The first step of the pipeline must be a transformer, and all the following steps can be either transformers or an estimator. <br>\n",
    "On the other hand, make_pipeline is a function that allows you to create a pipeline by sequentially applying a sequence of transformers and a final estimator. <br>\n",
    "Unlike Pipeline, make_pipeline automatically names each step based on the class name of the transformer or estimator. <br>\n",
    "This can be convenient if you do not want to specify the name of each step manually. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;encoder&#x27;, OneHotEncoder(),\n",
       "                                                  [&#x27;Shape&#x27;, &#x27;Class&#x27;]),\n",
       "                                                 (&#x27;imputer&#x27;, SimpleImputer(),\n",
       "                                                  [&#x27;Val&#x27;])])),\n",
       "                (&#x27;classifier&#x27;, LogisticRegression())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;encoder&#x27;, OneHotEncoder(),\n",
       "                                                  [&#x27;Shape&#x27;, &#x27;Class&#x27;]),\n",
       "                                                 (&#x27;imputer&#x27;, SimpleImputer(),\n",
       "                                                  [&#x27;Val&#x27;])])),\n",
       "                (&#x27;classifier&#x27;, LogisticRegression())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">preprocessor: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                  transformers=[(&#x27;encoder&#x27;, OneHotEncoder(),\n",
       "                                 [&#x27;Shape&#x27;, &#x27;Class&#x27;]),\n",
       "                                (&#x27;imputer&#x27;, SimpleImputer(), [&#x27;Val&#x27;])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">encoder</label><div class=\"sk-toggleable__content\"><pre>[&#x27;Shape&#x27;, &#x27;Class&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">imputer</label><div class=\"sk-toggleable__content\"><pre>[&#x27;Val&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">remainder</label><div class=\"sk-toggleable__content\"><pre></pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">passthrough</label><div class=\"sk-toggleable__content\"><pre>passthrough</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('encoder', OneHotEncoder(),\n",
       "                                                  ['Shape', 'Class']),\n",
       "                                                 ('imputer', SimpleImputer(),\n",
       "                                                  ['Val'])])),\n",
       "                ('classifier', LogisticRegression())])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array([\n",
    "    ['circle', 'first', 'S', 83.1],\n",
    "    ['oval', 'second', 'M', 3.2],\n",
    "    ['square', 'third', 'L', 45.1],\n",
    "    ['triangle', 'fourth', 'XL', np.nan]\n",
    "])\n",
    "\n",
    "df6 = pd.DataFrame(data, columns=['Shape', 'Class', 'Size', 'Val'])\n",
    "\n",
    "ohe6 = OneHotEncoder()\n",
    "sim6 = SimpleImputer()\n",
    "clf6 = LogisticRegression()\n",
    "\n",
    "ohe6.fit_transform(df1[['Shape']])\n",
    "\n",
    "ct6 = ColumnTransformer([('encoder', ohe6, ['Shape', 'Class']), ('imputer', imp, ['Val'])], remainder= 'passthrough')\n",
    "pipe6 = Pipeline([('preprocessor', ct6), ('classifier', clf6)])\n",
    "pipe6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/notto4/anaconda3/envs/MLearning/lib/python3.10/site-packages/sklearn/datasets/_openml.py:292: UserWarning: Multiple active versions of the dataset matching the name boston exist. Versions may be fundamentally different, returning version 1.\n",
      "  warn(\n",
      "/home/notto4/anaconda3/envs/MLearning/lib/python3.10/site-packages/sklearn/datasets/_openml.py:932: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.62976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.949</td>\n",
       "      <td>61.8</td>\n",
       "      <td>4.7075</td>\n",
       "      <td>4</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>8.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.17171</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5.13</td>\n",
       "      <td>0</td>\n",
       "      <td>0.453</td>\n",
       "      <td>5.966</td>\n",
       "      <td>93.4</td>\n",
       "      <td>6.8185</td>\n",
       "      <td>8</td>\n",
       "      <td>284.0</td>\n",
       "      <td>19.7</td>\n",
       "      <td>378.08</td>\n",
       "      <td>14.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>9.82349</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.671</td>\n",
       "      <td>6.794</td>\n",
       "      <td>98.8</td>\n",
       "      <td>1.3580</td>\n",
       "      <td>24</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>21.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.02763</td>\n",
       "      <td>75.0</td>\n",
       "      <td>2.95</td>\n",
       "      <td>0</td>\n",
       "      <td>0.428</td>\n",
       "      <td>6.595</td>\n",
       "      <td>21.8</td>\n",
       "      <td>5.4011</td>\n",
       "      <td>3</td>\n",
       "      <td>252.0</td>\n",
       "      <td>18.3</td>\n",
       "      <td>395.63</td>\n",
       "      <td>4.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>4.55587</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.718</td>\n",
       "      <td>3.561</td>\n",
       "      <td>87.9</td>\n",
       "      <td>1.6132</td>\n",
       "      <td>24</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>354.70</td>\n",
       "      <td>7.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>0.11460</td>\n",
       "      <td>20.0</td>\n",
       "      <td>6.96</td>\n",
       "      <td>0</td>\n",
       "      <td>0.464</td>\n",
       "      <td>6.538</td>\n",
       "      <td>58.7</td>\n",
       "      <td>3.9175</td>\n",
       "      <td>3</td>\n",
       "      <td>223.0</td>\n",
       "      <td>18.6</td>\n",
       "      <td>394.96</td>\n",
       "      <td>7.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>0.13587</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.59</td>\n",
       "      <td>1</td>\n",
       "      <td>0.489</td>\n",
       "      <td>6.064</td>\n",
       "      <td>59.1</td>\n",
       "      <td>4.2392</td>\n",
       "      <td>4</td>\n",
       "      <td>277.0</td>\n",
       "      <td>18.6</td>\n",
       "      <td>381.32</td>\n",
       "      <td>14.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>0.52058</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.20</td>\n",
       "      <td>1</td>\n",
       "      <td>0.507</td>\n",
       "      <td>6.631</td>\n",
       "      <td>76.5</td>\n",
       "      <td>4.1480</td>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>17.4</td>\n",
       "      <td>388.45</td>\n",
       "      <td>9.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.08187</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.89</td>\n",
       "      <td>0</td>\n",
       "      <td>0.445</td>\n",
       "      <td>7.820</td>\n",
       "      <td>36.9</td>\n",
       "      <td>3.4952</td>\n",
       "      <td>2</td>\n",
       "      <td>276.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>393.53</td>\n",
       "      <td>3.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>3.47428</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.718</td>\n",
       "      <td>8.780</td>\n",
       "      <td>82.9</td>\n",
       "      <td>1.9047</td>\n",
       "      <td>24</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>354.55</td>\n",
       "      <td>5.29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        CRIM    ZN  INDUS CHAS    NOX     RM   AGE     DIS RAD    TAX  \\\n",
       "13   0.62976   0.0   8.14    0  0.538  5.949  61.8  4.7075   4  307.0   \n",
       "61   0.17171  25.0   5.13    0  0.453  5.966  93.4  6.8185   8  284.0   \n",
       "377  9.82349   0.0  18.10    0  0.671  6.794  98.8  1.3580  24  666.0   \n",
       "39   0.02763  75.0   2.95    0  0.428  6.595  21.8  5.4011   3  252.0   \n",
       "365  4.55587   0.0  18.10    0  0.718  3.561  87.9  1.6132  24  666.0   \n",
       "272  0.11460  20.0   6.96    0  0.464  6.538  58.7  3.9175   3  223.0   \n",
       "208  0.13587   0.0  10.59    1  0.489  6.064  59.1  4.2392   4  277.0   \n",
       "236  0.52058   0.0   6.20    1  0.507  6.631  76.5  4.1480   8  307.0   \n",
       "98   0.08187   0.0   2.89    0  0.445  7.820  36.9  3.4952   2  276.0   \n",
       "364  3.47428   0.0  18.10    1  0.718  8.780  82.9  1.9047  24  666.0   \n",
       "\n",
       "     PTRATIO       B  LSTAT  \n",
       "13      21.0  396.90   8.26  \n",
       "61      19.7  378.08  14.44  \n",
       "377     20.2  396.90  21.24  \n",
       "39      18.3  395.63   4.32  \n",
       "365     20.2  354.70   7.12  \n",
       "272     18.6  394.96   7.73  \n",
       "208     18.6  381.32  14.66  \n",
       "236     17.4  388.45   9.54  \n",
       "98      18.0  393.53   3.57  \n",
       "364     20.2  354.55   5.29  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Boston Housing dataset\n",
    "boston = fetch_openml(name='boston')\n",
    "# Get the feature matrix X and target vector y\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "X_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#B0EE8F \">  # 7) train_test_split </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.62976, 0.0, 8.14, '0', 0.538, 5.949, 61.8, 4.7075, '4', 307.0,\n",
       "        21.0, 396.9, 8.26],\n",
       "       [0.17171, 25.0, 5.13, '0', 0.453, 5.966, 93.4, 6.8185, '8', 284.0,\n",
       "        19.7, 378.08, 14.44],\n",
       "       [9.82349, 0.0, 18.1, '0', 0.671, 6.794, 98.8, 1.358, '24', 666.0,\n",
       "        20.2, 396.9, 21.24],\n",
       "       [0.02763, 75.0, 2.95, '0', 0.428, 6.595, 21.8, 5.4011, '3', 252.0,\n",
       "        18.3, 395.63, 4.32],\n",
       "       [4.55587, 0.0, 18.1, '0', 0.718, 3.561, 87.9, 1.6132, '24', 666.0,\n",
       "        20.2, 354.7, 7.12],\n",
       "       [0.1146, 20.0, 6.96, '0', 0.464, 6.538, 58.7, 3.9175, '3', 223.0,\n",
       "        18.6, 394.96, 7.73],\n",
       "       [0.13587, 0.0, 10.59, '1', 0.489, 6.064, 59.1, 4.2392, '4', 277.0,\n",
       "        18.6, 381.32, 14.66],\n",
       "       [0.52058, 0.0, 6.2, '1', 0.507, 6.631, 76.5, 4.148, '8', 307.0,\n",
       "        17.4, 388.45, 9.54],\n",
       "       [0.08187, 0.0, 2.89, '0', 0.445, 7.82, 36.9, 3.4952, '2', 276.0,\n",
       "        18.0, 393.53, 3.57],\n",
       "       [3.47428, 0.0, 18.1, '1', 0.718, 8.78, 82.9, 1.9047, '24', 666.0,\n",
       "        20.2, 354.55, 5.29]], dtype=object)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Get the feature matrix X and target vector y using slicing (need to convert the dictionary-like object to a NumPy array)\n",
    "data = boston.data.to_numpy()\n",
    "target = boston.target.to_numpy()\n",
    "# Get the feature matrix X and target vector y using slicing\n",
    "X = data[:, :]\n",
    "y = target[:]\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) # random_state avoid to obtain different results\n",
    "X_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing random_state from 1 to 2 will result in a different random seed being used to split the data into training and test sets. <br>\n",
    "This will result in different training and test sets being generated, which may affect the performance of the model. <br>\n",
    "However, the overall impact of changing the random seed will likely be small, especially if the dataset is large enough. <br>\n",
    "In practice, it is common to try multiple random seeds and choose the one that gives the best performance on average.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.67367, 0.0, 18.1, ..., 20.2, 388.62, 10.58],\n",
       "       [0.09604, 40.0, 6.41, ..., 17.6, 396.9, 2.98],\n",
       "       [3.53501, 0.0, 19.58, ..., 14.7, 88.01, 15.02],\n",
       "       ...,\n",
       "       [0.17331, 0.0, 9.69, ..., 19.2, 396.9, 12.01],\n",
       "       [0.62739, 0.0, 8.14, ..., 21.0, 395.62, 8.47],\n",
       "       [2.3004, 0.0, 19.58, ..., 14.7, 297.09, 11.1]], dtype=object)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2) \n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"line-height:0.5\">\n",
    "<h2 style=\"color:#B0EE8F \">  # 8) IterativeImputer </h2>\n",
    "</div>\n",
    "IterativeImputer built a regression model and those prediction are the missing values. <br>\n",
    "It is a strategy for imputing missing values by modeling each feature with missing values <br>\n",
    "as a function of other features in a round-robin fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[17.        , 80.        , 21.        , 39.        , 82.        ],\n",
       "       [88.        , 31.        , 35.        , 63.        , 34.        ],\n",
       "       [52.65692187, 90.        , 59.        ,  2.        , 85.        ],\n",
       "       [52.37222056, 91.        , 34.        , 34.        , 10.        ],\n",
       "       [52.60022313, 62.        ,  4.        ,  8.        , 24.        ],\n",
       "       [26.        , 19.        , 24.        , 99.        , 78.        ],\n",
       "       [45.        , 80.        ,  0.        , 15.        , 97.        ],\n",
       "       [26.        , 37.        , 45.17234036, 53.        , 85.        ],\n",
       "       [76.        , 93.        , 44.74688926, 60.        , 79.        ],\n",
       "       [29.        , 63.        , 84.        , 79.        , 26.        ],\n",
       "       [28.        , 56.        , 72.        , 63.        , 62.        ],\n",
       "       [ 6.        , 66.        , 75.        , 95.        , 89.        ],\n",
       "       [69.        , 30.        , 95.        , 55.5177167 , 39.        ],\n",
       "       [52.        , 72.        , 24.        , 99.        , 26.        ],\n",
       "       [83.        , 93.        ,  2.        , 52.        , 37.        ],\n",
       "       [20.        , 43.        , 35.        , 55.        , 54.99848745],\n",
       "       [31.        , 16.        , 77.        , 48.        , 49.        ],\n",
       "       [49.        , 21.        , 15.        , 46.        , 75.        ],\n",
       "       [91.        , 94.        , 22.        ,  0.        , 28.        ],\n",
       "       [ 8.        , 72.        , 66.        , 75.        , 50.        ],\n",
       "       [24.        ,  1.        , 48.        , 49.        , 95.        ],\n",
       "       [76.        , 28.        , 30.        ,  3.        , 73.        ],\n",
       "       [75.        , 24.        , 69.        , 39.        , 72.        ],\n",
       "       [53.        , 48.        ,  6.        ,  9.        , 58.        ],\n",
       "       [ 9.        ,  0.        , 94.        , 45.        , 11.        ],\n",
       "       [59.        ,  3.        , 33.        , 32.        , 53.        ],\n",
       "       [98.        , 78.        , 62.        , 83.        , 66.        ],\n",
       "       [42.        , 49.        ,  8.        , 37.        , 44.        ],\n",
       "       [79.        , 11.        , 40.        , 69.        , 95.        ],\n",
       "       [45.        , 65.        , 67.        , 86.        , 80.        ],\n",
       "       [32.        , 86.        , 67.        , 68.        , 62.        ],\n",
       "       [74.        , 47.        , 37.        , 19.        , 30.        ],\n",
       "       [72.        , 57.        , 40.        , 11.        , 20.        ],\n",
       "       [82.        , 13.        , 68.        , 50.        ,  5.        ],\n",
       "       [77.        , 20.        , 21.        , 58.        , 17.        ],\n",
       "       [58.        , 33.        , 17.        , 53.        , 33.        ],\n",
       "       [48.        ,  6.        , 30.        , 95.        , 46.        ],\n",
       "       [38.        ,  5.        ,  7.        , 97.        , 44.        ],\n",
       "       [ 7.        , 81.        , 84.        , 90.        ,  8.        ],\n",
       "       [87.        , 77.        ,  7.        , 53.        , 64.        ],\n",
       "       [20.        , 32.        , 49.        , 76.        , 92.        ],\n",
       "       [42.        , 38.        , 42.        , 62.        , 88.        ],\n",
       "       [69.        , 69.        , 50.        , 72.        , 74.        ],\n",
       "       [74.        , 65.        , 90.        ,  5.        , 88.        ],\n",
       "       [40.        , 37.        ,  6.        ,  8.        , 75.        ],\n",
       "       [92.        , 47.        , 91.        , 77.        , 93.        ],\n",
       "       [91.        , 92.        , 69.        , 58.        , 38.        ],\n",
       "       [ 5.        , 20.        , 61.        , 18.        , 14.        ],\n",
       "       [10.        ,  5.        , 49.        , 66.        , 26.        ],\n",
       "       [89.        , 32.        , 69.        , 83.        , 76.        ]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####### Create a Pandas DataFrame with 5 columns and 50 rows\n",
    "df8 = pd.DataFrame({\n",
    "    'A': np.random.randint(low=0, high=100, size=50),\n",
    "    'B': np.random.randint(low=0, high=100, size=50),\n",
    "    'C': np.random.randint(low=0, high=100, size=50),\n",
    "    'D': np.random.randint(low=0, high=100, size=50),\n",
    "    'E': np.random.randint(low=0, high=100, size=50),\n",
    "})\n",
    "\n",
    "#### Introduce some missing values into the DataFrame\n",
    "df8.iloc[2:5, 0] = np.nan\n",
    "df8.iloc[7:9, 2] = np.nan\n",
    "df8.iloc[12, 3] = np.nan\n",
    "df8.iloc[15, 4] = np.nan\n",
    "\n",
    "impute_it = IterativeImputer() \n",
    "impute_it.fit_transform(df8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"line-height:0.5\">\n",
    "<h2 style=\"color:#B0EE8F \">  # 9) KNNImputer </h2>\n",
    "</div>\n",
    "Imputation for completing missing values using k-Nearest Neighbors. <br>\n",
    "Find the two most similar rows how close are di nan values close to the value in similar rows, averaging the values (not missing in that same column). <br>\n",
    "All features that are passed to the Imputer are taken into account. <br> \n",
    "Each sample's missing values are imputed using the mean value from `n_neighbors` nearest neighbors found in the training set.  <br>\n",
    "Two samples are close if the features that neither is missing are close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create KNNImputer object \"\"\"\n",
    "impute_knn = KNNImputer()\n",
    "\n",
    "# ...with specific settings\n",
    "imputer1 = KNNImputer(n_neighbors=3, weights='distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...with a custom distance metric\n",
    "def my_distance(x, y):\n",
    "    return 1.0 / np.sum(np.abs(x - y))\n",
    "imputer2 = KNNImputer(metric=my_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...with an indicator for missing values\n",
    "imputer3 = KNNImputer(add_indicator=True)\n",
    "\n",
    "# ...that keeps empty features\n",
    "imputer4 = KNNImputer(keep_empty_features=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[17. , 80. , 21. , 39. , 82. ],\n",
       "       [88. , 31. , 35. , 63. , 34. ],\n",
       "       [60.6, 90. , 59. ,  2. , 85. ],\n",
       "       [82.2, 91. , 34. , 34. , 10. ],\n",
       "       [66.4, 62. ,  4. ,  8. , 24. ],\n",
       "       [26. , 19. , 24. , 99. , 78. ],\n",
       "       [45. , 80. ,  0. , 15. , 97. ],\n",
       "       [26. , 37. , 42.6, 53. , 85. ],\n",
       "       [76. , 93. , 38. , 60. , 79. ],\n",
       "       [29. , 63. , 84. , 79. , 26. ],\n",
       "       [28. , 56. , 72. , 63. , 62. ],\n",
       "       [ 6. , 66. , 75. , 95. , 89. ],\n",
       "       [69. , 30. , 95. , 59.8, 39. ],\n",
       "       [52. , 72. , 24. , 99. , 26. ],\n",
       "       [83. , 93. ,  2. , 52. , 37. ],\n",
       "       [20. , 43. , 35. , 55. , 74.2],\n",
       "       [31. , 16. , 77. , 48. , 49. ],\n",
       "       [49. , 21. , 15. , 46. , 75. ],\n",
       "       [91. , 94. , 22. ,  0. , 28. ],\n",
       "       [ 8. , 72. , 66. , 75. , 50. ],\n",
       "       [24. ,  1. , 48. , 49. , 95. ],\n",
       "       [76. , 28. , 30. ,  3. , 73. ],\n",
       "       [75. , 24. , 69. , 39. , 72. ],\n",
       "       [53. , 48. ,  6. ,  9. , 58. ],\n",
       "       [ 9. ,  0. , 94. , 45. , 11. ],\n",
       "       [59. ,  3. , 33. , 32. , 53. ],\n",
       "       [98. , 78. , 62. , 83. , 66. ],\n",
       "       [42. , 49. ,  8. , 37. , 44. ],\n",
       "       [79. , 11. , 40. , 69. , 95. ],\n",
       "       [45. , 65. , 67. , 86. , 80. ],\n",
       "       [32. , 86. , 67. , 68. , 62. ],\n",
       "       [74. , 47. , 37. , 19. , 30. ],\n",
       "       [72. , 57. , 40. , 11. , 20. ],\n",
       "       [82. , 13. , 68. , 50. ,  5. ],\n",
       "       [77. , 20. , 21. , 58. , 17. ],\n",
       "       [58. , 33. , 17. , 53. , 33. ],\n",
       "       [48. ,  6. , 30. , 95. , 46. ],\n",
       "       [38. ,  5. ,  7. , 97. , 44. ],\n",
       "       [ 7. , 81. , 84. , 90. ,  8. ],\n",
       "       [87. , 77. ,  7. , 53. , 64. ],\n",
       "       [20. , 32. , 49. , 76. , 92. ],\n",
       "       [42. , 38. , 42. , 62. , 88. ],\n",
       "       [69. , 69. , 50. , 72. , 74. ],\n",
       "       [74. , 65. , 90. ,  5. , 88. ],\n",
       "       [40. , 37. ,  6. ,  8. , 75. ],\n",
       "       [92. , 47. , 91. , 77. , 93. ],\n",
       "       [91. , 92. , 69. , 58. , 38. ],\n",
       "       [ 5. , 20. , 61. , 18. , 14. ],\n",
       "       [10. ,  5. , 49. , 66. , 26. ],\n",
       "       [89. , 32. , 69. , 83. , 76. ]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "impute_knn.fit_transform(df8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"line-height:0.5\">\n",
    "<h2 style=\"color:#B0EE8F \">  # 10) Pipeline </h2>\n",
    "</div>\n",
    "Examine the intermediate steps in a Pipeline  the \"named_steps\" attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     age        income     weight      height\n",
      "0   27.0           NaN        NaN  160.369850\n",
      "1    NaN  46098.954586        NaN  182.485959\n",
      "2   33.0           NaN        NaN  165.390121\n",
      "3    NaN  29154.694513        NaN  169.951719\n",
      "4   24.0           NaN        NaN  174.382466\n",
      "5    NaN  14139.835887        NaN  177.142351\n",
      "6   21.0           NaN        NaN  173.677084\n",
      "7    NaN  16951.347248        NaN  171.302470\n",
      "8   64.0           NaN        NaN  172.566214\n",
      "9    NaN   4399.612106        NaN  188.329464\n",
      "10  31.0   7099.221189  62.144848  177.983105\n",
      "11  20.0  36317.705028  77.394001  174.893749\n",
      "12  45.0  16892.663894  67.837276  162.693641\n",
      "13  24.0  25513.308948  56.349276  167.499316\n",
      "14  47.0  10806.506602  76.132502  155.175132\n",
      "15  60.0  13545.618444  53.723602  156.728152\n",
      "16  43.0  34168.227303  81.336970  164.925205\n",
      "17  33.0  39296.855218  66.351949  161.619488\n",
      "18  44.0  30385.773809  57.882477  171.160851\n",
      "19  48.0  29830.510104  72.507152  173.497910\n",
      "20  37.0  35359.521792  74.861651  149.546662\n",
      "21  20.0   1862.911920  72.954029  184.339525\n",
      "22  47.0  36831.731769  78.749963  170.375301\n",
      "23  58.0   4384.190698  70.283683  169.088706\n",
      "24  21.0  37530.810017  75.824071  175.202135\n",
      "25  25.0  37204.750281  67.385756  167.076583\n",
      "26  62.0   2773.345212  48.422683  185.806053\n",
      "27  47.0   6429.977188  78.050494  175.786646\n",
      "28  45.0  47058.577541  61.226208  165.423519\n",
      "29  61.0  44512.203024  88.863514  184.471077\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([   39.48      , 24341.95417275,    69.41410532])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data10 = {\n",
    "    'age': np.random.randint(18, 65, size=30),\n",
    "    'income': np.random.uniform(1000, 50000, size=30),\n",
    "    'weight': np.random.normal(70, 10, size=30),\n",
    "    'height': np.random.normal(170, 10, size=30)    \n",
    "}\n",
    "# Create a dataFrame from the dictionary\n",
    "df_df10 = pd.DataFrame(data10)\n",
    "## Add some missing values to the DataFrame\n",
    "df_df10.iloc[[1, 3, 5, 7, 9], [0, 2]] = np.nan\n",
    "df_df10.iloc[[0, 2, 4, 6, 8], [1, 2]] = np.nan\n",
    "print(df_df10)\n",
    "\n",
    "# N.B using a Logistic Regressor to predict a continuous target variable ('height') instead of a categorical or binary target variable,\n",
    "# will lead to an error.\n",
    "pipeline10 = make_pipeline(SimpleImputer(strategy='mean'), LinearRegression())\n",
    "\n",
    "# Fit and transform the pipeline on the dataframe\n",
    "X = df_df10[['age', 'income', 'weight']]\n",
    "y = df_df10[['height']]\n",
    "y = y.values.ravel()\n",
    "pipeline10.fit(X, y)\n",
    "pipeline10.named_steps.simpleimputer.statistics_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ValueError: Unknown label type: 'continuous' error occurs when you are trying to use a classification algorithm on a regression problem.    \n",
    "In other words, you are trying to predict a continuous value (the height in this case) with a classifier (Logistic Regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     age        income     weight      height\n",
      "1    NaN  11054.445086        NaN  172.639935\n",
      "3    NaN  36508.536719        NaN  188.096091\n",
      "5    NaN   4776.843955        NaN  153.047794\n",
      "7    NaN   8100.747119        NaN  174.241175\n",
      "9    NaN  35064.573432        NaN  143.435412\n",
      "10  51.0  13174.220919  50.638642  177.626253\n",
      "11  25.0  48637.415259  81.489223  174.199149\n",
      "12  28.0  37645.152788  76.323688  177.732540\n",
      "13  54.0  11317.419565  62.753748  164.563532\n",
      "14  32.0  21117.539019  55.345778  167.980545\n",
      "15  40.0  31380.635226  65.353420  176.902690\n",
      "16  32.0  28190.970242  72.387077  172.067400\n",
      "17  28.0  22987.606034  89.239745  177.966825\n",
      "18  35.0  28471.498554  73.346421  165.829286\n",
      "19  60.0  44531.739827  56.089202  175.550565\n",
      "20  18.0  38073.433008  54.920984  162.340931\n",
      "21  27.0  29122.933297  68.919706  174.474787\n",
      "22  59.0  38783.650454  69.170823  157.333739\n",
      "23  26.0  32295.215340  71.356488  168.477760\n",
      "24  52.0   1671.559132  95.750649  178.362821\n",
      "25  63.0  10298.318636  59.068668  176.863354\n",
      "26  47.0  23557.902439  70.732125  168.560729\n",
      "27  41.0  28861.101149  70.020630  161.169585\n",
      "28  36.0  11564.985510  72.596593  182.173338\n",
      "29  26.0   7348.888438  77.990322  177.440251\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([   39.        , 24181.493246  ,    69.67469663])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data10_1 = {\n",
    "    'age': np.random.randint(18, 65, size=30),\n",
    "    'income': np.random.uniform(1000, 50000, size=30),\n",
    "    'weight': np.random.normal(70, 10, size=30),\n",
    "    'height': np.random.normal(170, 10, size=30)    \n",
    "}\n",
    "# DataFrame from the dictionary\n",
    "df_df10_1 = pd.DataFrame(data10_1)\n",
    "# Add some missing values to the DataFrame\n",
    "df_df10_1.iloc[[1, 3, 5, 7, 9], [0, 2]] = np.nan\n",
    "df_df10_1.iloc[[0, 2, 4, 6, 8], [1, 3]] = np.nan\n",
    "# ...to avoid that y contains nan values as targets!\n",
    "df_df10_1.dropna(subset=['height'], inplace=True) \n",
    "print(df_df10_1)\n",
    "\n",
    "pipeline10_1 = make_pipeline(SimpleImputer(strategy='mean'), LinearRegression())\n",
    "# Fit and transform the pipeline on the dataframe\n",
    "X = df_df10_1[['age', 'income', 'weight']]\n",
    "y = df_df10_1['height']\n",
    "pipeline10_1.fit(X, y)\n",
    "pipeline10_1.named_steps.simpleimputer.statistics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "      <th>weight</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>11054.445086</td>\n",
       "      <td>NaN</td>\n",
       "      <td>172.639935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>36508.536719</td>\n",
       "      <td>NaN</td>\n",
       "      <td>188.096091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4776.843955</td>\n",
       "      <td>NaN</td>\n",
       "      <td>153.047794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>8100.747119</td>\n",
       "      <td>NaN</td>\n",
       "      <td>174.241175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>35064.573432</td>\n",
       "      <td>NaN</td>\n",
       "      <td>143.435412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>51.0</td>\n",
       "      <td>13174.220919</td>\n",
       "      <td>50.638642</td>\n",
       "      <td>177.626253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>25.0</td>\n",
       "      <td>48637.415259</td>\n",
       "      <td>81.489223</td>\n",
       "      <td>174.199149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>28.0</td>\n",
       "      <td>37645.152788</td>\n",
       "      <td>76.323688</td>\n",
       "      <td>177.732540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>54.0</td>\n",
       "      <td>11317.419565</td>\n",
       "      <td>62.753748</td>\n",
       "      <td>164.563532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>32.0</td>\n",
       "      <td>21117.539019</td>\n",
       "      <td>55.345778</td>\n",
       "      <td>167.980545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>40.0</td>\n",
       "      <td>31380.635226</td>\n",
       "      <td>65.353420</td>\n",
       "      <td>176.902690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>32.0</td>\n",
       "      <td>28190.970242</td>\n",
       "      <td>72.387077</td>\n",
       "      <td>172.067400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>28.0</td>\n",
       "      <td>22987.606034</td>\n",
       "      <td>89.239745</td>\n",
       "      <td>177.966825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>35.0</td>\n",
       "      <td>28471.498554</td>\n",
       "      <td>73.346421</td>\n",
       "      <td>165.829286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>60.0</td>\n",
       "      <td>44531.739827</td>\n",
       "      <td>56.089202</td>\n",
       "      <td>175.550565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>18.0</td>\n",
       "      <td>38073.433008</td>\n",
       "      <td>54.920984</td>\n",
       "      <td>162.340931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>27.0</td>\n",
       "      <td>29122.933297</td>\n",
       "      <td>68.919706</td>\n",
       "      <td>174.474787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>59.0</td>\n",
       "      <td>38783.650454</td>\n",
       "      <td>69.170823</td>\n",
       "      <td>157.333739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>26.0</td>\n",
       "      <td>32295.215340</td>\n",
       "      <td>71.356488</td>\n",
       "      <td>168.477760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>52.0</td>\n",
       "      <td>1671.559132</td>\n",
       "      <td>95.750649</td>\n",
       "      <td>178.362821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>63.0</td>\n",
       "      <td>10298.318636</td>\n",
       "      <td>59.068668</td>\n",
       "      <td>176.863354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>47.0</td>\n",
       "      <td>23557.902439</td>\n",
       "      <td>70.732125</td>\n",
       "      <td>168.560729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>41.0</td>\n",
       "      <td>28861.101149</td>\n",
       "      <td>70.020630</td>\n",
       "      <td>161.169585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>36.0</td>\n",
       "      <td>11564.985510</td>\n",
       "      <td>72.596593</td>\n",
       "      <td>182.173338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>26.0</td>\n",
       "      <td>7348.888438</td>\n",
       "      <td>77.990322</td>\n",
       "      <td>177.440251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age        income     weight      height\n",
       "1    NaN  11054.445086        NaN  172.639935\n",
       "3    NaN  36508.536719        NaN  188.096091\n",
       "5    NaN   4776.843955        NaN  153.047794\n",
       "7    NaN   8100.747119        NaN  174.241175\n",
       "9    NaN  35064.573432        NaN  143.435412\n",
       "10  51.0  13174.220919  50.638642  177.626253\n",
       "11  25.0  48637.415259  81.489223  174.199149\n",
       "12  28.0  37645.152788  76.323688  177.732540\n",
       "13  54.0  11317.419565  62.753748  164.563532\n",
       "14  32.0  21117.539019  55.345778  167.980545\n",
       "15  40.0  31380.635226  65.353420  176.902690\n",
       "16  32.0  28190.970242  72.387077  172.067400\n",
       "17  28.0  22987.606034  89.239745  177.966825\n",
       "18  35.0  28471.498554  73.346421  165.829286\n",
       "19  60.0  44531.739827  56.089202  175.550565\n",
       "20  18.0  38073.433008  54.920984  162.340931\n",
       "21  27.0  29122.933297  68.919706  174.474787\n",
       "22  59.0  38783.650454  69.170823  157.333739\n",
       "23  26.0  32295.215340  71.356488  168.477760\n",
       "24  52.0   1671.559132  95.750649  178.362821\n",
       "25  63.0  10298.318636  59.068668  176.863354\n",
       "26  47.0  23557.902439  70.732125  168.560729\n",
       "27  41.0  28861.101149  70.020630  161.169585\n",
       "28  36.0  11564.985510  72.596593  182.173338\n",
       "29  26.0   7348.888438  77.990322  177.440251"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_df10_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"line-height:0.5\">\n",
    "<h2 style=\"color:#B0EE8F \">  # 11) Handling missing values </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age        income  weight      height\n",
      "1  NaN  11054.445086     NaN  172.639935\n",
      "3  NaN  36508.536719     NaN  188.096091\n",
      "5  NaN   4776.843955     NaN  153.047794\n",
      "   age       income  weight     height\n",
      "1  NaN  1776.319014     NaN  69.438018\n",
      "3  NaN  5866.491484     NaN  75.654684\n",
      "5  NaN   767.582514     NaN  61.557805\n",
      "\n",
      "10\n",
      "age       5\n",
      "income    0\n",
      "weight    5\n",
      "height    0\n",
      "dtype: int64\n",
      "1     69.438018\n",
      "3     75.654684\n",
      "5     61.557805\n",
      "7     70.082057\n",
      "9     57.691581\n",
      "10    71.443580\n",
      "11    70.065154\n",
      "12    71.486330\n",
      "13    66.189584\n",
      "14    67.563951\n",
      "15    71.152553\n",
      "16    69.207737\n",
      "17    71.580562\n",
      "18    66.698687\n",
      "19    70.608711\n",
      "20    65.295625\n",
      "21    70.176019\n",
      "22    63.281668\n",
      "23    67.763937\n",
      "24    71.739837\n",
      "25    71.136732\n",
      "26    67.797308\n",
      "27    64.824495\n",
      "28    73.272476\n",
      "29    71.368767\n",
      "Name: height, dtype: float64\n",
      "\n",
      "   age       income  weight\n",
      "1  NaN  1776.319014     NaN\n",
      "3  NaN  5866.491484     NaN\n",
      "5  NaN   767.582514     NaN\n",
      "7  NaN  1301.694570     NaN\n",
      "9  NaN  5634.463606     NaN\n"
     ]
    }
   ],
   "source": [
    "df_11 = copy(df_df10_1)\n",
    "df_11 = df_11.apply(lambda x: x * np.random.rand())\n",
    "\n",
    "print(df_df10_1.head(3))\n",
    "print(df_11.head(3))\n",
    "print()\n",
    "# Count the NaNs in the whole dataframe\n",
    "print(df_11.isna().sum().sum())  \n",
    "# Count the NaNs in each column\n",
    "print(df_11.isna().sum())        \n",
    "\n",
    "df_11_1 = df_11\n",
    "label = df_11_1.pop('height')\n",
    "print(label)\n",
    "print()\n",
    "print(df_11_1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1776.319014</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5866.491484</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>767.582514</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1301.694570</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5634.463606</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age       income  weight\n",
       "1  NaN  1776.319014     NaN\n",
       "3  NaN  5866.491484     NaN\n",
       "5  NaN   767.582514     NaN\n",
       "7  NaN  1301.694570     NaN\n",
       "9  NaN  5634.463606     NaN"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_11.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          age       income    weight\n",
      "1         NaN  1776.319014       NaN\n",
      "3         NaN  5866.491484       NaN\n",
      "5         NaN   767.582514       NaN\n",
      "7         NaN  1301.694570       NaN\n",
      "9         NaN  5634.463606       NaN\n",
      "10  20.327706  2116.942003  1.196770\n",
      "11   9.964562  7815.459289  1.925878\n",
      "12  11.160309  6049.132288  1.803799\n",
      "13  21.523453  1818.575913  1.483093\n",
      "14  12.754639  3393.339585  1.308016\n",
      "15  15.943299  5042.498163  1.544532\n",
      "16  12.754639  4529.956600  1.710762\n",
      "17  11.160309  3693.837309  2.109051\n",
      "18  13.950386  4575.034193  1.733435\n",
      "19  23.914948  7155.725645  1.325586\n",
      "20   7.174484  6117.951871  1.297977\n",
      "21  10.761727  4679.712077  1.628816\n",
      "22  23.516366  6232.075442  1.634751\n",
      "23  10.363144  5189.460406  1.686406\n",
      "24  20.726288   268.599848  2.262926\n",
      "25  25.110695  1654.818407  1.396001\n",
      "26  18.733376  3785.477219  1.671650\n",
      "27  16.341881  4637.638737  1.654835\n",
      "28  14.348969  1858.356842  1.715714\n",
      "          age       income    weight\n",
      "1         NaN  1776.319014       NaN\n",
      "3         NaN  5866.491484       NaN\n",
      "5         NaN   767.582514       NaN\n",
      "7         NaN  1301.694570       NaN\n",
      "9         NaN  5634.463606       NaN\n",
      "10  20.327706  2116.942003  1.196770\n",
      "11   9.964562  7815.459289  1.925878\n",
      "12  11.160309  6049.132288  1.803799\n",
      "13  21.523453  1818.575913  1.483093\n",
      "14  12.754639  3393.339585  1.308016\n",
      "15  15.943299  5042.498163  1.544532\n",
      "16  12.754639  4529.956600  1.710762\n",
      "17  11.160309  3693.837309  2.109051\n",
      "18  13.950386  4575.034193  1.733435\n",
      "19  23.914948  7155.725645  1.325586\n",
      "20   7.174484  6117.951871  1.297977\n",
      "21  10.761727  4679.712077  1.628816\n",
      "22  23.516366  6232.075442  1.634751\n",
      "23  10.363144  5189.460406  1.686406\n",
      "24  20.726288   268.599848  2.262926\n",
      "25  25.110695  1654.818407  1.396001\n",
      "26  18.733376  3785.477219  1.671650\n",
      "27  16.341881  4637.638737  1.654835\n"
     ]
    }
   ],
   "source": [
    "## Select all rows except the last one and all columns\n",
    "X1 = df_11[:-1] \n",
    "X2 = df_11[:-2]\n",
    "\n",
    "print(X1)\n",
    "print(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          age       income    weight\n",
      "1    2.000000  1776.319014  2.000000\n",
      "3    2.000000  5866.491484  2.000000\n",
      "5    2.000000   767.582514  2.000000\n",
      "7    2.000000  1301.694570  2.000000\n",
      "9    2.000000  5634.463606  2.000000\n",
      "10  20.327706  2116.942003  1.196770\n",
      "11   9.964562  7815.459289  1.925878\n",
      "12  11.160309  6049.132288  1.803799\n",
      "13  21.523453  1818.575913  1.483093\n",
      "14  12.754639  3393.339585  1.308016\n",
      "15  15.943299  5042.498163  1.544532\n",
      "16  12.754639  4529.956600  1.710762\n",
      "17  11.160309  3693.837309  2.109051\n",
      "18  13.950386  4575.034193  1.733435\n",
      "19  23.914948  7155.725645  1.325586\n",
      "20   7.174484  6117.951871  1.297977\n",
      "21  10.761727  4679.712077  1.628816\n",
      "22  23.516366  6232.075442  1.634751\n",
      "23  10.363144  5189.460406  1.686406\n",
      "24  20.726288   268.599848  2.262926\n",
      "25  25.110695  1654.818407  1.396001\n",
      "26  18.733376  3785.477219  1.671650\n",
      "27  16.341881  4637.638737  1.654835\n",
      "28  14.348969  1858.356842  1.715714\n",
      "29  10.363144  1180.879743  1.843187\n",
      "1     69.438018\n",
      "3     75.654684\n",
      "5     61.557805\n",
      "7     70.082057\n",
      "9     57.691581\n",
      "10    71.443580\n",
      "11    70.065154\n",
      "12    71.486330\n",
      "13    66.189584\n",
      "14    67.563951\n",
      "15    71.152553\n",
      "16    69.207737\n",
      "17    71.580562\n",
      "18    66.698687\n",
      "19    70.608711\n",
      "20    65.295625\n",
      "21    70.176019\n",
      "22    63.281668\n",
      "23    67.763937\n",
      "24    71.739837\n",
      "25    71.136732\n",
      "26    67.797308\n",
      "27    64.824495\n",
      "28    73.272476\n",
      "29    71.368767\n",
      "Name: height, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "1) drop rows containing Nans\n",
    "2) drop columns containing NaNs \n",
    "3) fill NaNs with imputed values\n",
    "4) use a model that natively handles NaNs\n",
    "\"\"\"\n",
    "X = df_11\n",
    "X = X.fillna(2)\n",
    "print(X)\n",
    "print(label)\n",
    "# Split data defining random_state avoid to obtain different results\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, label, test_size=0.3, random_state=1) \n",
    "\n",
    "clf_11 = GradientBoostingRegressor()\n",
    "clf_11.fit(X_train, y_train)\n",
    "y_pred = clf_11.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"line-height:0.5\">\n",
    "<h2 style=\"color:#B0EE8F \">  # 12) HistGradientBoostingClassifier </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "## Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, label, test_size=0.3, random_state=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create an instance of the classifier\n",
    "clf = HistGradientBoostingClassifier()      \n",
    "# Fit the classifier to the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Use the classifier to make predictions on the testing data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N.B. <br>\n",
    "The make_pipeline function from the sklearn.pipeline module can accept either a sequence of transformer and estimator <br> objects or a sequence of (name, transformer/estimator) tuples. <br> \n",
    "In this case, SimpleImputer() and LogisticRegression() are the transformer and estimator objects, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Age  Grade Passed\n",
      "0  22.0      2     no\n",
      "1  21.0      1     no\n",
      "2  29.0      3    yes\n",
      "3  19.0      4    yes\n",
      "4  31.0      5    yes\n",
      "5  23.0      4    yes\n",
      "6  44.0      1     no\n",
      "7   NaN      5    yes\n",
      "8  21.0      4    yes\n",
      "9  42.0      0     no\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;simpleimputer&#x27;, SimpleImputer()),\n",
       "                (&#x27;logisticregression&#x27;, LogisticRegression())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" ><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;simpleimputer&#x27;, SimpleImputer()),\n",
       "                (&#x27;logisticregression&#x27;, LogisticRegression())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" ><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-21\" type=\"checkbox\" ><label for=\"sk-estimator-id-21\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('simpleimputer', SimpleImputer()),\n",
       "                ('logisticregression', LogisticRegression())])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple dataframe\n",
    "df = pd.DataFrame({'Age': [22, 21, 29, 19, 31, 23, 44, np.nan, 21, 42], \n",
    "                'Grade': [2, 1, 3, 4, 5, 4, 1, 5, 4, 0], \n",
    "                'Passed': ['no', 'no', 'yes', 'yes', 'yes', 'yes', 'no', 'yes', 'yes', 'no']})\n",
    "print(df)\n",
    "\n",
    "X = df[['Age', 'Grade']]\n",
    "y = df['Passed']\n",
    "\n",
    "pipe = make_pipeline(SimpleImputer(), LogisticRegression())\n",
    "#pipe.fit(X, y); # jupyter trick! use ; to avoid printing the output!\n",
    "pipe.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28.   2.9]\n",
      "[[-0.02176605  1.35158464]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Inspect the imputerm collecting the mean in case you have value to impute. \n",
    "Use lower-case verion of the name of the imputer object! (in our case SimpleImputer())\n",
    "\"\"\"\n",
    "print(pipe.named_steps.simpleimputer.statistics_)\n",
    "print(pipe.named_steps.logisticregression.coef_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
