{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"line-height:0.45\">\n",
    "<h1 style=\"color:#F10C7F  \"> Decision Trees 1 </h1>\n",
    "</div>\n",
    "<div style=\"line-height:0.5\">\n",
    "<h4> Decision Trees implementation in numpy from scratch.\n",
    "</h4>\n",
    "</div>\n",
    "<div style=\"margin-top: 4px;\">\n",
    "<span style=\"display: inline-block;\">\n",
    "    <h3 style=\"color: lightblue; display: inline;\">Keywords:</h3> functools update class + Counter + __name__ + globals()\n",
    "</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_class(main_class=None, exclude=(\"__module__\", \"__name__\", \"__dict__\", \"__weakref__\")):\n",
    "    \"\"\" Class decorator.\\\\\n",
    "    Adds all methods and members from the wrapped class to main_class, to allow write in various cell methods of same class.\n",
    "    \n",
    "    Args:\n",
    "        - main_class: class to which to append members. Defaults to the class with the same name as the wrapped class\n",
    "        - exclude: black-list of members which should not be copied\n",
    "    \"\"\"\n",
    "    def decorates(main_class, exclude, appended_class):\n",
    "        if main_class is None:\n",
    "            main_class = globals()[appended_class.__name__]\n",
    "        for k, v in appended_class.__dict__.items():\n",
    "            if k not in exclude:\n",
    "                setattr(main_class, k, v)\n",
    "        return main_class\n",
    "\n",
    "    return functools.partial(decorates, main_class, exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    \"\"\" Decision Tree Class.\n",
    "\n",
    "    Args:\n",
    "        - Maximum depth of the decision tree [int]\n",
    "        - Minimum number of instances required to create a node [int]\n",
    "\n",
    "    Attributes:\n",
    "        - Maximum depth of the decision tree [int]\n",
    "        - Minimum number of instances required to create a node [int]\n",
    "        - Final decision tree structure [dict]\n",
    "        \"\"\"\n",
    "    def __init__(self, max_depth, min_node_size):\n",
    "        \"\"\" Constructor to initialize the DecisionTree object with the specified maximum depth and minimum node size. \"\"\"\n",
    "        self.max_depth = max_depth\n",
    "        self.min_node_size = min_node_size\n",
    "        self.final_tree = {}\n",
    "\n",
    "    def calculate_gini_index(self, child_nodes):\n",
    "        \"\"\" Calculate the Gini index (the impurity in a node) for a set of child nodes\\\\\n",
    "        It is used to evaluate the quality of a split.\n",
    "\n",
    "    Parameters:\n",
    "        List of child nodes\n",
    "\n",
    "    Details:\n",
    "        - Initialize the total_instances variable to 0\n",
    "        - Iterate over each child node to calculate the total number of instances in the parent node\n",
    "        - Initialize the gini_index variable to 0\n",
    "        - For each child node:\n",
    "            - Calculate the number of instances in the current child node\n",
    "            - If the child node is empty, skip the calculation to avoid division by zero\n",
    "            - Create a list (class_values) to store the class values of instances in the current node\n",
    "            - Count the frequency of each class value using the Counter class\n",
    "            - Calculate the Gini index for the current node by subtracting the squared frequency of each class value\n",
    "            - Update the node_gini variable\n",
    "            - Add the weighted node Gini index to the gini_index variable based on the number of instances in the node\n",
    "    \n",
    "    Returns:\n",
    "        Gini index [float]\n",
    "    \"\"\"\n",
    "        total_instances = 0\n",
    "        for node in child_nodes:\n",
    "            total_instances += len(node)\n",
    "        gini_index = 0\n",
    "        for node in child_nodes:\n",
    "            num_instances = len(node)\n",
    "            if num_instances == 0:\n",
    "                continue\n",
    "            class_values = []\n",
    "            for row in node:\n",
    "                class_values.append(row[-1])\n",
    "            class_freq = Counter(class_values).values()\n",
    "            node_gini = 1\n",
    "            for freq in class_freq:\n",
    "                node_gini -= (freq / num_instances) ** 2\n",
    "            gini_index += (num_instances / total_instances) * node_gini\n",
    "        return gini_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@update_class()\n",
    "class DecisionTree:\n",
    "    def apply_split(self, feature_idx, threshold, data):\n",
    "        \"\"\" Split to the data based on a given feature index and threshold.\n",
    "\n",
    "    Parameters:\n",
    "        - Index of the feature to split on [int].\n",
    "        - Threshold value for the split [float].\n",
    "        - Input data to be split [ndarray].\n",
    "\n",
    "    Returns:\n",
    "        Left and right child nodes after the split.\n",
    "\n",
    "    Details:\n",
    "        - Convert the data to a list of instances\n",
    "        - Iterate over each instance in the data\n",
    "            - If the value of the feature at the given index is less than the threshold, append the instance to the left child\n",
    "            - Otherwise, append the instance to the right child\n",
    "        - Convert the left and right child nodes to numpy arrays\n",
    "    \n",
    "    Returns:\n",
    "        left and right child nodes\n",
    "        \"\"\"\n",
    "        instances = data.tolist()\n",
    "        left_child = []\n",
    "        right_child = []\n",
    "        for row in instances:\n",
    "            if row[feature_idx] < threshold:\n",
    "                left_child.append(row)\n",
    "            else:\n",
    "                right_child.append(row)\n",
    "        left_child = np.array(left_child)\n",
    "        right_child = np.array(right_child)\n",
    "\n",
    "        return left_child, right_child\n",
    "\n",
    "\n",
    "    def find_best_split(self, data):\n",
    "        \"\"\" Find the best split for the given data based on the minimum Gini index.\n",
    "\n",
    "    Parameters:\n",
    "        Input data to find the best split for [ndarray].\n",
    "\n",
    "    Returns:\n",
    "        Best split node [dict]\n",
    "\n",
    "    Details:\n",
    "        - Get the number of features in the data\n",
    "        - Initialize the minimum Gini index to 1000 (a high value)\n",
    "        - Iterate through each feature and each row in the data\n",
    "            - Get the value of the current feature and split the data into left and right child nodes\n",
    "            - Calculate the Gini index for the child nodes using the calculate_gini_index method\n",
    "            - If the calculated Gini index is lower than the current minimum Gini index, update the min Gini index,\\\\\n",
    "                best feature index, best feature value, and best child nodes\n",
    "        - Create a node dictionary with the best feature index, best feature value, and best child nodes\n",
    "        \"\"\"\n",
    "        num_of_features = len(data[0]) - 1\n",
    "        min_gini_index = 1000\n",
    "        best_feature_idx = 0\n",
    "        best_feature_value = 0\n",
    "        for feature_idx in range(num_of_features):\n",
    "            for row in data:\n",
    "                value = row[feature_idx]\n",
    "                left_child, right_child = self.apply_split(feature_idx, value, data)\n",
    "                child_nodes = [left_child, right_child]\n",
    "                gini_index = self.calculate_gini_index(child_nodes)\n",
    "                if gini_index < min_gini_index:\n",
    "                    min_gini_index = gini_index\n",
    "                    best_feature_idx = feature_idx\n",
    "                    best_feature_value = value\n",
    "                    best_child_nodes = child_nodes\n",
    "        node = {\"feature\": best_feature_idx, \"value\": best_feature_value, \"children\": best_child_nodes}\n",
    "\n",
    "        return node\n",
    "\n",
    "    def calculate_class_value(self, node):\n",
    "        \"\"\" Compute the class value for a given node.\n",
    "\n",
    "    Parameters:\n",
    "        Input node to calculate the class value for [ndarray].\n",
    "\n",
    "    Returns:\n",
    "        The the most common class value from the occurrence count.\n",
    "\n",
    "    Details:\n",
    "        - Iterate over each row in the node and append to the class_values list \n",
    "            the class value (the last element in each row of the node array).\n",
    "        - Count the occurrence of each class value using the Counter class.\n",
    "        \"\"\"\n",
    "        class_values = []\n",
    "        for row in node:\n",
    "            class_values.append(row[-1])\n",
    "        occurrence_count = Counter(class_values)\n",
    "        \n",
    "        return occurrence_count.most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@update_class()\n",
    "class DecisionTree:\n",
    "    def recursive_split(self, node, depth):\n",
    "        \"\"\" Split the decision tree nodes based on the given node and depth.\n",
    "\n",
    "    Parameters:\n",
    "        - Current node to split [dict]\n",
    "        - Current depth of the node in the decision tree [int]\n",
    "\n",
    "    Details:\n",
    "        - Extract the left and right child nodes from the current node\n",
    "        - Remove the \"children\" key from the current node\n",
    "        - Check if either the left or right child node is empty (has size 0)\n",
    "            - If the left child is empty, calculate the class value for the right child and terminate both child nodes\n",
    "            - If the right child is empty, calculate the class value for the left child and terminate both child nodes\n",
    "        - Check if the tree has reached the maximum depth\n",
    "            - If the maximum depth is reached, calculate the class values for both child nodes and terminate them\n",
    "        - If the tree has not reached the maximum depth, continue splitting the left and right child nodes\n",
    "            - If the size of the left child is less than or equal to the minimum node size, calculate the class value for\\\\\n",
    "                the left child and terminate it\n",
    "            - Otherwise, find the best split for the left child, recursively split its child nodes, and assign the result\\\\\n",
    "                to the \"left\" key of the current node\n",
    "            - If the size of the right child is less than or equal to the minimum node size, calculate the class value for\\\\\n",
    "                the right child and terminate it\n",
    "            - Otherwise, find the best split for the right child, recursively split its child nodes, and assign the result\\\\\n",
    "                to the \"right\" key of the current node\n",
    "        \"\"\"\n",
    "        left_child, right_child = node[\"children\"]\n",
    "        del node[\"children\"]\n",
    "        if left_child.size == 0:\n",
    "            class_value = self.calculate_class_value(right_child)\n",
    "            node[\"left\"] = node[\"right\"] = {\"class_value\": class_value, \"depth\": depth}\n",
    "            return\n",
    "        elif right_child.size == 0:\n",
    "            class_value = self.calculate_class_value(left_child)\n",
    "            node[\"left\"] = node[\"right\"] = {\"class_value\": class_value, \"depth\": depth}\n",
    "            return\n",
    "        if depth >= self.max_depth:\n",
    "            class_value = self.calculate_class_value(left_child)\n",
    "            node[\"left\"] = {\"class_value\": class_value, \"depth\": depth}\n",
    "            class_value = self.calculate_class_value(right_child)\n",
    "            node[\"right\"] = {\"class_value\": class_value, \"depth\": depth}\n",
    "            return\n",
    "        if len(left_child) <= self.min_node_size:\n",
    "            class_value = self.calculate_class_value(left_child)\n",
    "            node[\"left\"] = {\"class_value\": class_value, \"depth\": depth}\n",
    "        else:\n",
    "            node[\"left\"] = self.find_best_split(left_child)\n",
    "            self.recursive_split(node[\"left\"], depth + 1)\n",
    "        if len(right_child) <= self.min_node_size:\n",
    "            class_value = self.calculate_class_value(right_child)\n",
    "            node[\"right\"] = {\"class_value\": class_value, \"depth\": depth}\n",
    "        else:\n",
    "            node[\"right\"] = self.find_best_split(right_child)\n",
    "            self.recursive_split(node[\"right\"], depth + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@update_class()\n",
    "class DecisionTree:\n",
    "    def train(self, X):\n",
    "        \"\"\" Train the decision tree using the provided input data.\n",
    "    \n",
    "    Parameters:\n",
    "        X : Input data for decision tree training [ndarray]\n",
    "\n",
    "    Details:\n",
    "        - Create the initial node by finding the best split for the input data\n",
    "        - Recursively split the initial node and generate the rest of the decision tree\n",
    "        - Set the final_tree attribute to the trained decision tree\n",
    "        - Return the trained decision tree\n",
    "    \n",
    "    Returns:\n",
    "        Trained decision tree [dict]\n",
    "    \"\"\"\n",
    "        tree = self.find_best_split(X)\n",
    "        self.recursive_split(tree, 1)\n",
    "        self.final_tree = tree\n",
    "        return tree\n",
    "\n",
    "\n",
    "    def print_decision_tree(self, tree, depth=0):\n",
    "        \"\"\" Print the decision tree structure.\n",
    "\n",
    "        Args:\n",
    "            - Decision tree to print [dict]\n",
    "            - Current depth of the tree node [int]\n",
    "\n",
    "        Details:\n",
    "            - Check if the current tree node has a \"feature\" key.\n",
    "                - If it does, print the split node information (feature, value, and depth).\n",
    "                - Recursively call the print_decision_tree method for the left and right child nodes, incrementing the depth.\n",
    "            - If the current tree node does not have a \"feature\" key, it is a terminal node.\n",
    "                - Print the terminal node information (class value and depth).\n",
    "        \"\"\"\n",
    "        if \"feature\" in tree:\n",
    "            print(\n",
    "                \"\\nSPLIT NODE: feature #{} < {} depth: {}\\n\".format(\n",
    "                    tree[\"feature\"], tree[\"value\"], depth\n",
    "                )\n",
    "            )\n",
    "            self.print_decision_tree(tree[\"left\"], depth + 1)\n",
    "            self.print_decision_tree(tree[\"right\"], depth +1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@update_class()\n",
    "class DecisionTree:\n",
    "    def recursive_split(self, node, depth):\n",
    "        \"\"\" Recursively split the decision tree nodes based on the given node and depth.\n",
    "\n",
    "        Parameters:\n",
    "            - Current node to split [dict]\n",
    "            - Current depth of the node in the decision tree [int]\n",
    "        \n",
    "        Details:\n",
    "            - Extract the left and right child nodes from the current node\n",
    "            - Remove the \"children\" key from the current node\n",
    "            - Check if either the left or right child node is empty (has size 0)\n",
    "                - If the left child is empty, calculate the class value for the right child and terminate both child nodes\n",
    "                - If the right child is empty, calculate the class value for the left child and terminate both child nodes\n",
    "            - Check if the tree has reached the maximum depth\n",
    "                - If the maximum depth is reached, calculate the class values for both child nodes and terminate them\n",
    "            - If the tree has not reached the maximum depth, continue splitting the left and right child nodes\n",
    "                - If the size of the left child is less than or equal to the minimum node size, calculate the class value for\\\\\n",
    "                the left child and terminate it\n",
    "                - Otherwise, find the best split for the left child, recursively split its child nodes, and assign the result\\\\\n",
    "                to the \"left\" key of the current node\n",
    "                - If the size of the right child is less than or equal to the minimum node size, calculate the class value for\\\\\n",
    "                the right child and terminate it\n",
    "                - Otherwise, find the best split for the right child, recursively split its child nodes, and assign the result\\\\\n",
    "                to the \"right\" key of the current node\n",
    "        \"\"\"\n",
    "        left_child, right_child = node[\"children\"]\n",
    "        del node[\"children\"]\n",
    "        if left_child.size == 0:\n",
    "            class_value = self.calculate_class_value(right_child)\n",
    "            node[\"left\"] = node[\"right\"] = {\"class_value\": class_value, \"depth\": depth}\n",
    "            return\n",
    "        elif right_child.size == 0:\n",
    "            class_value = self.calculate_class_value(left_child)\n",
    "            node[\"left\"] = node[\"right\"] = {\"class_value\": class_value, \"depth\": depth}\n",
    "            return\n",
    "        if depth >= self.max_depth:\n",
    "            class_value = self.calculate_class_value(left_child)\n",
    "            node[\"left\"] = {\"class_value\": class_value, \"depth\": depth}\n",
    "            class_value = self.calculate_class_value(right_child)\n",
    "            node[\"right\"] = {\"class_value\": class_value, \"depth\": depth}\n",
    "            return\n",
    "        if len(left_child) <= self.min_node_size:\n",
    "            class_value = self.calculate_class_value(left_child)\n",
    "            node[\"left\"] = {\"class_value\": class_value, \"depth\": depth}\n",
    "        else:\n",
    "            node[\"left\"] = self.find_best_split(left_child)\n",
    "            self.recursive_split(node[\"left\"], depth + 1)\n",
    "        if len(right_child) <= self.min_node_size:\n",
    "            class_value = self.calculate_class_value(right_child)\n",
    "            node[\"right\"] = {\"class_value\": class_value, \"depth\": depth}\n",
    "        else:\n",
    "            node[\"right\"] = self.find_best_split(right_child)\n",
    "            self.recursive_split(node[\"right\"], depth + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@update_class()\n",
    "class DecisionTree:\n",
    "    def predict_single_instance(self, tree, instance):\n",
    "        \"\"\" Predicts the class value for a single instance using the trained decision tree.\n",
    "\n",
    "        Parameters:\n",
    "            - Decision tree to use for prediction [dict]\n",
    "            - Input instance for which to predict the class value [list / ndarray]\n",
    "\n",
    "        Details:\n",
    "            Traverse the decision tree to predict the class value for a single instance\n",
    "            - If the tree is empty (not trained), it raises a ValueError\n",
    "            - If the current tree node is a split node (the \"feature\" key)\n",
    "                it compares the value of the instance for the corresponding feature with the split value\n",
    "            - If the value is less than the split value, it recursively calls 'predict_single_instance' on the left child node\n",
    "            - If the value is greater than or equal to the split value, it recursively calls 'predict_single_instance' on the right child node\n",
    "            - If the current tree node is a terminal node (leaf node), it returns the class value stored in the \"class_value\" key\n",
    "        \n",
    "        Returns:\n",
    "            The predicted class value for the given instance [int]\n",
    "        \"\"\"\n",
    "        if not tree:\n",
    "            print(\"ERROR: Please train the decision tree first\")\n",
    "            return -1\n",
    "        if \"feature\" in tree:\n",
    "            if instance[tree[\"feature\"]] < tree[\"value\"]:\n",
    "                return self.predict_single_instance(tree[\"left\"], instance)\n",
    "            else:\n",
    "                return self.predict_single_instance(tree[\"right\"], instance)\n",
    "        else:\n",
    "            return tree[\"class_value\"]\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" Predict the class values for multiple instances using the trained decision tree.\n",
    "\n",
    "        Parameters:\n",
    "            X : Input instances [list / ndarray]\n",
    "\n",
    "        Details:\n",
    "            - Iterate over the instances in X and calls 'predict_single_instance' to predict the class value for each instance\n",
    "                - The predicted class value is appended to the list of predictions\n",
    "                - Convert the list of predictions to a numpy array and returns it as the predicted class values\n",
    "\n",
    "        Returns:\n",
    "            Predicted class values for the given instances [ndarray]\n",
    "        \"\"\"\n",
    "        y_predictions = []\n",
    "        for row in X:\n",
    "            y_predictions.append(self.predict_single_instance(self.final_tree, row))\n",
    "        return np.array(y_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9888888888888889\n"
     ]
    }
   ],
   "source": [
    "train_data = np.loadtxt(\"./data_trees/data.txt\", delimiter=\",\")\n",
    "train_y = np.loadtxt(\"./data_trees/targets.txt\")\n",
    "\n",
    "## Build tree\n",
    "dt = DecisionTree(5, 1)\n",
    "tree = dt.train(train_data)\n",
    "y_pred = dt.predict(train_data)\n",
    "print(f\"Accuracy: {sum(y_pred == train_y) / train_y.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SPLIT NODE: feature #1 < -0.1189 depth: 0\n",
      "\n",
      "\n",
      "SPLIT NODE: feature #0 < 0.3505 depth: 1\n",
      "\n",
      "\n",
      "SPLIT NODE: feature #1 < -1.8785 depth: 2\n",
      "\n",
      "\n",
      "SPLIT NODE: feature #0 < -0.2029 depth: 3\n",
      "\n",
      "\n",
      "SPLIT NODE: feature #0 < 0.0555 depth: 3\n",
      "\n",
      "\n",
      "SPLIT NODE: feature #0 < -0.3927 depth: 4\n",
      "\n",
      "\n",
      "SPLIT NODE: feature #0 < 1.3432 depth: 2\n",
      "\n",
      "\n",
      "SPLIT NODE: feature #1 < -2.1079 depth: 3\n",
      "\n",
      "\n",
      "SPLIT NODE: feature #0 < 0.8358 depth: 4\n",
      "\n",
      "\n",
      "SPLIT NODE: feature #0 < 1.1107 depth: 4\n",
      "\n",
      "\n",
      "SPLIT NODE: feature #0 < 1.4758 depth: 3\n",
      "\n",
      "\n",
      "SPLIT NODE: feature #0 < 1.3432 depth: 4\n",
      "\n",
      "\n",
      "SPLIT NODE: feature #0 < 1.4758 depth: 4\n",
      "\n",
      "\n",
      "SPLIT NODE: feature #1 < 1.8649 depth: 1\n",
      "\n",
      "\n",
      "SPLIT NODE: feature #0 < 1.7032 depth: 2\n",
      "\n",
      "\n",
      "SPLIT NODE: feature #0 < 0.1166 depth: 3\n",
      "\n",
      "\n",
      "SPLIT NODE: feature #0 < -0.5498 depth: 4\n",
      "\n",
      "\n",
      "SPLIT NODE: feature #0 < 0.3832 depth: 4\n",
      "\n",
      "\n",
      "SPLIT NODE: feature #0 < 2.3009 depth: 3\n",
      "\n",
      "\n",
      "SPLIT NODE: feature #0 < 2.3009 depth: 4\n",
      "\n",
      "\n",
      "SPLIT NODE: feature #0 < 0.0988 depth: 2\n",
      "\n",
      "\n",
      "SPLIT NODE: feature #1 < 3.1421 depth: 3\n",
      "\n",
      "\n",
      "SPLIT NODE: feature #0 < -0.0382 depth: 4\n",
      "\n",
      "\n",
      "SPLIT NODE: feature #0 < 2.9737 depth: 3\n",
      "\n",
      "\n",
      "SPLIT NODE: feature #0 < 0.6896 depth: 4\n",
      "\n",
      "\n",
      "SPLIT NODE: feature #0 < 2.9737 depth: 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt.print_decision_tree(tree)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
