{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"line-height:0.45\">\n",
    "<h1 style=\"color:#26BBEE  \"> Linear Regression 0  </h1>\n",
    "</div>\n",
    "<div style=\"line-height:0.5\">\n",
    "<h4> Linear Regression from scratch with numpy. Gradient Descent and Regularization. \n",
    "</h4>\n",
    "<span style=\"display: inline-block;\">\n",
    "    <h3 style=\"color: lightblue; display: inline;\">Keywords:</h3> class inheritance + class magic method __call__matplotlib styles\n",
    "</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "from itertools import combinations_with_replacement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"line-height:0.65\">\n",
    "<h2 style=\"color:#26BBEE  \"> <u> Example class 1 </u> </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinearRegression:\n",
    "    \"\"\" Linear regression model that uses gradient descent to optimize the weights.\n",
    "\n",
    "    Args:\n",
    "        - Whether to print the cost function during training. Default is False [bool, optional]\n",
    "\n",
    "    Attributes:    \n",
    "        - Learning rate used for gradient descent [float] \n",
    "        - Maximum number of iterations for gradient descent [int]\n",
    "        - Whether to print the cost function during training [bool]\n",
    "        - Number of training examples [int]\n",
    "        - Number of features (including the bias term) [int]\n",
    "\n",
    "    Details: \n",
    "        - The learning rate controls the step size in the weight updates\n",
    "        - The update rule adjusts the weights in the direction of steepest descent of the cost function to\\\\\n",
    "        minimize the error between predicted and actual target values.\\\\\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, print_cost=False):\n",
    "        \"\"\" Constructor of the linear regression model. \"\"\"\n",
    "        self.learning_rate = 0.01\n",
    "        self.total_iterations = 1000\n",
    "        self.print_cost = print_cost\n",
    "\n",
    "    def predict(self, X, w):\n",
    "        \"\"\"Predict the target variable using the current weights, calculating the dot product.\n",
    "        \n",
    "        Parameters:\n",
    "                - Feature matrix [ndarray, shape (n, m)]:\\\\\n",
    "                    => n is the number of features (including the bias term).\\\\\n",
    "                    => m is the number of training examples. \n",
    "                - Weights for each feature, including the bias term [ndarray, shape (n, 1)]\n",
    "        \n",
    "        Returns:\n",
    "            Predicted target variable yhat for each training example [ndarray, shape (1, m)]\n",
    "        \"\"\"\n",
    "        return np.dot(w.T, X)\n",
    "\n",
    "    def calculate_cost(self, yhat, y):\n",
    "        \"\"\"Calculate the mean squared error cost function.\n",
    "\n",
    "        Parameters:\n",
    "            - predicted target variable for each training example.\n",
    "            - true target variable for each training example.\n",
    "\n",
    "        Returns:\n",
    "            - Mean squared error cost function C [float]\n",
    "        \"\"\"\n",
    "        C = 1 / self.m * np.sum(np.power(yhat - y, 2))\n",
    "        return C\n",
    "\n",
    "    def gradient_descent(self, w, X, y, yhat):\n",
    "        \"\"\"Perform one iteration of gradient descent to update the weights.\n",
    "\n",
    "        Parameters:\n",
    "            - Weights for each feature, including the bias term [w : ndarray, shape (n, 1)]\n",
    "            - Feature matrix [X : ndarray, shape (n, m)]\n",
    "            - True target variable for each training example [y : ndarray, shape (1, m)]\n",
    "            - Predicted target variable for each training example [yhat : ndarray, shape (1, m)]\n",
    "\n",
    "        Details: \n",
    "            - Calculate the dot product of the input features X and the error vector (yhat - y)\n",
    "            - Update the weights w using the gradient of the cost function (dCdW) and the learning rate\n",
    "        \n",
    "        Returns:\n",
    "            - Updated weights after one iteration of gradient descent.\n",
    "        \"\"\"\n",
    "        dCdW = 2 / self.m * np.dot(X, (yhat - y).T)\n",
    "        w = w - self.learning_rate * dCdW\n",
    "        return w\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the linear regression model to the training data using gradient descent.\n",
    "\n",
    "        Parameters:\n",
    "            - Feature matrix\n",
    "            - True target variable for each training example\n",
    "\n",
    "        Details:\n",
    "            - Add a column of ones to the feature matrix X to represent the bias term in the linear regression model.\\\\\n",
    "                to create a column vector of ones with the same number of columns as X and appends it to X along the first axis (rows).\n",
    "            - Assign the columns (X.shape[1]) and rows (X.shape[0]) in the augmented feature matrix X\\\\\n",
    "            to instance variables m and n, respectively.\n",
    "            - Initialize the weights vector w to a column vector of zeros with the same number of rows as X.\\\\\n",
    "            In each iteration, the predicted values of the target variable (yhat) are computed using the current weights (w)\\\\ \n",
    "            and the augmented feature matrix (X) by calling the predict method.\n",
    "\n",
    "        Returns:\n",
    "            Final weights after training [ndarray, shape (n, 1)]\n",
    "        \"\"\"\n",
    "        ones = np.ones((1, X.shape[1]))\n",
    "        X = np.append(ones, X, axis=0)\n",
    "\n",
    "        self.m = X.shape[1]\n",
    "        self.n = X.shape[0]\n",
    "\n",
    "        w = np.zeros((self.n, 1))\n",
    "\n",
    "        for it in range(self.total_iterations + 1):\n",
    "            yhat = self.predict(X, w)\n",
    "            cost = self.calculate_cost(yhat, y)\n",
    "\n",
    "            if it % 2000 == 0 and self.print_cost:\n",
    "                print(f\"Cost at iteration {it} is {cost}\")\n",
    "\n",
    "            w = self.gradient_descent(w, X, y, yhat)\n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.02665273],\n",
       "       [2.95803314]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Try the defined MyLinearRegression class. \"\"\"\n",
    "X = np.random.rand(1, 500)\n",
    "y = 3 * X + 5 + np.random.randn(1, 500) * 0.1\n",
    "## Train Regressor\n",
    "regression = MyLinearRegression()\n",
    "w = regression.fit(X, y)\n",
    "\n",
    "w"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"line-height:0.65\">\n",
    "<h2 style=\"color:#26BBEE  \"> <u> Example class 2 </u> </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRegression2(object):\n",
    "    \"\"\" Regression basic model (second version)\n",
    "\n",
    "    Parameters:\n",
    "        - Number of iterations for gradient descent [int]\n",
    "        - Learning rate for gradient descent [float]\n",
    "\n",
    "    Attributes:\n",
    "        - n_iterations (int): The number of iterations for gradient descent\n",
    "        - learning_rate (float): The learning rate for gradient descent\n",
    "        - weights (numpy.ndarray): The weight vector used in the regression model\n",
    "        - training_errs (list): A list to store training errors during the fitting process\n",
    "\n",
    "    Methods:\n",
    "        - initialize_weights(self, n_features): Initialize the weights of the model\n",
    "        - fit(self, X, y): Fit the regression model to the training data\n",
    "        - predict(self, X): Predict target values for input data\n",
    "    \"\"\"\n",
    "    def __init__(self, n_iterations, learning_rate):\n",
    "        \"\"\" Constructor of the Regression model being created, to initialize the model with specified parameters. \"\"\"\n",
    "        self.n_iterations = n_iterations  # Set the number of iterations\n",
    "        self.learning_rate = learning_rate  # Set the learning rate\n",
    "\n",
    "    def initialize_weights(self, n_features):\n",
    "        \"\"\" Initialize the weights of the model.\n",
    "\n",
    "        Parameters:\n",
    "            - Number of features in the input data [int]\n",
    "\n",
    "        Details: \n",
    "            - Calculate the limit of weight initialization:\n",
    "                - Scale the weights (taking the reciprocal of the square root) to ensure that weights are scaled\\\\\n",
    "                based on the number of features and not become extremely large.\n",
    "            - Initialize weights randomly within the range of -limit to limit:\n",
    "                - To prevent the model from being stuck in a local minimum during training,\\\\\n",
    "                allowing the model to explore different weight values.\n",
    "        \"\"\"\n",
    "        limit = 1 / math.sqrt(n_features)  \n",
    "        self.weights = np.random.uniform(-limit, limit, (n_features, )) \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\" Fit the Regression model => Training.\n",
    "\n",
    "        Parameters:\n",
    "            - X = Input training data [ndarray]\n",
    "            - y = Label target training data [ndarray]\n",
    "        \n",
    "        Details: \n",
    "            - Insert constant ones for bias weights\n",
    "            - Add a column of ones to input data for bias\n",
    "            - Initialize weights\n",
    "\n",
    "            - Perform gradient descent for a specified number of iterations\n",
    "            - In each interaction\n",
    "                - Make predictions => y_pred\n",
    "                - #*** Calculate MSE (mean squared error) loss with Ridge L2 regularization \n",
    "        \n",
    "                - Compute the gradient directly of the L2 loss with respect to weights (grad_w)\n",
    "                - Update the weights using gradient descent\n",
    "        \"\"\"\n",
    "        self.training_errs = [] \n",
    "        X = np.insert(X, 0, 1, axis=1)  \n",
    "        self.initialize_weights(n_features=X.shape[1])  \n",
    "\n",
    "        for i in range(self.n_iterations):\n",
    "            y_pred = X.dot(self.weights)  \n",
    "            mse = np.mean(0.5 * (y - y_pred)**2 + self.regularization(self.weights)) #***\n",
    "            self.training_errs.append(mse)\n",
    "            \n",
    "            grad_w = -(y - y_pred).dot(X) + self.regularization.grad(self.weights)\n",
    "            self.weights -= self.learning_rate * grad_w\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" Predict the target values for the input data.\n",
    "\n",
    "        Parameters:\n",
    "            - X = Input data for prediction [ndarray]\n",
    "        \n",
    "        Details: \n",
    "            - Insert constant ones for bias weights\n",
    "            - Add a column of  ones to input data for bias\n",
    "            - Make predictions using the learned weights\n",
    "        Returns:\n",
    "            - Predicted target values [ndarray]\n",
    "        \"\"\"\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "        y_pred = X.dot(self.weights)   \n",
    "        return y_pred  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"line-height:0.65\">\n",
    "<h3 style=\"color:#26BBEE  \"> Recap:  </h3>\n",
    "</div>\n",
    "The \"\\__call\\__\" magic method allows instances of a class to be called as callable entity, just like a regular function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class l1_regularization():\n",
    "    \"\"\" Regularization for Lasso Regression.\n",
    "\n",
    "    Args and Attributes:\n",
    "        - Regularization strength alpha [float]\n",
    "\n",
    "    Methods:\n",
    "        - __call__(self, w): Callable to compute the l1 regularization term\n",
    "        - grad(self, w): Compute the gradient of the l1 regularization term\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha):\n",
    "        \"\"\" Constructor to initialize l1_regularization. \"\"\"\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def __call__(self, w):\n",
    "        \"\"\" Compute the l1 regularization term as alpha * L1 norm of the weight vector.\n",
    "        \n",
    "        Parameters:\n",
    "            - Weight vector [ndarray]\n",
    "\n",
    "        Returns:\n",
    "            - Value of the l1 regularization term [float]\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.alpha * np.linalg.norm(w)\n",
    "\n",
    "    def grad(self, w):\n",
    "        \"\"\" Compute the gradient of the l1 regularization term as alpha * sign of the weight vector.\n",
    "\n",
    "        Parameters:\n",
    "            - Weight vector [ndarray]\n",
    "\n",
    "\n",
    "        Returns:\n",
    "            - Gradient of the l1 regularization term [ndarray]\n",
    "        \"\"\"\n",
    "        return self.alpha * np.sign(w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class l2_regularization():\n",
    "    \"\"\"\n",
    "    Regularization for Ridge Regression.\n",
    "\n",
    "    Args and Attributes:\n",
    "        - Regularization strength alpha [float]\n",
    "    \n",
    "    Attributes:\n",
    "        - Regularization strength alpha [float]\n",
    "\n",
    "    Methods:\n",
    "        - __call__(self, w): Callable to compute the l2 regularization term\n",
    "        - grad(self, w): Compute the gradient of the l2 regularization term\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha):\n",
    "        \"\"\" Constructor to initialize l2_regularization. \"\"\"\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def __call__(self, w):\n",
    "        \"\"\" Compute the l2 regularization term.\n",
    "\n",
    "        Parameters:\n",
    "            - Weight vector [ndarray]\n",
    "\n",
    "        Returns:\n",
    "            - Value of the l2 regularization term [float]\n",
    "        \"\"\"\n",
    "        # Compute the l2 regularization term as alpha * 0.5 * transpose(w) * w\n",
    "        return self.alpha * 0.5 * w.T.dot(w)\n",
    "\n",
    "    def grad(self, w):\n",
    "        \"\"\" Compute the gradient of the l2 regularization term as alpha * weight vector.\n",
    "\n",
    "        Parameters:\n",
    "            - Weight vector [ndarray]\n",
    "\n",
    "        Returns:\n",
    "            - Gradient of the l2 regularization term [ndarray]\n",
    "        \"\"\"\n",
    "        return self.alpha * w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class l1_l2_regularization():\n",
    "    \"\"\" Regularization for Elastic Net Regression.\n",
    "\n",
    "    Args and Attributes:\n",
    "        - Regularization strength alpha [float]\n",
    "        - Ratio between l1 and l2 regularization (default is 0.5) [float, optional]\n",
    "\n",
    "    Attributes:\n",
    "        - Regularization strength alpha [float]\n",
    "        - Ratio between l1 and l2 regularization (default is 0.5) [float]\n",
    "\n",
    "    Methods:\n",
    "        - __call__(self, w): Callable method to compute the elastic net regularization term\n",
    "        - grad(self, w): Compute the gradient of the elastic net regularization term\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha, l1_ratio=0.5):\n",
    "        \"\"\" Constructor to initialize l1_l2_regularization. \"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.l1_ratio = l1_ratio\n",
    "\n",
    "    def __call__(self, w):\n",
    "        \"\"\" Compute the elastic net regularization term as alpha * (l1 contribution + l2 contribution).\n",
    "\n",
    "        Parameters:\n",
    "            - Weight vector [ndarray]\n",
    "\n",
    "        Returns:\n",
    "            - Value of the elastic net regularization term [float]\n",
    "        \"\"\"\n",
    "        l1_contr = self.l1_ratio * np.linalg.norm(w)\n",
    "        l2_contr = (1 - self.l1_ratio) * 0.5 * w.T.dot(w)\n",
    "        return self.alpha * (l1_contr + l2_contr)\n",
    "\n",
    "    def grad(self, w):\n",
    "        \"\"\" Compute the gradient of the elastic net regularization term as alpha * (l1 contribution + l2 contribution).\n",
    "\n",
    "        Parameters:\n",
    "        - w (numpy.ndarray): Weight vector.\n",
    "\n",
    "        Returns:\n",
    "        - numpy.ndarray: Gradient of the elastic net regularization term.\n",
    "        \"\"\"\n",
    "        l1_contr = self.l1_ratio * np.sign(w)\n",
    "        l2_contr = (1 - self.l1_ratio) * w\n",
    "        return self.alpha * (l1_contr + l2_contr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X, axis=-1, order=2):\n",
    "    \"\"\" Normalize the dataset X.\n",
    "\n",
    "    Parameters:\n",
    "        - Input dataset X [ndarray]\n",
    "        - Axis along which normalization is performed (default is -1, last axis) [int]\n",
    "        - Order of the normalization (default is 2 => L2 normalization) [int]\n",
    "\n",
    "    Details: \n",
    "        - Calculate the L2 norms along the specified axis (default is last axis)\n",
    "        - Ensure that the computed L2 norms are not zero (replace zeros with ones to avoid division by zero)\n",
    "        - Normalize the input dataset by dividing it by the computed L2 norms\n",
    "\n",
    "    Returns:\n",
    "        Normalized dataset [ndarray]\n",
    "    \"\"\"\n",
    "    l2 = np.atleast_1d(np.linalg.norm(X, order, axis))\n",
    "    l2[l2 == 0] = 1\n",
    "    return X / np.expand_dims(l2, axis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(X):\n",
    "    \"\"\" Standardize the dataset X \n",
    "    \n",
    "    Parameters:\n",
    "        - Input dataset [ndarray]\n",
    "\n",
    "    Details: \n",
    "        - Create a copy of the input dataset where store the standardized values\n",
    "        - Calculate the mean values for each feature (column)\n",
    "        - Get the standard deviation for each feature (column)\n",
    "\n",
    "        - For each column in the dataset and standardize the values\n",
    "            - Check if the standard deviation of the current column is non-zero, to avoid division by zero!\n",
    "                - Standardize the column by subtracting the mean and dividing by the standard deviation\n",
    "                    - Center the column's values around zero and scales them by the standard deviation, making them unit-variance.\n",
    "                        - [:, col] => select all rows (samples) of the column \"col\"\n",
    "                        - Subtract the mean value of the current column from each element in that column\n",
    "                        - Divides the result by the standard deviation of the column\n",
    "\n",
    "    Returns:        \n",
    "        Standardized dataset [ndarray]\n",
    "    \n",
    "    \"\"\"\n",
    "    X_std = X\n",
    "    mean = X.mean(axis=0)\n",
    "    std = X.std(axis=0)\n",
    "    for col in range(np.shape(X)[1]):\n",
    "        if std[col]:\n",
    "            X_std[:, col] = (X_std[:, col] - mean[col]) / std[col]\n",
    "    # X_std = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "    return X_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, y, test_size=0.5, shuffle=True, seed=None):\n",
    "    \"\"\" Split the data into train and test sets.\n",
    "\n",
    "    Parameters:\n",
    "        - Input dataset [ndarray]\n",
    "        - target label dataset [ndarray]\n",
    "        - test_size => Proportion of data to be used as the test set (default is 0.5) [float, optional]\n",
    "        - shuffle option (before splitting) (default is True) [float, optional]\n",
    "        - Seed for random shuffling (default is None) [int, optional]\n",
    "\n",
    "    Details:\n",
    "        - Shuffle the input data and corresponding targets (only if \"shuffle\" is True)\n",
    "        - Compute the index to split the data into training and test sets based on the specified test_size: \n",
    "            - The reciprocal test_size is used to determine the ratio of the training set size to the test set size \n",
    "            - Get an integer index with a Floor (integer) division // of the total number of samples by the reciprocal of test_size.\\\\\n",
    "            to get the size of the test set based on the specified test_size as a fraction of the total dataset size. \n",
    "            \n",
    "        - Split the input data and targets into training and test sets as usual\n",
    "        \n",
    "    Returns:\n",
    "        - Training data (X_train) [ndarray]\n",
    "        - Test data (X_test) [ndarray]\n",
    "        - Training targets (y_train) [ndarray]\n",
    "        - Test targets (y_test) [ndarray]\n",
    "    \"\"\"\n",
    "    if shuffle:\n",
    "        X, y = shuffle_data(X, y, seed)\n",
    "    # Compute the index for splitting\n",
    "    split_i = len(y) - int(len(y) // (1 / test_size))\n",
    "    ## Split\n",
    "    X_train, X_test = X[:split_i], X[split_i:]\n",
    "    y_train, y_test = y[:split_i], y[split_i:]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_features(X, degree):\n",
    "    \"\"\" Generate polynomial features up to the given degree.\n",
    "\n",
    "    Parameters:\n",
    "        - Input dataset [ndarray]\n",
    "        - Degree of the polynomial features [int]\n",
    "\n",
    "    Details: \n",
    "        - Get the number of samples and features in the input dataset\n",
    "        - Define a function in a function => to generate combinations of indexes\n",
    "        \n",
    "        - Get the list of index combinations for polynomial features\n",
    "        - Calculate the number of output features after adding polynomial features\n",
    "        - Create an empty array to store the dataset with polynomial features\n",
    "\n",
    "        - For each combination:\n",
    "            - Generate polynomial features by taking the product of the input features at the selected indices\n",
    "\n",
    "    Returns:\n",
    "        Dataset with polynomial features up to the specified degree [dataset]\n",
    "    \n",
    "    \"\"\"    \n",
    "    n_samples, n_features = np.shape(X)\n",
    "\n",
    "    def index_combinations():\n",
    "        \"\"\" Generate combinations with replacement for each degree from 0 to the specified degree. \"\"\"\n",
    "        combs = [combinations_with_replacement(range(n_features), i) for i in range(0, degree + 1)]\n",
    "        # Flatten the list of combinations\n",
    "        flat_combs = [item for sublist in combs for item in sublist]\n",
    "        return flat_combs\n",
    "    \n",
    "    combinations = index_combinations()\n",
    "    n_output_features = len(combinations)\n",
    "    X_new = np.empty((n_samples, n_output_features))\n",
    "    \n",
    "    for i, index_combs in enumerate(combinations):  \n",
    "        X_new[:, i] = np.prod(X[:, index_combs], axis=1)\n",
    "\n",
    "    return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression2(MyRegression2):\n",
    "    \"\"\" Linear regression model that extends the base Regression class.\\\\\n",
    "        Inherit the predict method from the father,\\\\\n",
    "        but it has its implementation of \"fit\", to train the model based on the value of the gradient_descent attribute. \n",
    "\n",
    "        Args:\n",
    "            - Number of iterations for gradient descent [int]\n",
    "            - Learning rate for gradient descent [float]\n",
    "            - Indication to use gradient descent or not [bool]\n",
    "\n",
    "        Attributes:\n",
    "            - Gradient descent option for fitting [bool]\n",
    "            - Regularization function (initialized as zero function)\n",
    "            - Weight vector used in the linear regression model [ndarray]\n",
    "\n",
    "        Methods:            \n",
    "            - fit(self, X, y): Fit the linear regression model\n",
    "        \"\"\"\n",
    "    def __init__(self, n_iterations=100, learning_rate=0.001, gradient_descent=True):\n",
    "        \"\"\" Constructor to initialize the LinearRegression2 model. \n",
    "        \n",
    "            Details: \n",
    "                - Initialize:\n",
    "                    - regularization as zero function\n",
    "                    - regularization gradient as zero function\n",
    "                    - LinearRegression2 by calling the constructor of the base Regression class\n",
    "        \"\"\"\n",
    "        self.gradient_descent = gradient_descent\n",
    "        \n",
    "        self.regularization = lambda x: 0\n",
    "        self.regularization.grad = lambda x: 0\n",
    "        super(LinearRegression2, self).__init__(n_iterations=n_iterations, learning_rate=learning_rate)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\" Fit the model to the training data.\n",
    "\n",
    "        Parameters:\n",
    "            - Input training data [ndarray]\n",
    "            - Target training data [ndarray]\n",
    "\n",
    "        Details: \n",
    "            - if not self.gradient_descent => closed-form solution => Least squares approximation of w, without iterative optimization: \n",
    "                - Add a column of constant ones to input data for bias \n",
    "                - Compute weights by least squares (using Moore-Penrose pseudoinverse)\n",
    "                    - Singular Value Decomposition (SVD) of the matrix factorization \n",
    "                        - X^T * X (dot product of the transposed matrix X with itself)\\\\\n",
    "                        The result is a square matrix (symmetric positive semidefinite),\\\\ \n",
    "                        often used in linear optimization problems\n",
    "                    - U, S, and V are matrices representing the singular value decomposition:\n",
    "                        - U = unit matrix (columns are orthogonal unit vectors);\n",
    "                        - S = diagonal matrix containing the singular values of A;\n",
    "                        - V = orthogonal matrix that contains the right singular vectors.\n",
    "                            - a set of orthonormal basis vectors that span the column space (range) of original input matrix;\n",
    "                            - V provides a transformation that maps the original feature space into a new orthogonal coordinate system,\\\\\n",
    "                            where each dimension is represented by a right singular vector.\\\\\n",
    "                            The singular values in the diagonal matrix S determine the scaling along each of these dimensions. \n",
    "                \n",
    "                - Convert the singular values to a diagonal matrix\n",
    "                - Calculate regularized inverse of the product of V, the pseudo-inverse of S, and the transpose of U \n",
    "                - Compute weights using closed-form solution\n",
    "            \n",
    "            - else:\n",
    "                - Fit the base Regression class\n",
    "        \"\"\"        \n",
    "        if not self.gradient_descent:\n",
    "            X = np.insert(X, 0, 1, axis=1)\n",
    "            U, S, V = np.linalg.svd(X.T.dot(X))\n",
    "            S = np.diag(S)\n",
    "            X_sq_reg_inv = V.dot(np.linalg.pinv(S)).dot(U.T)\n",
    "            self.weights = X_sq_reg_inv.dot(X.T).dot(y)\n",
    "        else:\n",
    "            super(LinearRegression2, self).fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LassoRegression(MyRegression2):\n",
    "    \"\"\" Lasso Regression model, with a regularization factor which does both variable selection and regularization.\\\\\n",
    "        Introducing bias to reduce variance with l1 regularization\n",
    "    \n",
    "    Args:\n",
    "        - Degree of the polynomial that the independent variable X will be transformed to [int]\n",
    "        - Regularization factor for L1 regularization [float]\n",
    "        - Number of iterations for gradient descent (default is 3000) [int, optional]\n",
    "        - Learning rate for gradient descent (default is 0.01) [float, optional]\n",
    "\n",
    "    Attributes:\n",
    "        - Degree of polynomial features [int]\n",
    "        - Regularization factor for L1 regularization [float]\n",
    "        - Number of iterations for gradient descent (default is 3000) [int, optional]\n",
    "        - Learning rate for gradient descent (default is 0.01) [float, optional]\n",
    "\n",
    "    Methods:\n",
    "        - fit(self, X, y): Fit the Lasso Regression model to the data\n",
    "        - predict(self, X): Make predictions using the Lasso Regression model\n",
    "    \"\"\"\n",
    "    def __init__(self, degree, reg_factor, n_iterations=3000, learning_rate=0.01):\n",
    "        \"\"\" Constructor to initialize LassoRegression. \"\"\"\n",
    "        self.degree = degree\n",
    "        self.regularization = l1_regularization(alpha=reg_factor)\n",
    "        super(LassoRegression, self).__init__(n_iterations, learning_rate)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\" Fit the Lasso Regression model to the data.\n",
    "\n",
    "        Details:\n",
    "            - Normalize input data and create polynomial features\n",
    "            - call the parent fit method\n",
    "        Parameters:\n",
    "            - Input data [ndarray]\n",
    "            - Target values [ndarray]\n",
    "        \"\"\"\n",
    "        X = normalize(polynomial_features(X, degree=self.degree))\n",
    "        super(LassoRegression, self).fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" Predict data the Lasso Regression model.\n",
    "\n",
    "        Parameters:\n",
    "            - (new) Input data [ndarray]\n",
    "        \n",
    "        Details: \n",
    "            - Normalize input data and create polynomial features for prediction\n",
    "            - Call the parent predict method\n",
    "\n",
    "        Returns:\n",
    "            - Predicted target values [ndarray]\n",
    "        \"\"\"\n",
    "        X = normalize(polynomial_features(X, degree=self.degree))\n",
    "        return super(LassoRegression, self).predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolynomialRegression(MyRegression2):\n",
    "    \"\"\" Polynomial Regression model for non-linear regression.\n",
    "\n",
    "    Args:\n",
    "        - Degree of polynomial features [int] \n",
    "        - Number of iterations for gradient descent (default is 3000) [int, optional]\n",
    "        - Learning rate for gradient descent (default is 0.001) [float, optional]\n",
    "\n",
    "    Attributes:\n",
    "        - Degree of polynomial features [int] \n",
    "        - Number of iterations for gradient descent (default is 3000) [int, optional]\n",
    "        - Learning rate for gradient descent (default is 0.001) [float, optional]\n",
    "\n",
    "    Methods:\n",
    "        - fit(self, X, y): Fit the Polynomial Regression model to the data\n",
    "        - predict(self, X): Make predictions using the Polynomial Regression model\n",
    "    \"\"\"\n",
    "    def __init__(self, degree, n_iterations=3000, learning_rate=0.001):\n",
    "        \"\"\" Constructor to initialize PolynomialRegression. \"\"\"\n",
    "        self.degree = degree\n",
    "        self.regularization = lambda x: 0\n",
    "        self.regularization.grad = lambda x: 0\n",
    "        super(PolynomialRegression, self).__init__(n_iterations=n_iterations, learning_rate=learning_rate)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\" Fit the Polynomial Regression model to the data.\n",
    "\n",
    "        Parameters:\n",
    "            - Input data [ndarray]\n",
    "            - Target values [ndarray]\n",
    "        \"\"\"\n",
    "        # Create polynomial features\n",
    "        X = polynomial_features(X, degree=self.degree)\n",
    "        super(PolynomialRegression, self).fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" Make predictions using the Polynomial Regression model.\n",
    "\n",
    "        Parameters:\n",
    "            - (New) Input data [ndarray]\n",
    "\n",
    "        Returns:\n",
    "            - Predicted target values.\n",
    "        \"\"\"\n",
    "        # Create polynomial features for prediction\n",
    "        X = polynomial_features(X, degree=self.degree)\n",
    "        return super(PolynomialRegression, self).predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgeRegression(MyRegression2):\n",
    "    \"\"\" Ridge Regression model (L2 - Tikhonov regularization).\\\\\n",
    "    Balance the fit of the model with respect to the training data and the complexity of the model,\\\\\n",
    "    adding bias to decrease the variance of the model.\n",
    "\n",
    "    Args:\n",
    "        - Regularization factor for L2 regularization [float]\n",
    "        - Number of iterations for gradient descent (default is 1000) [int, optional]\n",
    "        - Learning rate for gradient descent (default is 0.001) [float, optional]\n",
    "\n",
    "    Attributes:\n",
    "        - Regularization factor for L2 regularization [float]\n",
    "        - Number of iterations for gradient descent (default is 1000) [int, optional]\n",
    "        - Learning rate for gradient descent (default is 0.001) [float, optional]\n",
    "    \"\"\"\n",
    "    def __init__(self, reg_factor, n_iterations=1000, learning_rate=0.001):\n",
    "        \"\"\" Constructor to initialize RidgeRegression. \"\"\"\n",
    "        self.regularization = l2_regularization(alpha=reg_factor)\n",
    "        super(RidgeRegression, self).__init__(n_iterations, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElasticNet(MyRegression2):\n",
    "    \"\"\" Elastic Net Regression model.\n",
    "\n",
    "    Args:\n",
    "        - Degree of polynomial features [int]\n",
    "        - Regularization factor for Elastic Net regularization [float]\n",
    "        - L1 ratio for Elastic Net regularization [float]\n",
    "        - Number of iterations for gradient descent (default is 3000) [int, optional]\n",
    "        - Learning rate for gradient descent (default is 0.01) [float, optional]\n",
    "\n",
    "    Attributes:\n",
    "        - Degree of polynomial features [int]\n",
    "        - Regularization factor for Elastic Net regularization [float]\n",
    "        - L1 ratio for Elastic Net regularization [float]\n",
    "        - Number of iterations for gradient descent (default is 3000) [int, optional]\n",
    "        - Learning rate for gradient descent (default is 0.01) [float, optional]\n",
    "\n",
    "    Methods:\n",
    "        - fit(self, X, y): Fit the Elastic Net Regression model to the data\n",
    "        - predict(self, X): Make predictions using the Elastic Net Regression model\n",
    "    \"\"\"\n",
    "    def __init__(self, degree=1, reg_factor=0.05, l1_ratio=0.5, n_iterations=3000, learning_rate=0.01):\n",
    "        \"\"\" Constructor to initialize ElasticNet. \"\"\"\n",
    "        self.degree = degree\n",
    "        self.regularization = l1_l2_regularization(alpha=reg_factor, l1_ratio=l1_ratio)\n",
    "        super(ElasticNet, self).__init__(n_iterations, learning_rate)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\" Fit the Elastic Net Regression model to the data => Training\n",
    "\n",
    "        Parameters:\n",
    "            - Input data [ndarray]\n",
    "            - Target values [ndarray]\n",
    "        \"\"\"\n",
    "        # Normalize input data and create polynomial features\n",
    "        X = normalize(polynomial_features(X, degree=self.degree))\n",
    "        super(ElasticNet, self).fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" Make predictions using the Elastic Net Regression model.\n",
    "\n",
    "        Parameters:\n",
    "            - Input data [ndarray]\n",
    "\n",
    "        Returns:\n",
    "            - Predicted target values [ndarray]\n",
    "        \"\"\"\n",
    "        # Normalize input data and create polynomial features for prediction\n",
    "        X = normalize(polynomial_features(X, degree=self.degree))\n",
    "        return super(ElasticNet, self).predict(X)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
