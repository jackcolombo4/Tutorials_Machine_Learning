{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"line-height:0.5\">\n",
    "<h1 style=\"color:#E74C3C\"> Data preprocessing 3 </h1>\n",
    "</div>\n",
    "<div style=\"line-height:1.2\">\n",
    "<h4> Scaling and standardizing features with sklearn.preprocessing. </h4>\n",
    "</div>\n",
    "<div style=\"margin-top: 5px;\">\n",
    "<span style=\"display: inline-block;\">\n",
    "    <h3 style=\"color: lightblue; display: inline;\">Keywords:</h3>  Binarizer + fit_transform\n",
    "</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import scale, normalize\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler, PolynomialFeatures, KBinsDiscretizer\n",
    "from sklearn.preprocessing import PolynomialFeatures, KBinsDiscretizer, Binarizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.datasets import make_regression, load_breast_cancer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#E74C3C\"> => Binarization </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Binarized data:\n",
      " [[1. 1. 1. 0.]\n",
      " [1. 1. 1. 0.]\n",
      " [1. 0. 1. 0.]\n",
      " [1. 1. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Binarization is used when it is necessary to convert our numerical values into Boolean values.\"\"\"\n",
    "\n",
    "data_inp = np.array([[2.3, 4.3, 6.4, -1.1],\n",
    "                    [1.5, 5.7, 8.2, -6.3], \n",
    "                    [3.3, -6.3, 3.5, -4.5],\n",
    "                    [7.8, 2.1, -2.2, 1.3]])\n",
    "data_binarized = Binarizer(threshold=0.5).transform(data_inp)\n",
    "print(\"\\nBinarized data:\\n\", data_binarized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#E74C3C\"> => Standarization </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Min max scaled data:\n",
      " [[0.2015873  0.80666667 0.76153846 0.64736842]\n",
      " [0.1        0.9        0.9        0.1       ]\n",
      " [0.32857143 0.1        0.53846154 0.28947368]\n",
      " [0.9        0.66       0.1        0.9       ]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Minmax scaler estimator! transform (scales and translates) features individually by scaling each feature to a given range.\n",
    "    e.g. between zero and one.\n",
    "    The transformation is given by::\n",
    "        X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
    "        X_scaled = X_std * (max - min) + min\n",
    "\"\"\"\n",
    "\n",
    "data_inp = np.array([[2.3, 4.3, 6.4, -1.1],\n",
    "                    [1.5, 5.7, 8.2, -6.3], \n",
    "                    [3.3, -6.3, 3.5, -4.5],\n",
    "                    [7.8, 2.1, -2.2, 1.3]])\n",
    "\n",
    "data_scaler_minmax = MinMaxScaler(feature_range=(0.1, 0.9))\n",
    "#data_scaler_minmax.fit(data_inp)\n",
    "#data_scaled = data_scaler_minmax.transform(data_inp)\n",
    "data_scaler_minmax = data_scaler_minmax.fit_transform(data_inp)\n",
    "print (\"\\nMin max scaled data:\\n\", data_scaler_minmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean = [ 1.75  -1.275  2.2  ]\n",
      "Stddeviation =  [2.71431391 4.20022321 4.69414529]\n",
      "Mean_removed = [1.11022302e-16 0.00000000e+00 0.00000000e+00]\n",
      "Stddeviation_removed = [1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Remove mean \"\"\"\n",
    "\n",
    "example_data = np.array([[2.1, -1.9, 5.5],\n",
    "                      [-1.5, 2.4, 3.5],\n",
    "                      [0.5, -7.9, 5.6],\n",
    "                      [5.9, 2.3, -5.8]])\n",
    "## Display the mean and the standard deviation of the input data\n",
    "print(\"Mean =\", example_data.mean(axis=0)) \n",
    "print(\"Stddeviation = \", example_data.std(axis=0))\n",
    "## Remove the mean and the standard deviation of the input data\n",
    "data_scaled = scale(example_data)\n",
    "print(\"Mean_removed =\", data_scaled.mean(axis=0))\n",
    "print(\"Stddeviation_removed =\", data_scaled.std(axis=0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Standardize features by removing the mean and scaling to unit variance.\n",
    "\n",
    "    The standard score of a sample `x` is calculated as:\n",
    "\n",
    "        z = (x - u) / s\n",
    "\n",
    "    where `u` is the mean of the training samples or zero if `with_mean=False`, \\\n",
    "    and `s` is the standard deviation of the training samples or one if `with_std=False`. \n",
    "\n",
    "2) RobustScaler (robust to outliers) removes the median and scales the data according to \n",
    "    the quantile range (defaults to IQR: Interquartile Range). \\\n",
    "    The IQR is the range between the 1st quartile (25th quantile)\n",
    "    and the 3rd quartile (75th quantile).\n",
    "\n",
    "3) MaxAbsScaler scale each feature by its maximum absolute value.It does not shift/center the data, and\n",
    "    thus does not destroy any sparsity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_std_scaled\n",
      " [[ 0.12894603 -0.14880162  0.70300338]\n",
      " [-1.19735598  0.8749535   0.27694073]\n",
      " [-0.46052153 -1.57729713  0.72430651]\n",
      " [ 1.52893149  0.85114524 -1.70425062]]\n",
      "data_scaled1\n",
      " [[0.48648649 0.58252427 0.99122807]\n",
      " [0.         1.         0.81578947]\n",
      " [0.27027027 0.         1.        ]\n",
      " [1.         0.99029126 0.        ]]\n",
      "data_scaled2\n",
      " [[ 0.26229508 -0.36681223  0.22988506]\n",
      " [-0.91803279  0.38427948 -0.22988506]\n",
      " [-0.26229508 -1.41484716  0.25287356]\n",
      " [ 1.50819672  0.36681223 -2.36781609]]\n",
      "data_scaled3\n",
      " [[ 0.3559322  -0.24050633  0.94827586]\n",
      " [-0.25423729  0.30379747  0.60344828]\n",
      " [ 0.08474576 -1.          0.96551724]\n",
      " [ 1.          0.29113924 -1.        ]]\n"
     ]
    }
   ],
   "source": [
    "std_scaler = StandardScaler()\n",
    "data_std_scaled = std_scaler.fit_transform(example_data)\n",
    "\n",
    "scaler1 = MinMaxScaler()\n",
    "data_scaled1 = scaler1.fit_transform(example_data)\n",
    "scaler2 = RobustScaler()\n",
    "data_scaled2 = scaler2.fit_transform(example_data)\n",
    "scaler3 = MaxAbsScaler()\n",
    "data_scaled3 = scaler3.fit_transform(example_data)\n",
    "\n",
    "print(f\"data_std_scaled\\n {data_std_scaled}\")\n",
    "print(f\"data_scaled1\\n {data_scaled1}\")\n",
    "print(f\"data_scaled2\\n {data_scaled2}\")\n",
    "print(f\"data_scaled3\\n {data_scaled3}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#E74C3C\"> => Imputation </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.1, -1.9,  5.5],\n",
       "       [-1.5,  2.4,  3.5],\n",
       "       [ 0.5, -7.9,  5.6],\n",
       "       [ 5.9,  2.3, -5.8]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create imputer obj\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "data_imputed1 = imputer.fit_transform(example_data)\n",
    "data_imputed1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#E74C3C\"> => Discretization </h3>\n",
    "Technique used to convert continuous data into discrete data by dividing the data into bins or categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 2.],\n",
       "       [0., 2., 2.],\n",
       "       [0., 0., 2.],\n",
       "       [2., 2., 0.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create discretizer obj\n",
    "discretizer = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n",
    "data_discretized = discretizer.fit_transform(example_data)\n",
    "data_discretized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#E74C3C\"> => Polynomial features </h3>\n",
    "Technique used to create polynomial features from the existing features to capture non-linear relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_poly\n",
      "[[  1.     2.1   -1.9    5.5    4.41  -3.99  11.55   3.61 -10.45  30.25]\n",
      " [  1.    -1.5    2.4    3.5    2.25  -3.6   -5.25   5.76   8.4   12.25]\n",
      " [  1.     0.5   -7.9    5.6    0.25  -3.95   2.8   62.41 -44.24  31.36]\n",
      " [  1.     5.9    2.3   -5.8   34.81  13.57 -34.22   5.29 -13.34  33.64]]\n"
     ]
    }
   ],
   "source": [
    "# Polynomial features\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "data_poly = poly.fit_transform(example_data)\n",
    "print(\"data_poly\")\n",
    "print(data_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_poly\n",
      " [[ 1.          0.34361829  0.11807353  0.04057222]\n",
      " [ 1.         -1.01283112  1.02582688 -1.03898939]\n",
      " [ 1.         -0.60063869  0.36076684 -0.21669052]\n",
      " [ 1.          1.52302986  2.31961994  3.53285043]]\n",
      "X_test_poly\n",
      " [[ 1.         -1.32818605  1.76407818 -2.34302403]\n",
      " [ 1.          1.47789404  2.18417081  3.22797303]\n",
      " [ 1.          0.81252582  0.66019821  0.5364281 ]\n",
      " [ 1.         -0.39210815  0.1537488  -0.06028616]]\n",
      "\n",
      "-------------------------------------------------------\n",
      "Linear regression score: 0.9374151607623286\n",
      "Polynomial regression score: 0.9365814213479909\n"
     ]
    }
   ],
   "source": [
    "## Generate a random regression dataset\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n",
    "\n",
    "## Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "## Create polynomial features up to degree 3\n",
    "poly = PolynomialFeatures(degree=3)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "print(f\"X_train_poly\\n {X_train_poly[:4]}\")\n",
    "print(f\"X_test_poly\\n {X_test_poly[:4]}\")\n",
    "print()\n",
    "print(\"-------------------------------------------------------\")\n",
    "\n",
    "## Fit a linear regression model on the original features\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "## Evaluate the linear regression model on the test set\n",
    "lr_score = lr.score(X_test, y_test)\n",
    "print(\"Linear regression score:\", lr_score)\n",
    "\n",
    "## Fit a linear regression model on the polynomial features\n",
    "lr_poly = LinearRegression()\n",
    "lr_poly.fit(X_train_poly, y_train)\n",
    "\n",
    "## Evaluate the polynomial regression model on the test set\n",
    "lr_poly_score = lr_poly.score(X_test_poly, y_test)\n",
    "print(\"Polynomial regression score:\", lr_poly_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#E74C3C\"> => Normalization </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "L1 normalized data:\n",
      " [[ 0.22105263 -0.2         0.57894737]\n",
      " [-0.2027027   0.32432432  0.47297297]\n",
      " [ 0.03571429 -0.56428571  0.4       ]\n",
      " [ 0.42142857  0.16428571 -0.41428571]]\n",
      "\n",
      "L1 normalized data:\n",
      " [[ 0.33946114 -0.30713151  0.88906489]\n",
      " [-0.33325106  0.53320169  0.7775858 ]\n",
      " [ 0.05156558 -0.81473612  0.57753446]\n",
      " [ 0.68706914  0.26784051 -0.6754239 ]]\n",
      "\n",
      "###########################################################\n",
      "\n",
      "Original data:\n",
      " [[0.17698986 0.01327682 0.46659534]\n",
      " [0.70704891 0.9379844  0.27753776]\n",
      " [0.9209406  0.77397533 0.45378789]\n",
      " [0.89133978 0.19404063 0.31596212]]\n",
      "\n",
      "L1 normalized data:\n",
      " [[0.26944755 0.02021249 0.71033996]\n",
      " [0.36776217 0.48788022 0.14435761]\n",
      " [0.42860286 0.36020568 0.21119146]\n",
      " [0.63606132 0.13846767 0.22547102]]\n",
      "\n",
      "L2 normalized data:\n",
      " [[0.35453828 0.02659553 0.93466319]\n",
      " [0.58580819 0.77714418 0.22994716]\n",
      " [0.71628028 0.60197505 0.35294276]\n",
      " [0.92329869 0.20099794 0.32729092]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Lasso l1 and Ridge l2 \n",
    "Neccesary to modify the feature vectors, so that the feature vectors can be measured at common scale.\n",
    "L1 and L2 are types of vector norms that are used to measure the magnitude of a vector, \n",
    "used to scale the data so that the sum of the absolute values (L1 norm) or the sum of the squared values (L2 norm).\n",
    "\"\"\"\n",
    "\n",
    "dat = np.array([[2.1, -1.9, 5.5],\n",
    "                    [-1.5, 2.4, 3.5],\n",
    "                    [0.5, -7.9, 5.6],\n",
    "                    [5.9, 2.3, -5.8]])\n",
    "\n",
    "data_normalized_l1 = normalize(dat, norm='l1')\n",
    "data_normalized_l2 = normalize(dat, norm='l2')\n",
    "print(\"\\nL1 normalized data:\\n\", data_normalized_l1)\n",
    "print(\"\\nL1 normalized data:\\n\", data_normalized_l2)\n",
    "\n",
    "print()\n",
    "print(\"###########################################################\")\n",
    "print()\n",
    "X = np.random.rand(4, 3)\n",
    "\n",
    "X_norm_l1 = normalize(X, norm='l1')\n",
    "X_norm_l2 = normalize(X, norm='l2')\n",
    "\n",
    "print(\"Original data:\\n\", X)\n",
    "print(\"\\nL1 normalized data:\\n\", X_norm_l1)\n",
    "print(\"\\nL2 normalized data:\\n\", X_norm_l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_norm_l1 ==>\n",
      " [[1.15192592e-02 2.21097310e-02 7.50046789e-02 ... 2.23266181e-04\n",
      "  5.39411094e-04 1.49907293e-04]\n",
      " [5.38293180e-03 6.78162627e-03 3.64222081e-02 ... 7.40950736e-05\n",
      "  1.04595801e-04 3.27723302e-05]\n",
      " [1.17838189e-02 1.78048327e-02 7.60495018e-02 ... 6.53486175e-05\n",
      "  4.21612272e-04 1.09064235e-04]\n",
      " ...\n",
      " [9.03326722e-03 1.06325791e-02 5.70821575e-02 ... 2.10691950e-05\n",
      "  1.55379782e-04 3.86869107e-05]\n",
      " [7.85783596e-03 1.10279500e-02 5.12164530e-02 ... 1.02691461e-04\n",
      "  1.78684267e-04 5.92991197e-05]\n",
      " [9.62106825e-03 1.62088933e-02 6.09966248e-02 ... 5.86979952e-05\n",
      "  2.12800968e-04 5.43298090e-05]]\n",
      "\n",
      "X_test_norm_l1 ==>\n",
      " [[8.63782899e-03 1.28840112e-02 5.61701325e-02 ... 7.03079104e-05\n",
      "  2.08776396e-04 6.06102676e-05]\n",
      " [5.43924641e-03 6.11987017e-03 3.54958214e-02 ... 5.13770424e-05\n",
      "  7.32603886e-05 1.89224892e-05]\n",
      " [6.82127280e-03 8.59498021e-03 4.48721503e-02 ... 6.68008216e-05\n",
      "  1.25174327e-04 3.53814920e-05]\n",
      " ...\n",
      " [1.01387941e-02 1.31399475e-02 6.50132567e-02 ... 8.45603587e-05\n",
      "  2.34459612e-04 6.87272941e-05]\n",
      " [8.27863026e-03 1.62137730e-02 5.38809585e-02 ... 4.78495514e-05\n",
      "  1.10032427e-04 4.53869209e-05]\n",
      " [3.73887173e-03 5.61281661e-03 2.44749105e-02 ... 2.99217955e-05\n",
      "  5.17273715e-05 1.48220202e-05]]\n",
      "\n",
      "X_train_norm_l2 ==>\n",
      " [[2.14461549e-02 4.11631259e-02 1.39641095e-01 ... 4.15669188e-04\n",
      "  1.00425676e-03 2.79092169e-04]\n",
      " [8.50859961e-03 1.07194638e-02 5.75712264e-02 ... 1.17119320e-04\n",
      "  1.65330684e-04 5.18020005e-05]\n",
      " [2.20318191e-02 3.32891107e-02 1.42187255e-01 ... 1.22180163e-04\n",
      "  7.88274613e-04 2.03913817e-04]\n",
      " ...\n",
      " [1.51713721e-02 1.78574163e-02 9.58694821e-02 ... 3.53857125e-05\n",
      "  2.60960340e-04 6.49746656e-05]\n",
      " [1.31018761e-02 1.83876115e-02 8.53964912e-02 ... 1.71224089e-04\n",
      "  2.97931788e-04 9.88732422e-05]\n",
      " [1.67995582e-02 2.83027040e-02 1.06507544e-01 ... 1.02493856e-04\n",
      "  3.71576436e-04 9.48664708e-05]]\n",
      "\n",
      "X_test_norm_l2 ==>\n",
      " [[1.48008851e-02 2.20767012e-02 9.62472956e-02 ... 1.20472321e-04\n",
      "  3.57737513e-04 1.03855449e-04]\n",
      " [8.63315961e-03 9.71344410e-03 5.63388874e-02 ... 8.15455256e-05\n",
      "  1.16278723e-04 3.00337321e-05]\n",
      " [1.11366053e-02 1.40324109e-02 7.32595578e-02 ... 1.09060935e-04\n",
      "  2.04363191e-04 5.77648371e-05]\n",
      " ...\n",
      " [1.77702285e-02 2.30303396e-02 1.13948505e-01 ... 1.48208642e-04\n",
      "  4.10936535e-04 1.20458086e-04]\n",
      " [1.42499839e-02 2.79087236e-02 9.27451481e-02 ... 8.23633033e-05\n",
      "  1.89398520e-04 7.81243840e-05]\n",
      " [5.56001549e-03 8.34672851e-03 3.63962422e-02 ... 4.44962165e-05\n",
      "  7.69229349e-05 2.20415858e-05]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "## Normalize the data using L1 norm\n",
    "X_train_norm_l1 = normalize(X_train, norm='l1')\n",
    "X_test_norm_l1 = normalize(X_test, norm='l1')\n",
    "\n",
    "## Normalize the data using L2 norm\n",
    "X_train_norm_l2 = normalize(X_train, norm='l2')\n",
    "X_test_norm_l2 = normalize(X_test, norm='l2')\n",
    "\n",
    "print(f\"X_train_norm_l1 ==>\\n {X_train_norm_l1}\")\n",
    "print()\n",
    "print(f\"X_test_norm_l1 ==>\\n {X_test_norm_l1}\")\n",
    "print()\n",
    "print(f\"X_train_norm_l2 ==>\\n {X_train_norm_l2}\")\n",
    "print()\n",
    "print(f\"X_test_norm_l2 ==>\\n {X_test_norm_l2}\")\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
